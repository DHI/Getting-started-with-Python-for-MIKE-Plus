# Data Validation and Cleaning with `tsod`

## Introduction
Once you've identified potential outliers or anomalies in your time series data, typically using a library like `tsod` as introduced in the previous section, the next crucial step is to decide how to handle them. This process, often called data cleaning or data validation, is vital for ensuring the quality of your data before it's used for further analysis, visualization, or as input to MIKE+ models.

In this section, we'll walk through:
- A practical example of detecting anomalies using `tsod`'s `SigmaDetector`.
- Common strategies for handling these detected anomalies.
- Methods for dealing with missing data (`NaN` values), which can arise from removing outliers or be inherently present in your original dataset.

This builds directly on your `pandas` skills for data manipulation and `matplotlib` for visualization.

## Detecting Anomalies with `tsod` (Example)
As mentioned in the introduction to `tsod`, we'll use a straightforward, "cookie cutter" approach for anomaly detection. The `SigmaDetector` from `tsod` is a good example of this, as it uses a common statistical method based on standard deviations from the mean.

Let's see it in action:
```python
import mikeio
import pandas as pd
import numpy as np # For np.nan
from tsod.detectors import SigmaDetector # Import the SigmaDetector
import matplotlib.pyplot as plt

# Ensure you have this sample file in your 'data' subfolder
dfs0_file = "data/flow_with_anomalies.dfs0" 
# This file can be downloaded from: [link_to_sample_data_with_anomalies_dfs0]{download="sample_data_with_anomalies.dfs0"}
# (Assuming this link is provided in the video section, or it's a general course data file)

try:
    ds = mikeio.read(dfs0_file)
    df = ds.to_dataframe()
    # Let's work with the first item in the dfs0 file
    # In a real scenario, you might select a specific column by name
    series_to_check = df.iloc[:, 0] 
except FileNotFoundError:
    print(f"Error: The file {dfs0_file} was not found. Please ensure it's in the correct path.")
    # Create dummy data if file not found, so the rest of the example can run
    # This is for illustrative purposes if the file is missing during generation
    print("Using dummy data for demonstration.")
    rng = pd.date_range('2023-01-01', periods=100, freq='H')
    data = np.random.randn(100).cumsum() + 20
    data[20] = 50 # Anomaly
    data[60] = -10 # Another anomaly
    series_to_check = pd.Series(data, index=rng, name="Dummy_Flow")


print("Original Series head:")
print(series_to_check.head())

# Initialize the SigmaDetector
# - window_size: The rolling window (number of observations) to calculate mean and standard deviation.
#                If None, global mean and standard deviation are used.
# - n_sigma: The number of standard deviations from the mean to set the threshold.
#            Points outside mean +/- (n_sigma * std_dev) are considered anomalies.
detector = SigmaDetector(window_size=None, n_sigma=3) 

# The .detect() method returns a boolean pandas Series:
# True where an anomaly is detected, False otherwise.
anomalies = detector.detect(series_to_check)

print("\nDetected Anomalies (True means anomaly):")
print(anomalies.head())
print(f"\nNumber of anomalies detected: {anomalies.sum()}")

# Visualize the original data and the detected anomalies
plt.figure(figsize=(12, 6))
series_to_check.plot(label="Original Data", color='blue')
series_to_check[anomalies].plot(marker='o', linestyle='None', color='red', label="Detected Anomalies")
plt.title(f"Time Series with Detected Anomalies ({series_to_check.name})")
plt.xlabel("Time")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.show()
```

The `SigmaDetector` is a simple yet effective way to flag points that are statistically unusual compared to the rest of the data (or a rolling window of it). A point is flagged if it falls outside the range defined by `mean ± (n_sigma * standard_deviation)`.

## Handling Detected Anomalies
Once anomalies have been identified (i.e., where the `anomalies` Series is `True`), you need to decide what to do with them. Common strategies include:

1.  **Removal:** Delete the anomalous data points. In `pandas`, this usually means replacing them with `np.nan` (Not a Number), which signifies missing data. This is often a first step if anomalies are considered errors.
2.  **Replacement/Imputation:** Replace anomalous values with an estimated value. This could be a simple statistical measure (like the mean or median of surrounding points) or a more sophisticated interpolated value.
3.  **Investigation:** If the anomalies are not obvious errors, they might represent genuine extreme events or interesting phenomena. In such cases, further domain-specific investigation is required before altering the data.

For this course, we'll primarily focus on removal (replacing with `np.nan`) and then imputation (filling `np.nan` values).

Let's replace the detected anomalies with `np.nan`:
```python
# Work on a copy to preserve the original series
series_cleaned = series_to_check.copy() 

# Where 'anomalies' is True, set the value in 'series_cleaned' to np.nan
series_cleaned[anomalies] = np.nan 

print("\nSeries with anomalies replaced by NaN:")
print(series_cleaned.head())
print(f"Number of NaNs after replacing anomalies: {series_cleaned.isna().sum()}")

# Plot to see the effect
plt.figure(figsize=(12, 6))
series_to_check.plot(label="Original Data", alpha=0.5) # Original slightly transparent
series_cleaned.plot(label="Data with Anomalies as NaN", color='green')
series_to_check[anomalies].plot(marker='o', linestyle='None', color='red', label="Detected Anomalies")
plt.title(f"Anomalies Replaced with NaN ({series_to_check.name})")
plt.xlabel("Time")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.show()
```

## Filling `NaN` Values
Whether `NaN` values result from removing anomalies, or were already present in your dataset, you'll often need to handle them. Leaving `NaN`s can cause problems for subsequent calculations or model runs. `pandas` provides several convenient methods for filling missing values, primarily through the `.fillna()` method and the `.interpolate()` method.

Here are some common techniques:

```python
# Assuming 'series_cleaned' is the Series with NaN values from the previous step

# 1. Forward Fill (ffill): Propagate the last valid observation forward.
# Useful when you assume a value remains constant until the next measurement.
series_ffill = series_cleaned.fillna(method='ffill')
print("\nForward Filled Series (head):")
print(series_ffill.head(10)) # Print more rows to see effect if NaNs are early

# 2. Backward Fill (bfill): Propagate the next valid observation backward.
# Useful if you assume a value was constant leading up to a measurement.
series_bfill = series_cleaned.fillna(method='bfill')
print("\nBackward Filled Series (head):")
print(series_bfill.head(10))

# 3. Interpolation: Fill NaN values using an interpolation method.
# For time series, linear interpolation based on time is often suitable.
# 'method="time"' requires the Series to have a DatetimeIndex.
series_interp = series_cleaned.interpolate(method='time')
print("\nInterpolated (time-based) Series (head):")
print(series_interp.head(10))

# 4. Fill with a specific value (e.g., the mean or median of the non-NaN values)
mean_val = series_cleaned.mean() # Calculate mean excluding NaNs
series_fill_mean = series_cleaned.fillna(mean_val)
print("\nSeries Filled with Mean (head):")
print(series_fill_mean.head(10))

# Visualize the different filling methods for a segment with NaNs
# This requires finding a segment with NaNs to make the plot meaningful.
# For now, we'll just plot the interpolated series as it's often a good choice.

plt.figure(figsize=(12, 6))
series_to_check.plot(label="Original Data", alpha=0.3, linestyle=':')
series_cleaned.plot(label="Data with Anomalies as NaN", marker='.', linestyle='--', alpha=0.6)
series_interp.plot(label="Interpolated Data (time-based)", color='purple')
series_to_check[anomalies].plot(marker='o', linestyle='None', color='red', label="Original Anomalies")
plt.title(f"Comparison of Original, NaN-replaced, and Interpolated Data ({series_to_check.name})")
plt.xlabel("Time")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.show()
```

The choice of filling method depends heavily on the nature of your data and the specific goals of your analysis.
-   **Forward/Backward fill** are simple but can introduce steps or plateaus.
-   **Interpolation** (especially `method='time'` for time series) often provides a smoother and more realistic imputation, provided the gaps are not too large.
-   Filling with a **mean/median** can dampen variability and might not be suitable for dynamic time series.

::: {.callout-tip}
## `tsod` Detectors
`tsod` offers a variety of detectors beyond the `SigmaDetector` used here. For example:
-   `ConstantValueDetector`: Finds consecutive constant values.
-   `RangeDetector`: Identifies values outside a predefined min/max range.
-   `CombinedDetector`: Allows you to chain multiple detectors.

The `SigmaDetector` is a good "cookie cutter" starting point. As your needs become more complex, you might explore other detectors. Refer to the [official `tsod` documentation on GitHub](https://github.com/DHI/tsod) for more details and examples.
:::

::: {.callout-caution}
## Order of Operations is Key
Be mindful of the sequence in which you perform data cleaning steps:

1.  **Detect anomalies:** Always try to detect anomalies on the most original version of your data possible. Prior manipulations (like filling `NaN`s) can mask or distort outliers.
2.  **Handle anomalies:** Decide on your strategy (e.g., replace with `np.nan`, investigate).
3.  **Fill `NaN` values:** After anomalies have been dealt with (and potentially converted to `NaN`s), then proceed to fill any remaining or newly created `NaN` values using an appropriate method.

Performing these steps out of order can lead to suboptimal or incorrect data cleaning.
:::

## Video - Example: Anomaly Detection with `tsod` and Filling NaNs (5-10 min)
- This video will demonstrate using a `tsod` detector (e.g., `SigmaDetector`) to find anomalies in a sample time series.
- It will show how to replace these anomalies with `np.nan` and then use various `pandas` methods (`ffill`, `bfill`, `interpolate`) to fill the resulting `NaN` values, similar to the code examples above.
- The video will also visually compare the original, anomaly-marked, and cleaned data.
- Example data file used in video: `[link_to_sample_data_with_anomalies_dfs0]{download="sample_data_with_anomalies.dfs0"}`
- {{< video URL_TO_DATA_VALIDATION_VIDEO >}}