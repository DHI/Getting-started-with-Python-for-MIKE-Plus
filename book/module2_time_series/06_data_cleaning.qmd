# Data Cleaning

Data cleaning is an essential step in any MIKE+ modelling workflow to ensure your input data is reliable and your models perform optimally. High-quality data leads to more trustworthy model outcomes. This section will cover how to handle missing values, often represented as `NaN`s, and then introduce `tsod`, a Python package from DHI for detecting and addressing anomalies in your time series data.

## Missing Values

Missing values can significantly impact your model results if not handled appropriately. In Pandas, and by extension in the scientific Python ecosystem, missing numerical data is typically represented by `NaN`, which stands for "Not a Number". These `NaN`s can arise from various sources, such as sensor malfunctions during data collection, gaps that occur during data transmission, or they might be the result of previous data processing or cleaning steps.

You can easily check for the presence and count of `NaN`s in a Pandas DataFrame column. For instance, let's create a small sample DataFrame and then determine how many missing values are in the 'WaterLevel' column:

```{python}
import pandas as pd
import numpy as np

data = {'WaterLevel': [1.0, 1.2, np.nan, 1.3, 1.1, np.nan, 1.5]}
df_sample = pd.DataFrame(data)
df_sample['WaterLevel'].isnull().sum()
```

## Imputation

It's often necessary to fill these `NaN`s, a process known as imputation. DHI's modelling engines typically require complete datasets for calculations, and thus DFS0 files, which are often used as inputs, should not contain missing values. Furthermore, many analytical algorithms perform better or exclusively with complete data.

For `NaN`s that occur *between* valid data points (sometimes called bounded `NaN`s), using the `.interpolate()` method is a common and effective approach. Linear interpolation is a general choice, and if your DataFrame has a `DatetimeIndex`, using `method='time'` can provide more accurate interpolation based on the time intervals.

Here's how you can apply linear interpolation to fill the bounded `NaN`s in our sample 'WaterLevel' column. We'll make a copy to preserve the original:

```{python}
df_interpolated = df_sample.copy()
df_interpolated['WaterLevel'] = df_interpolated['WaterLevel'].interpolate(method='linear')
df_interpolated
```

For `NaN`s that appear at the very beginning or end of your dataset (unbounded `NaN`s), the `.fillna()` method is more suitable. You can use `method='ffill'` (forward fill) to propagate the last valid observation forward, or `method='bfill'` (backward fill) to use the next valid observation to fill the gap.

Let's imagine our 'WaterLevel' data had a leading `NaN` (and to illustrate, a trailing one as well):

```{python}
data_unbounded_nan = {'WaterLevel': [np.nan, 1.0, 1.2, 1.3, np.nan]}
df_unbounded_nan = pd.DataFrame(data_unbounded_nan)
df_unbounded_nan['WaterLevel'] = df_unbounded_nan['WaterLevel'].fillna(method='bfill')
df_unbounded_nan['WaterLevel'] = df_unbounded_nan['WaterLevel'].fillna(method='ffill') # For any trailing
df_unbounded_nan
```

You can also replace `NaN`s with a specific constant value, such as the mean or median of the column, or zero. However, this approach should be used cautiously as it can introduce bias into your dataset or misrepresent the underlying process.

::: {.callout-tip}
These `NaN` imputation methods, such as `.fillna()` and `.interpolate()`, reuse skills and concepts you might have encountered in the "Resampling" section. The techniques for handling data gaps are quite versatile.
:::

::: {.callout-caution}
The choice of `NaN` imputation method can significantly impact your subsequent analysis and model results. Always consider the nature and context of your data, and the potential implications of the chosen imputation strategy.
:::

## Anomaly Detection (Rule-Based)

::: {.callout-tip}
Short on time? This section provides an introduction to a useful package but can be considered optional for core module understanding.
:::

Beyond clearly missing values, time series data can also contain anomaliesï¿½data points that deviate significantly from the general pattern or expected behavior. Identifying and addressing these anomalies is crucial for building robust MIKE+ models. The `tsod` Python package, developed by DHI, provides a suite of tools for rule-based anomaly detection in time series.

Anomaly detection is a broad and complex field; this section serves as a basic introduction to what `tsod` offers. For more comprehensive information, examples, and advanced usage, you are encouraged to visit the [tsod GitHub repository](https://github.com/DHI/tsod).

### Install `tsod`

First, if you haven't already, you'll need to install `tsod` into your Python environment. You can do this using `uv`:

```powershell
uv pip install tsod
```

### The Detector Concept

`tsod` operates using a concept called "detectors." Each detector is designed to implement a specific rule or heuristic to identify anomalies. For example, there are detectors for values falling outside a predefined range (`RangeDetector`), for values that remain constant over an extended period (`ConstantValueDetector`), for identifying sudden large changes or differences between consecutive points (`DiffDetector`), or for flagging points that deviate significantly from a rolling standard deviation (`RollingStdDetector`). When applied to a time series, these detectors output a boolean mask (a Series of `True`/`False` values) of the same length as the input, where `True` indicates a detected anomaly at that position.

::: {.callout-tip}
`tsod` offers a variety of detectors beyond those mentioned. It's highly recommended to check the official `tsod` documentation for a full list, detailed explanations, and usage examples for each detector.
:::

### Workflow: Detect, Visualize, NaN, Impute

Let's walk through a minimal example workflow using the `RollingStdDetector` to illustrate the typical process:

1.  **Prep Data**: `tsod` detectors generally expect a Pandas Series as input. Let's create a sample Series that includes an obvious anomaly.

    ```{python}
    data_anomaly = [1.0, 1.1, 1.0, 1.2, 5.0, 1.1, 1.0, 1.2]
    series_anomaly = pd.Series(data_anomaly, name="Flow_Anomaly")
    series_anomaly
    ```

2.  **Detect & Visualize Anomalies**: Now, we'll import the `RollingStdDetector` from `tsod.detectors`, initialize it with appropriate parameters (like `window_size` and `n_std` for the number of standard deviations), and then apply it to our series to get the `anomaly_mask`.

    ```{python}
    from tsod.detectors import RollingStdDetector

    detector = RollingStdDetector(window_size=3, n_std=1.5)
    anomaly_mask = detector.detect(series_anomaly)
    anomaly_mask
    ```

    After detection, it's good practice to visualize the original series and highlight the detected anomalies to verify the detector's performance.

    ```{python}
    import matplotlib.pyplot as plt

    fig, ax = plt.subplots(figsize=(8,4))
    series_anomaly.plot(ax=ax, label="Original Flow")
    if anomaly_mask.any():
        series_anomaly[anomaly_mask].plot(ax=ax, style='ro', markersize=7, label="Detected Anomaly")
    ax.legend()
    ```

3.  **Convert Anomalies to `NaN`**: Once you're satisfied with the detected anomalies, the next step is to convert these anomalous values in your original data to `np.nan`. This uses the boolean `anomaly_mask` for indexing. (Ensure `numpy` is imported as `np`, which we did at the start of this section).

    ```{python}
    series_cleaned = series_anomaly.copy()
    series_cleaned[anomaly_mask] = np.nan
    series_cleaned
    ```

4.  **Impute `NaN`s**: With the identified anomalies now represented as `NaN`s, you can use the imputation methods discussed earlier in the "Missing Values" section (like `.interpolate()` or `.fillna()`) to fill these gaps. For instance, using linear interpolation:

    ```{python}
    series_imputed = series_cleaned.interpolate(method='linear')
    series_imputed
    ```

    Optionally, you can plot the final imputed series to visually inspect the result of the cleaning process and compare it with the original data.

    ```{python}
    fig, ax = plt.subplots(figsize=(8,4))
    series_imputed.plot(ax=ax, label="Imputed Flow", marker='.')
    series_anomaly.plot(ax=ax, label="Original Flow", alpha=0.6, style='--')
    if anomaly_mask.any():
        series_anomaly[anomaly_mask].plot(ax=ax, style='ro', markersize=7, alpha=0.6, label="Original Anomaly")
    ax.legend()
    ```

This workflow provides a structured way to improve data quality by addressing both missing data and detectable anomalies before using the data in your MIKE+ models.