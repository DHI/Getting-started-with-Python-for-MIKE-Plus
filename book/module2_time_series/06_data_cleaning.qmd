# Data Cleaning

Data cleaning is an essential step in any MIKE+ modelling workflow to ensure your input data is complete. This section covers handling missing values (e.g. nan). Additionally, it introduces the topic of detecting anomalies in time series data.

## Missing Values

DHI's modelling engines typically require complete datasets for calculations, and thus dfs0 files, which are often used as inputs, should not contain missing values. For example, a rainfall boundary condition cannot have the value `nan`.

::: {.callout-note}
Missing numerical data is typically represented by `nan`. These arise from various sources, such as sensor malfunctions during data collection, gaps that occur during data transmission, or they might be the result of previous data processing or cleaning steps.
:::

```{python}
#| echo: false
#| output: false
#| warning: false
#| message: false
import mikeio
import numpy as np
np.random.seed(42)
df = mikeio.read("data/single_water_level.dfs0").to_dataframe()
df.loc["1993-12-06"] = np.nan # simulate gap
```

Assume we have a DateFrame with missing values on 1993-12-06:

```{python}
#| code-fold: true
#| code-summary: "Show Plotting Code"
ax = df.plot()
ax.axvspan(
    xmin="1993-12-06 00:00",
    xmax="1993-12-07 00:00",
    color='grey',
    alpha=0.3,
    label="Missing Data"
)
ax.legend(loc="upper right")
```

Count the number of missing values (e.g. `nan`) for each time series by summing the result of `isna()`.

```{python}
df.isna().sum()
```

## Imputation

The process of filling missing values is known as imputation. 

For missing values *between* valid data points (i.e. bounded), using the `.interpolate()` method is a common and effective approach.

```{python}
df_interpolated = df.interpolate(method='time')
```

```{python}
#| code-fold: true
#| code-summary: "Show Plotting Code"
ax = df.plot()
ax.axvspan(
    xmin="1993-12-06 00:00",
    xmax="1993-12-07 00:00",
    color='grey',
    alpha=0.3,
    label="Missing Data"
)
df_interpolated.columns = ["Interpolation"]
df_interpolated.loc["1993-12-06"].plot(ax=ax)
ax.legend(loc="upper right")
```

::: {.callout-note}
The example above uses `method='time'`, which is a linear interpolation that considers non-equidistant `DatetimeIndex` indices. Refer to [Pandas's documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html) for additional interpolation methods, such as polynomial.
:::

For missing values appearing at the very beginning or end of your dataset (i.e. unbounded), you can make use of:

- `.fillna()`
- `.ffill()`
- `.bfill()`

::: {.callout-tip}
Recall: these imputation methods were introduced in the section on resampling, where upsampling introduced `nan` values.
:::

Same example as above, but using `ffill()`.

```{python}
df_interpolated = df.ffill()
```

```{python}
#| code-fold: true
#| code-summary: "Show Plotting Code"
ax = df.plot()
ax.axvspan(
    xmin="1993-12-06 00:00",
    xmax="1993-12-07 00:00",
    color='grey',
    alpha=0.3,
    label="Missing Data"
)
df_interpolated.columns = ["Interpolation"]
df_interpolated.loc["1993-12-06"].plot(ax=ax)
ax.legend(loc="upper right")
```

Same example as above, but using `bfill()`.

```{python}
df_interpolated = df.bfill()
```

```{python}
#| code-fold: true
#| code-summary: "Show Plotting Code"
ax = df.plot()
ax.axvspan(
    xmin="1993-12-06 00:00",
    xmax="1993-12-07 00:00",
    color='grey',
    alpha=0.3,
    label="Missing Data"
)
df_interpolated.columns = ["Interpolation"]
df_interpolated.loc["1993-12-06"].plot(ax=ax)
ax.legend(loc="upper right")
```

Same example as above, but using `fillna()`. 

```{python}
df_interpolated = df.fillna(0.1) # specify the value to fill with
```

```{python}
#| code-fold: true
#| code-summary: "Show Plotting Code"
ax = df.plot()
ax.axvspan(
    xmin="1993-12-06 00:00",
    xmax="1993-12-07 00:00",
    color='grey',
    alpha=0.3,
    label="Missing Data"
)
df_interpolated.columns = ["Interpolation"]
df_interpolated.loc["1993-12-06"].plot(ax=ax)
ax.legend(loc="upper right")
```

## Anomaly Detection (Rule-Based)

::: {.callout-tip}
Short on time? This section provides an introduction to a useful package but can be considered optional for core module understanding.
:::

Beyond clearly missing values, time series data can also contain anomalies. Identifying and addressing these anomalies is crucial for building robust MIKE+ models. 

Anomaly detection is a broad and complex field. This section offers a basic introduction to rule-based anomaly detection using DHI's [tsod Python package](https://github.com/DHI/tsod).

### Install `tsod`

```powershell
uv pip install tsod
```

### The Detector Concept

`tsod` operates using a concept called "detectors." Each detector is designed to implement a specific rule or heuristic to identify anomalies. Example anomaly detectors:

*   **RangeDetector:** Flags values outside a set range.
*   **ConstantValueDetector:** Detects unchanging values over time.
*   **DiffDetector:** Catches large changes between points.
*   **RollingStdDetector:**  Finds points far from rolling standard deviation.

There's also the `CombinedDetector`, which allows combining the rules of several detectors.

### Detecting Anomalies

 ```{python}
from tsod.detectors import RangeDetector

detector = RollingStdDetector(window_size=3, n_std=1.5)
anomaly_mask = detector.detect(series_anomaly)
anomaly_mask
```

Let's walk through a minimal example workflow using the `RollingStdDetector` to illustrate the typical process:

1.  **Prep Data**: `tsod` detectors generally expect a Pandas Series as input. Let's create a sample Series that includes an obvious anomaly.

    ```{python}
    data_anomaly = [1.0, 1.1, 1.0, 1.2, 5.0, 1.1, 1.0, 1.2]
    series_anomaly = pd.Series(data_anomaly, name="Flow_Anomaly")
    series_anomaly
    ```

2.  **Detect & Visualize Anomalies**: Now, we'll import the `RollingStdDetector` from `tsod.detectors`, initialize it with appropriate parameters (like `window_size` and `n_std` for the number of standard deviations), and then apply it to our series to get the `anomaly_mask`.

    ```{python}
    from tsod.detectors import RollingStdDetector

    detector = RollingStdDetector(window_size=3, n_std=1.5)
    anomaly_mask = detector.detect(series_anomaly)
    anomaly_mask
    ```

    After detection, it's good practice to visualize the original series and highlight the detected anomalies to verify the detector's performance.

    ```{python}
    import matplotlib.pyplot as plt

    fig, ax = plt.subplots(figsize=(8,4))
    series_anomaly.plot(ax=ax, label="Original Flow")
    if anomaly_mask.any():
        series_anomaly[anomaly_mask].plot(ax=ax, style='ro', markersize=7, label="Detected Anomaly")
    ax.legend()
    ```

3.  **Convert Anomalies to `NaN`**: Once you're satisfied with the detected anomalies, the next step is to convert these anomalous values in your original data to `np.nan`. This uses the boolean `anomaly_mask` for indexing. (Ensure `numpy` is imported as `np`, which we did at the start of this section).

    ```{python}
    series_cleaned = series_anomaly.copy()
    series_cleaned[anomaly_mask] = np.nan
    series_cleaned
    ```

4.  **Impute `NaN`s**: With the identified anomalies now represented as `NaN`s, you can use the imputation methods discussed earlier in the "Missing Values" section (like `.interpolate()` or `.fillna()`) to fill these gaps. For instance, using linear interpolation:

    ```{python}
    series_imputed = series_cleaned.interpolate(method='linear')
    series_imputed
    ```

    Optionally, you can plot the final imputed series to visually inspect the result of the cleaning process and compare it with the original data.

    ```{python}
    fig, ax = plt.subplots(figsize=(8,4))
    series_imputed.plot(ax=ax, label="Imputed Flow", marker='.')
    series_anomaly.plot(ax=ax, label="Original Flow", alpha=0.6, style='--')
    if anomaly_mask.any():
        series_anomaly[anomaly_mask].plot(ax=ax, style='ro', markersize=7, alpha=0.6, label="Original Anomaly")
    ax.legend()
    ```

This workflow provides a structured way to improve data quality by addressing both missing data and detectable anomalies before using the data in your MIKE+ models.