# Creating `dfs0` Files from `pandas` DataFrames

After meticulously processing your time series data in `pandas`—perhaps by resampling, cleaning, or validating it—you'll often need to save it back into a `dfs0` file. This format is standard for use in MIKE+ or other DHI software, and `mikeio` makes this export process straightforward. The general workflow involves ensuring your `pandas` DataFrame is correctly prepared, converting relevant data columns into `mikeio.DataArray` objects, collecting these into a `mikeio.Dataset`, and finally, writing this `Dataset` to a `.dfs0` file.

## Preparing Your Pandas DataFrame

Before you can export your data, it's crucial to ensure your `pandas` DataFrame is suitably formatted. First, the DataFrame **must** have a `DatetimeIndex`, as this provides the time information for your series. Second, the columns you intend to export should have meaningful names; these names can directly translate into item names within the `dfs0` file. Lastly, ensure the data itself is numeric, typically floats or integers.

Let's assume you have a processed DataFrame named `processed_df`.

## Creating `mikeio.DataArray` Objects

A `mikeio.DataArray` is the `mikeio` representation of a single time series item, complete with its data, time axis, and metadata. You will need to create one `DataArray` for each `pandas` Series (i.e., each column from your DataFrame) that you wish to include in your `dfs0` file.

To create a `DataArray`, you'll primarily use the `data` (your `pandas` Series), `time` (the `DatetimeIndex`), and an `item` argument. The `item` argument takes a `mikeio.ItemInfo` object, which describes the data, including its name, EUM type (e.g., water level, discharge), and EUM unit.

Here's how you might create a `DataArray` from a column in `processed_df`:

```python
import pandas as pd
import mikeio

# Assume processed_df is a pre-existing DataFrame with a DatetimeIndex
# For example:
index = pd.date_range(start='2023-01-01', periods=3, freq='H')
data = {'WaterLevel_m': [1.0, 1.1, 1.2], 'Discharge_m3s': [10, 12, 11]}
processed_df = pd.DataFrame(data, index=index)

item_info_wl = mikeio.ItemInfo("Processed Water Level", 
                               mikeio.EUMType.Water_Level, 
                               mikeio.EUMUnit.meter)
da_water_level = mikeio.DataArray(data=processed_df['WaterLevel_m'],
                                  time=processed_df.index,
                                  item=item_info_wl)
```

::: {.callout-tip}
## Item Information (`ItemInfo` and `EUMType`)
Specifying correct item information using `mikeio.ItemInfo`, along with `mikeio.EUMType` and `mikeio.EUMUnit`, is important for ensuring compatibility and proper interpretation by MIKE software. For instance, use `mikeio.EUMType.Rainfall` and `mikeio.EUMUnit.mm_per_hour` for precipitation data. You can find comprehensive details on available `EUMType`s, `EUMUnit`s, and creating `ItemInfo` objects in the [MIKE IO documentation](https://dhi.github.io/mikeio/api-dfs.html#iteminfo). While you can use generic types if unsure, specific types are always better for robust MIKE integration.
:::

## Assembling `DataArray`s into a `mikeio.Dataset`

Once you have your individual `DataArray` objects, the next step is to collect them into a `mikeio.Dataset`. A `Dataset` is essentially a container for one or more `DataArray`s that share the same time axis.

Continuing the example, if we also had a discharge column:

```python
item_info_q = mikeio.ItemInfo("Simulated Discharge", 
                              mikeio.EUMType.Discharge, 
                              mikeio.EUMUnit.m3_per_sec)
da_discharge = mikeio.DataArray(data=processed_df['Discharge_m3s'],
                                time=processed_df.index,
                                item=item_info_q)

dataset_to_save = mikeio.Dataset([da_water_level, da_discharge])
```

If you only have one `DataArray`, you still place it within a list when creating the `Dataset`.

## Writing the `Dataset` to a `dfs0` File

The final step is to write your `Dataset` object to a `.dfs0` file. This is accomplished using the `.to_dfs()` method of the `Dataset`. You simply provide the desired file path and name for your new `dfs0` file.

```python
output_dfs0_path = "data/processed_timeseries.dfs0"
dataset_to_save.to_dfs(output_dfs0_path)

print(f"Data successfully saved to {output_dfs0_path}")
```

And with that, your processed time series data, originating from a `pandas` DataFrame, is now neatly packaged in `dfs0` format, ready for use in your MIKE+ modelling workflows or other DHI applications.

## Video - Example: Creating a `dfs0` File (5-10 min)
*   Start with a processed `pandas` DataFrame.
*   Demonstrate creating `mikeio.DataArray` objects for a couple of columns, including setting basic `ItemInfo`.
*   Show creating a `mikeio.Dataset` from these `DataArray`s.
*   Illustrate saving the `Dataset` to a new `.dfs0` file using `.to_dfs()`.
*   Optionally, briefly open the created `dfs0` file in MIKE Zero or another tool to verify its contents (if feasible to show quickly).

```markdown
{{< video https://www.youtube.com/embed/placeholder_video_id_for_creating_dfs0 >}}
```