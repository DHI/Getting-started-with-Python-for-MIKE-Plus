[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting started with Python for MIKE+",
    "section": "",
    "text": "Introduction\nDHI offers a range of free, open-source Python libraries that enable automated and reproducible MIKE+ workflows, as well as unlock the potential for robust and flexible analyses. This course is designed for experienced MIKE+ modelers who are new to Python, providing a practical foundation to begin applying concepts to real projects. You’ll gain essential skills to read, run, and modify Python scripts relevant to MIKE+ modelling through focused, hand-tailored examples. The course will orient you to a new way of working, guiding you through the transition from a GUI to a script-based environment, helping you navigate common challenges, and giving you the confidence to continue exploring Python and seek out resources to further develop your skills.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-python-with-mike",
    "href": "index.html#why-python-with-mike",
    "title": "Getting started with Python for MIKE+",
    "section": "Why Python with MIKE+?",
    "text": "Why Python with MIKE+?\nUsing Python alongside MIKE+ provides the following advantages:\n\nEfficient handling of various file types, including dfs0, res1d, and xns11\nConversion of data between MIKE+ and third-party formats such as CSV and Excel\nFlexibility to modify MIKE+ databases, access tools, and run simulations\nAutomation of modelling tasks using a straightforward scripting syntax\nReproducible and documented workflows that enhance model quality assurance",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Getting started with Python for MIKE+",
    "section": "Intended Audience",
    "text": "Intended Audience\nThis course is ideal for MIKE+ modelers who:\n\nAre eager to explore Python’s potential in MIKE+ modelling\nWant to enhance, automate, or document parts of their workflows with Python\nSeek more flexible and robust techniques for advanced modelling needs",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Getting started with Python for MIKE+",
    "section": "Course Structure",
    "text": "Course Structure\nThe course focuses on practical applications of Python for common MIKE+ modelling tasks. Content generally consists of a combination of videos, live sessions, and hands-on exercises. We will cover Python libraries such as MIKE IO, MIKE IO 1D, and MIKE+Py.\n\nModule 1 | Foundations\n\nTopics: Python and Python Packages, Visual Studio Code, GitHub, Jupyter Notebooks, LLMs for coding, Pandas, Matplotlib, Documentation\n\nModule 2 | Time Series\n\nTopics: dfs0 files, plotting, statistics, selections, resampling, basic data validation\n\nModule 3 | Network Results\n\nTopics: network result files (e.g. res1d, res, res11), selecting data, extracting results, geospatial formats (e.g. shapefiles)\n\nModule 4 | Calibration Plots and Statistics\n\nTopics: basic statistics and plots relevant for model calibration\n\nModule 5 | MIKE+Py\n\nTopics: databases and SQL, modifying MIKE+ databases, accessing GUI tools, running simulations\n\nModule 6 | Putting Everything Together\n\nTopics: final project applying lessons of previous modules.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#course-objectives",
    "href": "index.html#course-objectives",
    "title": "Getting started with Python for MIKE+",
    "section": "Course Objectives",
    "text": "Course Objectives\nAfter completing this course, you should be able to:\n\nInstall Python and related packages for use with MIKE+\nApply Python to create reproducible and automated workflows\nExplore documentation and run example notebooks and scripts\nConnect with the open-source Python community and MIKE+ modelers",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "module1_foundations/index.html",
    "href": "module1_foundations/index.html",
    "title": "Welcome to Module One!",
    "section": "",
    "text": "In this module, you’ll gain the skills to confidently set up your coding environment, access course materials, and run Python code with ease. Our focus is on:\n\nMastering essential tools (e.g. GitHub, Visual Studio Code, uv).\nRunning and understanding Python scripts and Jupyter notebooks.\nLearning basic Python syntax and concepts.\nExploring core libraries (e.g. NumPy, Pandas, Matplotlib) for data analysis.\n\nDon’t worry if it feels fast-paced — you’ll practice these concepts in later modules. Let’s get you set up and ready to dive in!",
    "crumbs": [
      "Module 1 - Foundations",
      "Welcome to Module One!"
    ]
  },
  {
    "objectID": "module1_foundations/github.html",
    "href": "module1_foundations/github.html",
    "title": "1  GitHub",
    "section": "",
    "text": "1.1 DHI’s Python Ecosystem on GitHub\nGitHub is a website for storing, sharing, and collaborating on software development projects. It’s an especially popular platform for open-source software. DHI uses GitHub for hosting its entire open-source Python ecosystem, including documentation and examples.\nDHI’s Python ecosystem is organized into modular Python packages based on functionality. This is a common pattern in Python that empowers users to flexibly combine functionalities to meet specific project needs. An overview of Python packages useful for MIKE+ modelling is provided in the table below.\nFeel free to browse additional open-source packages on DHI’s GitHub profile.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "module1_foundations/github.html#dhis-python-ecosystem-on-github",
    "href": "module1_foundations/github.html#dhis-python-ecosystem-on-github",
    "title": "1  GitHub",
    "section": "",
    "text": "Package\nDescription\n\n\n\n\n\nRead, write and manipulate dfs0, dfs1, dfs2, dfs3, dfsu and mesh files.\n\n\n\nRead, manipulate, and analyze res1d, res, resx, out, and xns11 files.\n\n\n\nMIKE+Py is a python interface for MIKE+.\n\n\n\nCompare MIKE model results and observations.\n\n\n\nAnomaly Detection for time series data.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "module1_foundations/github.html#why-visit-a-python-packages-github-page",
    "href": "module1_foundations/github.html#why-visit-a-python-packages-github-page",
    "title": "1  GitHub",
    "section": "1.2 Why visit a Python package’s GitHub page?",
    "text": "1.2 Why visit a Python package’s GitHub page?\nYou’ll use GitHub for:\n\nAccessing documentation and examples.\nCreating ‘issues’ and/or ‘discussions’ when you need help.\nChecking out changes with new package versions.\nBrowsing source code and/or contributing code you think is generally useful.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "module1_foundations/github.html#typical-structure-of-a-python-package-on-github",
    "href": "module1_foundations/github.html#typical-structure-of-a-python-package-on-github",
    "title": "1  GitHub",
    "section": "1.3 Typical structure of a Python package on GitHub",
    "text": "1.3 Typical structure of a Python package on GitHub\nPlease watch the video below for a guided tour of how DHI organizes their Python packages on GitHub.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html",
    "href": "module1_foundations/python_management.html",
    "title": "2  Python Management",
    "section": "",
    "text": "2.1 Tools\nPython continuously releases new versions. Similarly, individual Python packages (hosted on PyPI) also continuously release new versions. Python scripts usually have dependencies on specific Python versions and packages, which highlights the need to carefully managing these. This is similar to different versions of MIKE+: you would not expect a MIKE+ 2025 model to run with MIKE+ 2023.\nThere are several tools for managing Python and packages together. Two common options are:\nThis course uses uv. Please install uv according to their official installation instructions. Use the “standalone installer” for Windows.\nConfirm you properly installed uv by opening a terminal and running:",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#tools",
    "href": "module1_foundations/python_management.html#tools",
    "title": "2  Python Management",
    "section": "",
    "text": "uv\nMiniforge\n\n\n\nuv --version\n\n\n\n\n\n\nLearn basics of terminals\n\n\n\nInstalling and using uv requires using a terminal. Being familiar with terminals is generally useful for Python. This course assumes basic knowledge. If you’ve never used a terminal before, then please refer to an introductory resource such as: Windows PowerShell.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#installing-python-with-uv",
    "href": "module1_foundations/python_management.html#installing-python-with-uv",
    "title": "2  Python Management",
    "section": "2.2 Installing Python with uv",
    "text": "2.2 Installing Python with uv\nYou can install Python with uv from the command line:\nuv python install\nBy default, this installs the latest version of Python (3.13.2 at the time of writing).\nConfirm it installed correctly by running:\nuv run python --version",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#virtual-environments",
    "href": "module1_foundations/python_management.html#virtual-environments",
    "title": "2  Python Management",
    "section": "2.3 Virtual Environments",
    "text": "2.3 Virtual Environments\n\n\n\n\n\n\nNote\n\n\n\nVirtual environments are an advanced Python topic, however, they are fundamental to using uv. Therefore, they will not be covered in depth, but explained just enough to be useful.\n\n\nVirtual environments are useful for isolating dependencies between projects. For example, let’s say you work on two projects: Project A and Project B. If Project A requires a different version of Python than Project B, then you can handle that by creating virtual environments for each project. This avoids a common issue encountered when not using virtual environments. Conceptually, a virtual environment is a single Python version and set of Python packages.\nCreate a new folder, and make a virtual environment:\nuv venv\n\n\n\n\n\n\nTip\n\n\n\nUse the terminal cd command to change its current directory. Alternatively, install Windows Terminal to easily launch a terminal from a folder within File Explorer via the right-click context menu.\n\n\nNotice a folder called .venv was created. Explore that folder to see what it contains. Can you find the file Python.exeand the folder site-packages?\nIt’s good practice to create a single virtual environment in the root directory of each project. Therefore, the remainder of this course assumes you always run uv from within a folder containing a virtual environment.\nRefer to uv’s documentation for additional details.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#python-package-management",
    "href": "module1_foundations/python_management.html#python-package-management",
    "title": "2  Python Management",
    "section": "2.4 Python package management",
    "text": "2.4 Python package management\nuv provides two different approaches for Python package management. This course uses their pip interface. Common workflows are shown in the following sections. Refer to uv’s documentation for more details.\n\n2.4.1 Install packages\nInstall Python packages with uv as follows:\nuv pip install &lt;package-name&gt;\nFor example, install mikeio as follows:\nuv pip install mikeio\nLook at the site-packages folder again. Notice that it now includes mikeio and many other packages. When a package is installed, all of its dependencies are also installed automatically.\n\n\n2.4.2 List installed packages\nList all installed Python packages and their versions with:\nuv pip list\n\n\n2.4.3 Upgrade packages\nUpgrade an older package version to the latest version as follows:\nuv pip install --upgrade mikeio\n\n\n2.4.4 Install specific package versions\nOccasionally there’s a need to install an older version of a package, which can be done as follows:\nuv pip install mikeio==1.7.1",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#example-video",
    "href": "module1_foundations/python_management.html#example-video",
    "title": "2  Python Management",
    "section": "2.5 Example video",
    "text": "2.5 Example video",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html",
    "href": "module1_foundations/ide.html",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "",
    "text": "3.1 Why use an IDE?\nAn Integrated Development Environment (IDE) is a software that bundles together tools convenient for software development. This course uses Visual Studio Code as an IDE, which is a popular free and open-source software provided by Microsoft.\nThere are several benefits to using an IDE compared to using a text editor like Notepad:",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#why-use-an-ide",
    "href": "module1_foundations/ide.html#why-use-an-ide",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "",
    "text": "Designed for easy code writing, with several shortcuts\nSyntax highlighting for more readable code\nAutomatic code completion\nIntegrated terminal\nIntegrated LLM chat and code completion\nHighly customizable with extensions",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#install-visual-studio-code",
    "href": "module1_foundations/ide.html#install-visual-studio-code",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.2 Install Visual Studio Code",
    "text": "3.2 Install Visual Studio Code\nInstall Visual Studio Code (VSCode) according to their official instructions.\n\n\n\n\n\n\nCaution\n\n\n\nYou may stumble upon a software called Visual Studio, which is different than Visual Studio Code.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#getting-started",
    "href": "module1_foundations/ide.html#getting-started",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.3 Getting Started",
    "text": "3.3 Getting Started\nVS Code provides excellent documentation. Please refer to their getting started guide for a basic introduction.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#visual-studio-code-extensions",
    "href": "module1_foundations/ide.html#visual-studio-code-extensions",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.4 Visual Studio Code Extensions",
    "text": "3.4 Visual Studio Code Extensions\nThis course uses the Python extension for VS Code. Extensions can be installed from within VS Code. Refer to VS Code’s documentation for guidance.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#opening-projects",
    "href": "module1_foundations/ide.html#opening-projects",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.5 Opening Projects",
    "text": "3.5 Opening Projects\nVS Code can be used in different ways. This course uses a common workflow of opening VS Code from the root directory of a project folder. Alternatively, open a project folder via “Open Folder” from within VS Code.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#selecting-python-interpreters",
    "href": "module1_foundations/ide.html#selecting-python-interpreters",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.6 Selecting Python Interpreters",
    "text": "3.6 Selecting Python Interpreters\nVS Code should automatically detect virtual environments located in the root project directory.\nOtherwise, there’s an option of manually selecting which Python Interpreter VS Code uses. Access it via the Command Palette (CTRL + SHIFT + P) and typing “Python: Select Interpreter”.\nVS Code uses the selected interpreter for running scripts, as well as for other features like auto completion.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#example---setting-up-a-fresh-project",
    "href": "module1_foundations/ide.html#example---setting-up-a-fresh-project",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.7 Example - Setting up a fresh project",
    "text": "3.7 Example - Setting up a fresh project",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_scripts.html",
    "href": "module1_foundations/python_scripts.html",
    "title": "4  Python Scripts",
    "section": "",
    "text": "4.1 Running Python Scripts\nA Python script is a file with the extension .py that contains Python code that’s executable via Python’s interpreter.\nPython is most powerful when scripts are reused. Therefore, it’s important to understand both how to run scripts others have sent you, as well as how to explain how others can use scripts you wrote.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Scripts</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_scripts.html#running-python-scripts",
    "href": "module1_foundations/python_scripts.html#running-python-scripts",
    "title": "4  Python Scripts",
    "section": "",
    "text": "4.1.1 Running in Terminal\nYou can run a script from the terminal by running:\nuv run python example_script.py\n\n\n4.1.2 Running in VS Code\nYou can run scripts from VS Code’s user interface. Under the hood, it executes the script in the terminal, so this is only a matter of preference. Refer to VS Code’s documentation on how to run Python code.\n\n\n\n\n\n\nTip\n\n\n\nRunning scripts in debug mode is more convenient via VS Code’s user interface. This lets you walk through code line by line as it executes, which is helpful when investigating unexpected outcomes.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Scripts</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_scripts.html#script-dependencies",
    "href": "module1_foundations/python_scripts.html#script-dependencies",
    "title": "4  Python Scripts",
    "section": "4.2 Script Dependencies",
    "text": "4.2 Script Dependencies\nAs previously mentioned, Python code includes dependencies on a set of Python packages (e.g. mikeio). If a script is run with a virtual environment that is missing these dependencies, there’ll be an error along the lines of: ModuleNotFoundError: No module named ‘mikeio’. The package listed in the error message (e.g. mikeio) needs to be installed before running the script.\n\n\n\n\n\n\nTip\n\n\n\nuv provides a way of defining dependencies within the script itself, such that they are automatically detected and installed when running the script with uv. Refer to uv’s documentation on script inline metadata for details.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Scripts</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_scripts.html#example---running-scripts",
    "href": "module1_foundations/python_scripts.html#example---running-scripts",
    "title": "4  Python Scripts",
    "section": "4.3 Example - running scripts",
    "text": "4.3 Example - running scripts",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Scripts</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html",
    "href": "module1_foundations/jupyter_notebooks.html",
    "title": "5  Jupyter Notebooks",
    "section": "",
    "text": "5.1 Comparison with Python Scripts\nA Jupyter Notebook is a file with the extension .ipynb that combines code, its output, and markdown into an interactive notebook-like experience.\nA key difference is that notebooks are interactive, whereas scripts simply run from start to end. Generally, notebooks are more useful for exploratory or visual workflows (e.g. making plots, or analyzing data). It’s also a great tool for learning Python.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#terminology",
    "href": "module1_foundations/jupyter_notebooks.html#terminology",
    "title": "5  Jupyter Notebooks",
    "section": "5.2 Terminology",
    "text": "5.2 Terminology\nThe following are fundamental concepts of Jupyter Notebooks:\n\nCell\n\nA Jupyter Notebook is a collection of cells.\n\nCode Cell\n\nA cell containing Python code, whose output shows below after execution.\n\nCell Output\n\nThe output after executing a cell, which could be many things (e.g. a number, plot, or table)\n\nMarkdown Cell\n\nA cell containing markdown for nicely formatted text.\n\nKernel\n\nResponsible for executing cells. Same as Python virtual environment for the purposes of this course.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#running-a-jupyter-notebook",
    "href": "module1_foundations/jupyter_notebooks.html#running-a-jupyter-notebook",
    "title": "5  Jupyter Notebooks",
    "section": "5.3 Running a Jupyter Notebook",
    "text": "5.3 Running a Jupyter Notebook\nThe Python extension for VS Code allows opening jupyter notebook files (.ipynb).\nUpon opening a notebook, all cells are displayed along with any saved output of those cells.\nRunning a notebook first requires selecting the kernel (i.e. the Python virtual environment). If the virtual environment has not installed the package ipykernel, then VS Code will ask to do that. Alternatively, manually install it via:\nuv pip install ipykernel\nNext, “Run All” to run all cells from top to bottom. It’s also possible to run (or re-run) cells individually in any order.\n\n\n\n\n\n\nTip\n\n\n\nIt’s good practice to organize notebooks such that they run from top to bottom.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#creating-a-jupyter-notebook",
    "href": "module1_foundations/jupyter_notebooks.html#creating-a-jupyter-notebook",
    "title": "5  Jupyter Notebooks",
    "section": "5.4 Creating a Jupyter Notebook",
    "text": "5.4 Creating a Jupyter Notebook\nCreate a Jupyter Notebook from within VS Code by opening the Command Palette (CTRL + SHIFT + P) and typing “Create: New Jupyter Notebook”.\nSave the notebook in a project folder to help VS Code automatically find the project’s virtual environment. Then, start adding and running cells.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#useful-keyboard-shortcuts",
    "href": "module1_foundations/jupyter_notebooks.html#useful-keyboard-shortcuts",
    "title": "5  Jupyter Notebooks",
    "section": "5.5 Useful Keyboard Shortcuts",
    "text": "5.5 Useful Keyboard Shortcuts\nThere’s a few useful keyboard shortcuts when working with notebooks:\n\nShift + Enter: Run the current cell and move to the next.\nCtrl + Enter: Run the current cell.\nA: Insert a new cell above.\nB: Insert a new cell below.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#additional-resources",
    "href": "module1_foundations/jupyter_notebooks.html#additional-resources",
    "title": "5  Jupyter Notebooks",
    "section": "5.6 Additional resources",
    "text": "5.6 Additional resources\nFor additional information, refer to VS Code’s documentation on jupyter notebooks.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#example---using-jupyter-notebooks",
    "href": "module1_foundations/jupyter_notebooks.html#example---using-jupyter-notebooks",
    "title": "5  Jupyter Notebooks",
    "section": "5.7 Example - Using Jupyter Notebooks",
    "text": "5.7 Example - Using Jupyter Notebooks",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html",
    "href": "module1_foundations/python_basics.html",
    "title": "6  Python Basics",
    "section": "",
    "text": "6.1 Using libraries\nThis section provides a crash course on basic Python concepts used throughout the course. It is purposefully brief, with additional resources provided at the end.\nMost functionality useful for MIKE+ modelling exists in Python packages (e.g. mikeio). Therefore, it’s important to understand how to access functionality in a Python package.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#using-libraries",
    "href": "module1_foundations/python_basics.html#using-libraries",
    "title": "6  Python Basics",
    "section": "",
    "text": "Note\n\n\n\nThe terms package, library, and module are used interchangeably throughout this course.\n\n\n\n6.1.1 Import libraries\nImport libraries using the import statement:\n\nimport math\n\nOr import specific functionality from a library:\n\nfrom math import sqrt\n\n\n\n6.1.2 Objects\nAll imports are objects containing some functionality. Objects have members accessible via the dot notation:\n\nmath.pi\n\n3.141592653589793\n\n\nDot accessors can be chained together, since all members are also objects.\n\nmath.pi.is_integer()\n\nFalse\n\n\nThere are a few common types of objects to be aware of:\n\nModules: reusable code you can import into your program.\nClasses: templates for creating objects with specific properties and behaviors.\nFunctions / Methods: blocks of code that return a result.\nData: any stored information (e.g. numbers, text).\n\nSee the type of an object with:\n\ntype(math)\n\nmodule\n\n\nSee the members of an object with:\n\ndir(math)\n\nGet help for an object:\n\nhelp(math)\n\nGood libraries have documentation. For example, see the documentation for math.\n\n\n6.1.3 Using Functions / Methods\n\n\n\n\n\n\nNote\n\n\n\nThis course will use the terms ‘function’ and ‘method’ interchangeably.\n\n\nUse a function by invoking it with round brackets:\n\nsqrt(25)\n\n5.0\n\n\nBetween the brackets is the function arguments. There’s different ways of specifying arguments. For example, there could be a list of arguments:\n\nmath.pow(2, 3)\n\n8.0\n\n\n\n\n6.1.4 Using Classes\nSome library functionality is provided via a class that needs to be instantiated before using it.\nBelow, the Random class is instantiated and assigned to the identifier my_random for reference later on.\n\nfrom random import Random\nmy_random = Random()\n\nAn instantiation of a class is called an instance, and is also an object whose functionality is accessible with the dot notation:\n\nmy_random.random() # returns a random number\n\n0.24959114761204282",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#variables",
    "href": "module1_foundations/python_basics.html#variables",
    "title": "6  Python Basics",
    "section": "6.2 Variables",
    "text": "6.2 Variables\nStore data/objects in named variables by using the assignment operator =.\n\nresult = 1 + 1\nresult\n\n2\n\n\n\n\n\n\n\n\nNote\n\n\n\nA valid name must be used. In general, this means it must start with a letter or underscore.\n\n\nVariable names can be referenced anywhere after their definition.\n\nresult = result * 2\nresult\n\n4",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#collections",
    "href": "module1_foundations/python_basics.html#collections",
    "title": "6  Python Basics",
    "section": "6.3 Collections",
    "text": "6.3 Collections\nA common need is to have a collection of related data. Perhaps the most common type of collection is a list, which is briefly introduced below.\nCreate a list with square brackets. Optionally include comma separated elements, otherwise an empty list is created.\n\nmy_numbers = [1, 2, 3]\nmy_numbers\n\n[1, 2, 3]\n\n\nAppend elements to an existing list.\n\nmy_numbers.append(4)\n\nAccess a specific element by indexing the list with the zero-based index. Zero refers to the first element, one the second, and so on.\n\nmy_numbers[0]\n\n1\n\n\nAccess a subset of a list by slicing it. The example below accesses elements with index 0 up to, but excluding, 2.\n\nmy_numbers[0:2]\n\n[1, 2]",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#control-logic",
    "href": "module1_foundations/python_basics.html#control-logic",
    "title": "6  Python Basics",
    "section": "6.4 Control Logic",
    "text": "6.4 Control Logic\nControl logic allows the flow of a program to be controlled via boolean conditions.\n\n6.4.1 Conditional statements\nUse if statements to execute code only if the specified condition is true.\n\nif 100 &gt; 10:\n    print(\"100 is greater than 10\")\n\n100 is greater than 10\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code that the if statement applies to is called a block, which must be indented.\n\n\nUse else statements after an if statement to execute code only if the condition is untrue.\n\nif 100 &lt; 10:\n    print(\"100 is less than 10\")\nelse:\n    print(\"of course, 100 is not less than 10\")\n\nof course, 100 is not less than 10\n\n\n\n\n6.4.2 Loops\nA while loop continuously executes a block of code while the specified condition is true.\n\ni = 0\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\n\n0\n1\n2\n\n\nA for loop executes a block of code per element in a specified collection.\n\nfor fruit in [\"Apple\", \"Banana\", \"Orange\"]:\n    print(fruit)\n\nApple\nBanana\nOrange",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#additional-resources",
    "href": "module1_foundations/python_basics.html#additional-resources",
    "title": "6  Python Basics",
    "section": "6.5 Additional resources",
    "text": "6.5 Additional resources\nLearning Python should be a continuous endeavor through practice. Luckily there’s an abundance of high quality resources online. Here’s a few examples:\n\nOfficial Python Documentation\nLearn X in Y minutes\nFreeCodeCamp: Scientific Computing with Python\n\n\n\n\n\n\n\nTip\n\n\n\nPython is used by a wide variety of domains (e.g. web development). Try to use resources specific for engineering/science applications for a more efficient learning path.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#example---using-pythons-interpreter",
    "href": "module1_foundations/python_basics.html#example---using-pythons-interpreter",
    "title": "6  Python Basics",
    "section": "6.6 Example - Using Python’s Interpreter",
    "text": "6.6 Example - Using Python’s Interpreter",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/llm_coding.html",
    "href": "module1_foundations/llm_coding.html",
    "title": "7  LLMs for Coding",
    "section": "",
    "text": "7.1 Ways of using LLMs\nLarge Language Models (LLMs) can significantly enhance coding efficiency. They’re also a great tool for explaining code, which is helpful for learning Python.\nLLMs for coding is an area under rapid development. Here are a few ways of using LLMs for coding, roughly in the order in which they became available for use:\nUsing LLMs is completely optional for the course. However, since GitHub Copilot is free and integrated with VS Code, our suggestion is to try it out as a learning assistant.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LLMs for Coding</span>"
    ]
  },
  {
    "objectID": "module1_foundations/llm_coding.html#ways-of-using-llms",
    "href": "module1_foundations/llm_coding.html#ways-of-using-llms",
    "title": "7  LLMs for Coding",
    "section": "",
    "text": "Chat interfaces via web (e.g. ChatGPT, Mistral AI)\nChat interfaces via an IDE (e.g. GitHub Copilot Chat)\nInline chat and autocompletion in IDE (e.g. GitHub Copilot)\nAgentic coding with specialized IDEs (e.g. Windsurf)",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LLMs for Coding</span>"
    ]
  },
  {
    "objectID": "module1_foundations/llm_coding.html#ideas-of-how-to-use-llms-in-coding",
    "href": "module1_foundations/llm_coding.html#ideas-of-how-to-use-llms-in-coding",
    "title": "7  LLMs for Coding",
    "section": "7.2 Ideas of how to use LLMs in coding",
    "text": "7.2 Ideas of how to use LLMs in coding\nA few ideas of how to use LLMs in coding:\n\nWrite scripts from scratch based on a description of what’s needed\nExplain a given script line by line to enhance understanding\nUnderstand cryptic error messages, and get potential solutions\nReview the quality of your code to see if it could be improved",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LLMs for Coding</span>"
    ]
  },
  {
    "objectID": "module1_foundations/llm_coding.html#example---andrej-karpathy-using-llms-for-coding",
    "href": "module1_foundations/llm_coding.html#example---andrej-karpathy-using-llms-for-coding",
    "title": "7  LLMs for Coding",
    "section": "7.3 Example - Andrej Karpathy using LLMs for coding",
    "text": "7.3 Example - Andrej Karpathy using LLMs for coding",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LLMs for Coding</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html",
    "href": "module1_foundations/scientific_python.html",
    "title": "8  Scientific Python",
    "section": "",
    "text": "8.1 Package ecosystem for scientific Python\nPython is a general purpose programming language that’s used by a broad range of domains. MIKE+ modelling workflows most closely align with the scientific python community.\nThere are several useful packages for engineering and science. This course will use the following packages:\nCheck out packages sponsored by NumFOCUS for an overview of useful libraries.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#package-ecosystem-for-scientific-python",
    "href": "module1_foundations/scientific_python.html#package-ecosystem-for-scientific-python",
    "title": "8  Scientific Python",
    "section": "",
    "text": "NumPy\nMatplotlib\npandas\n\n\n\n\n\n\n\n\nTip\n\n\n\nDHI builds their Python ecosystem on top of these packages, to enable better integration between them and allow scientists/engineers the flexibility that’s often required.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#numpy",
    "href": "module1_foundations/scientific_python.html#numpy",
    "title": "8  Scientific Python",
    "section": "8.2 NumPy",
    "text": "8.2 NumPy\nNumPy is a package that essentially enables faster numerical computing on large arrays than would otherwise be possible via Python collections. It is foundational to many other packages.\nNumPy is imported as np by convention:\n\nimport numpy as np\n\n\n\n\n\n\n\nNote\n\n\n\nImport as ‘np’ simply imports numpy and creates an alias for it as ‘np’.\n\n\nCreate a NumPy array from a Python collection:\n\nmy_array = np.array([1, 2, 3])\nmy_array\n\narray([1, 2, 3])\n\n\nUse vectorized operations on arrays. For example, multiply all elements of the previous array by 2:\n\nmy_array * 2\n\narray([2, 4, 6])\n\n\nIndex and slice arrays the same way as Python collections:\n\nmy_array[0]\n\nnp.int64(1)\n\n\nPerform aggregation functions on an array (e.g. sum, mean, max):\n\nmy_array.sum()\n\nnp.int64(6)\n\n\nRefer to NumPy’s official documentation for additional information.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#pandas",
    "href": "module1_foundations/scientific_python.html#pandas",
    "title": "8  Scientific Python",
    "section": "8.3 Pandas",
    "text": "8.3 Pandas\nPandas builds upon NumPy with a special focus on tabular data (like spreadsheets, or csv files).\nPandas is imported as ‘pd’ by convention:\n\nimport pandas as pd\n\nCreate a DataFrame, which is like a 2D labeled array (rows + columns):\n\nimport pandas as pd\ndata = [['Alice', 25], ['Bob', 30]]\ndf = pd.DataFrame(data, columns=['name', 'age'])\ndf\n\n\n\n\n\n\n\n\nname\nage\n\n\n\n\n0\nAlice\n25\n\n\n1\nBob\n30\n\n\n\n\n\n\n\nSelect a single column by name:\n\ndf['age']\n\n0    25\n1    30\nName: age, dtype: int64\n\n\nPerform aggregation operations just like as with NumPy:\n\ndf['age'].mean()\n\nnp.float64(27.5)\n\n\nImport data from a csv file into a pandas DataFrame:\n\nrainfall = pd.read_csv('data/fake_daily_rainfall.csv', index_col='date')\nrainfall.head()\n\n\n\n\n\n\n\n\nrainfall_mm\n\n\ndate\n\n\n\n\n\n2025-06-01\n17.450712\n\n\n2025-06-02\n7.926035\n\n\n2025-06-03\n19.715328\n\n\n2025-06-04\n32.845448\n\n\n2025-06-05\n6.487699\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the head method of a DataFrame to view the first five rows of very long DataFrames.\n\n\nCreate plots from a DataFrame:\n\nrainfall.plot(kind='bar')\n\n\n\n\n\n\n\n\nExport a DataFrame to csv, excel, or other formats:\n\nrainfall.to_csv(\"temp.csv\")\nrainfall.to_excel(\"temp.xlsx\")\n\nRefer to pandas’s official documentation for additional information.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#matplotlib",
    "href": "module1_foundations/scientific_python.html#matplotlib",
    "title": "8  Scientific Python",
    "section": "8.4 Matplotlib",
    "text": "8.4 Matplotlib\nMatplotlib is a library for creating plots and is commonly used by other libraries.\nMatplotlib is imported as ‘plt’ by convention:\n\nimport matplotlib.pyplot as plt\n\nCreate a simple line plot:\n\n# Create some data\nx = np.array([1, 2, 3, 4, 5])\ny = x ** 2\n\n# Make the plot\nplt.plot(x, y)              # Plots x vs y\nplt.title(\"My plot\")        # Gives a title to the plot\nplt.xlabel(\"X Axis\")        # Labels the x-axis\nplt.ylabel(\"Y Axis\")        # Labels the y-axis\nplt.grid()                  # Turns on grid lines\n\n\n\n\n\n\n\n\nRefer to Matplotlib’s official documentation for additional information.\nAlso, feel free to check out their example gallery for a sense of what’s possible.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#example---importing-and-plotting-a-time-series-csv-file",
    "href": "module1_foundations/scientific_python.html#example---importing-and-plotting-a-time-series-csv-file",
    "title": "8  Scientific Python",
    "section": "8.5 Example - Importing and Plotting a Time Series CSV File",
    "text": "8.5 Example - Importing and Plotting a Time Series CSV File",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/homework.html",
    "href": "module1_foundations/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Exercise 1\n\nCreate a GitHub account.\nFind and explore the mikeio1d repository. Can you find its documentation?\nWhat’s the current version of mikeio1d?\nSearch around GitHub and star some repositories you think are cool.\n\nExercise 2\n\nMake a new folder somewhere on your PC.\nOpen the folder in Visual Studio Code.\nCreate a virtual environment in that folder using uv from VS Code’s terminal.\nInstall mikeio1d in the virtual environment using uv.\nList all the packages in the virtual environment. Do you recognize any?\nSelect the Python Interpreter in VS Code to be the virtual environment you created.\n\nExercise 3\n\nFrom VS Code, create a new .py file under the project folder created in exercise two.\nCopy the following code into the script:\n\n\nimport mikeio1d\n\nprint(\"I'm a script that uses mikeio1d version \" + mikeio1d.__version__)\n\n\nRun the script from VS Code’s terminal using uv.\nRun the script from VS Code’s user interface (i.e. via the ‘Run’ menu).\nDo you get the same output for steps 3 and 4?\n\nExercise 4\n\nInstall ipykernel into the same virtual environment of the previous exercises.\nCreate a new Jupyter Notebook from within VS Code.\nMake sure the kernel matches your virtual environment, otherwise update it.\nPaste the code from exercise three into a code cell.\nRun the cell created in the previous step. Does the output match that of exercise three?\n\nExercise 5\n\nInstall the package cowsay into your virtual environment.\nCreate a new script, and import the function cow from cowsay.\nMake a list containing the names of three countries you want to visit.\nLoop over the list, and invoke the function cow by passing the current element of the list.\nRun the script. What do you see?\nTry to get the same output in a jupyter notebook by using two code cells.\n\nExercise 6\n\nDownload this time series csv file into your project folder.\nInstall pandas and matplotlib into your virtual environment.\nCreate a new Jupyter Notebook and import pandas\nLoad the downloaded csv file into a DataFrame using pandas.\nCalculate the minimum, mean, and maximum values.\nPlot the DataFrame. Do the values calculated from the previous step make sense?\n\nPractice Exercises (optional)\n\nJupyter Notebook covering Python basics\nJupyter Notebook covering NumPy\nJupyter Notebook covering Pandas\nJupyter Notebook covering Matplotlib",
    "crumbs": [
      "Module 1 - Foundations",
      "Homework"
    ]
  },
  {
    "objectID": "module2_time_series/index.html",
    "href": "module2_time_series/index.html",
    "title": "Welcome to Module 2!",
    "section": "",
    "text": "This module launches you into the practical world of time series data, a fundamental component of nearly all MIKE+ modelling projects. Our focus is on empowering you to efficiently handle, analyze, and prepare time series for your MIKE+ workflows:\n\nConvert dfs0 to Pandas DataFrame\nSelect, resample, and clean data\nVisualize time series data\nConvert Pandas DataFrame to dfs0\n\nYou’ll build on your knowledge of Pandas, and be introduced to a new library: MIKE IO. Let’s go!\n\n\n\n\n\n\nWhere can I download sample data to follow along?\n\n\n\n\n\nAll of the sample data used in this module is available for download:\n\ndischarge.dfs0\nrain_events_2021_july.csv\nrain_events_2021_june.csv\nsingle_water_level.dfs0\nsirius_idf_rainfall.dfs0",
    "crumbs": [
      "Module 2 - Time Series",
      "Welcome to Module 2!"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html",
    "href": "module2_time_series/01_mikeio.html",
    "title": "9  MIKE IO",
    "section": "",
    "text": "9.1 What is MIKE IO?\nThis section introduces MIKE IO, a fundamental DHI Python package. You’ll learn what MIKE IO is, the scope of its usage in this course, how to install it, and grasp its basic concepts. We provide a brief overview here; the next section delves into more detail.\nMIKE IO is an open-source Python package developed by DHI, which you might recall from Module 1. It empowers modelers with full flexibility by bridging the gap between various MIKE file formats and Scientific Python’s rich and powerful package ecosystem.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#usage-in-course",
    "href": "module2_time_series/01_mikeio.html#usage-in-course",
    "title": "9  MIKE IO",
    "section": "9.2 Usage in course",
    "text": "9.2 Usage in course\nFor MIKE+ modelers, a key file format is dfs0, DHI’s standard for tabular time series data. You’re likely familiar with dfs0 files for storing data such as rainfall, water levels, or discharge.\nThis course primarily focuses on the dfs0 functionality of MIKE IO, since it’s most relevant for handling time series data in MIKE+. While MIKE IO also supports other formats like dfs2, dfsu, and mesh files, they are intermediate topics beyond the scope of this introductory course.\n\n\n\n\n\n\nAlternative ways of using MIKE IO\n\n\n\n\n\nIt’s important to note that this course primarily focuses on using MIKE IO to get time series data into Pandas DataFrames. This approach is chosen to:\n\nReduce the initial learning curve for Python beginners by leveraging Pandas skills.\nProvide a method that is sufficiently powerful for most common time series tasks in MIKE+ modelling.\n\nMIKE IO itself has a lot of other useful functionalities, especially for working directly with Dataset and DataArray objects, and for handling multidimensional data (like dfs2 or dfsu files). We encourage you to explore the official MIKE IO documentation after mastering the basics of Pandas.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#installation",
    "href": "module2_time_series/01_mikeio.html#installation",
    "title": "9  MIKE IO",
    "section": "9.3 Installation",
    "text": "9.3 Installation\nInstall MIKE IO with:\nuv pip install mikeio\n\n\n\n\n\n\nTip\n\n\n\nAlways check the official MIKE IO’s documentation for the most up-to-date installation instructions and information on the latest versions.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#quick-glance",
    "href": "module2_time_series/01_mikeio.html#quick-glance",
    "title": "9  MIKE IO",
    "section": "9.4 Quick glance",
    "text": "9.4 Quick glance\nLet’s take a quick look at some core MIKE IO objects and how to access them. When you read a dfs0 file, MIKE IO typically returns a Dataset object.\n\nimport mikeio\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\")\n\n\n\n\n\n\n\nNote\n\n\n\nExample dfs0 is from MIKE+ Example Project: Sirius.\n\n\nThis Dataset object, ds, holds the data and metadata. You can easily access its contents, such as the items:\n\nds.items\n\nThe time axis is a Pandas DatetimeIndex, shared between all items:\n\nds.time\n\nTo access data for a specific item, you can select it from the Dataset, which returns a DataArray object:\n\nda = ds[0] # Access the first item from ds.items",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#key-concepts",
    "href": "module2_time_series/01_mikeio.html#key-concepts",
    "title": "9  MIKE IO",
    "section": "9.5 Key Concepts",
    "text": "9.5 Key Concepts\nUnderstanding a few key concepts in MIKE IO will be helpful as you progress through this course:\n\n\nDataset\n\nA Dataset is a collection of one or more DataArray objects that share the same time axis. Think of it as the entire content of a dfs0 file.\n\nDataArray\n\nA DataArray holds the data for a single item, including its time series values and associated metadata. This is comparable to a single column in a dfs0 file when viewed in a tabular format.\n\nItems\n\nEach DataArray within a Dataset represents an “item.” An item is characterized by its name, type (e.g., water level, discharge), unit (e.g., meters, m\\(^3\\)/s), and value type (e.g., instantaneous, accumulated), which are crucial for correct data interpretation in MIKE software.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#additional-reading-optional",
    "href": "module2_time_series/01_mikeio.html#additional-reading-optional",
    "title": "9  MIKE IO",
    "section": "9.6 Additional reading (optional)",
    "text": "9.6 Additional reading (optional)\nThe following sections of MIKE IO’s documentation are particularly relevant for this course:\n\nGetting started\nData Structures\nDataArray\nDataset\nDfs0\nEUM (Units and Types)",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html",
    "href": "module2_time_series/02_reading_dfs0.html",
    "title": "10  Reading dfs0",
    "section": "",
    "text": "10.1 Workflow\nThis section guides you through loading time series data from dfs0 files into Pandas DataFrames. This approach allows you to leverage your existing Pandas skills, learned in Module 1, for powerful time series analysis and manipulation.\nThe general workflow for working with dfs0 data is as follows:",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html#workflow",
    "href": "module2_time_series/02_reading_dfs0.html#workflow",
    "title": "10  Reading dfs0",
    "section": "",
    "text": "Read dfs0 file into Dataset\nSubset Dataset for specific items and times (optional)\nConvert Dataset (or DataArray) to DataFrame\nPerform some additional analysis via the DataFrame",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html#datasets",
    "href": "module2_time_series/02_reading_dfs0.html#datasets",
    "title": "10  Reading dfs0",
    "section": "10.2 Datasets",
    "text": "10.2 Datasets\nThe primary function for reading MIKE IO files is mikeio.read(). It returns a Dataset object, which is a container for one or more DataArray objects (e.g. a specific time series).\nReading a dfs0 file into a Dataset is as simple as calling the read() method with the dfs0 file path as the argument:\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:22)\ntime: 2019-01-01 00:00:00 - 2019-01-02 00:00:00 (22 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=20 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  1:   F=10 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  2:   F=5 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  3:   F=2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  4:   F=1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  5:   F=0.5 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  6:   F=0.2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  7:   F=0.1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  8:   F=0.05 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n\n\nNotice the representation of the Dataset object shows information about:\n\nTotal number of time steps\nTimestamps for first and last time step\nAll the items (i.e. DataArrays) available\n\n\n\n\n\n\n\nRead() loads entire dfs0 into memory by default\n\n\n\n\n\nBy default, mikeio.read() loads the entire dfs0 file into memory. This is fine for smaller files, but for very large dfs0 files, you might want to load only specific items or a particular time range to conserve memory and improve performance. You can do this directly with the items or time arguments in the read() function. For example, to read only the first item (index 0):\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\", items=0)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:22)\ntime: 2019-01-01 00:00:00 - 2019-01-02 00:00:00 (22 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=20 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n\n\nSimilarly, to read only the data for the first time step (index 0):\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\", time=0)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: ()\ntime: 2019-01-01 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=20 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  1:   F=10 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  2:   F=5 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  3:   F=2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  4:   F=1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  5:   F=0.5 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  6:   F=0.2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  7:   F=0.1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  8:   F=0.05 &lt;Rainfall Intensity&gt; (mm per hour) - 3",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html#dataarrays",
    "href": "module2_time_series/02_reading_dfs0.html#dataarrays",
    "title": "10  Reading dfs0",
    "section": "10.3 DataArrays",
    "text": "10.3 DataArrays\nDataArray objects are accessed via the Dataset object after reading.\nSelect a specific DataArray from the Dataset by its index or its name. For example, to select the first DataArray by its index:\n\nda = ds[0]\nda\n\n&lt;mikeio.DataArray&gt;\nname:  F=20\ndims: ()\ntime: 2019-01-01 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nvalues: 0.0\n\n\nAlternatively, select a DataArray by its name using square brackets:\n\nda = ds[\" F=20\"]\nda\n\n&lt;mikeio.DataArray&gt;\nname:  F=20\ndims: ()\ntime: 2019-01-01 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nvalues: 0.0\n\n\nNotice the representation of the DataArray object is also informative, just like the Dataset object.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html#convert-to-pandas",
    "href": "module2_time_series/02_reading_dfs0.html#convert-to-pandas",
    "title": "10  Reading dfs0",
    "section": "10.4 Convert to Pandas",
    "text": "10.4 Convert to Pandas\nYou can convert an entire Dataset (which might contain multiple time series) into a Pandas DataFrame. Each item in the Dataset will become a column in the DataFrame.\n\nimport pandas as pd\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\")\ndf = ds.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nF=20\nF=10\nF=5\nF=2\nF=1\nF=0.5\nF=0.2\nF=0.1\nF=0.05\n\n\n\n\n2019-01-01 00:00:00\n0.00\n0.000000\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n\n\n2019-01-01 06:00:00\n0.15\n0.283333\n0.4\n0.4\n0.466667\n0.683333\n0.966667\n1.316667\n1.4\n\n\n2019-01-01 07:00:00\n0.20\n0.400000\n0.6\n0.6\n0.800000\n1.100000\n1.600000\n2.200000\n2.3\n\n\n2019-01-01 08:00:00\n0.30\n0.600000\n0.7\n0.8\n0.900000\n1.400000\n2.000000\n2.800000\n3.1\n\n\n2019-01-01 09:00:00\n0.30\n0.600000\n0.9\n1.0\n1.200000\n1.800000\n2.600000\n3.600000\n4.0\n\n\n\n\n\n\n\nSimilarly, a single DataArray can be converted to a Pandas DataFrame (which will have one data column).\n\nda = ds[\" F=20\"]\ndf_T20 = da.to_dataframe()\ndf_T20.head()\n\n\n\n\n\n\n\n\nF=20\n\n\n\n\n2019-01-01 00:00:00\n0.00\n\n\n2019-01-01 06:00:00\n0.15\n\n\n2019-01-01 07:00:00\n0.20\n\n\n2019-01-01 08:00:00\n0.30\n\n\n2019-01-01 09:00:00\n0.30\n\n\n\n\n\n\n\nOnce your data is in a DataFrame, you can use all of Pandas’ powerful methods. For instance, you can easily plot a time series:\n\ndf_T20.plot(\n    title=\"Rainfall for Return Period F=20\",\n    ylabel=\"Rainfall (mm/hr)\"\n)\n\n\n\n\n\n\n\n\nOr get some descriptive statistics:\n\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nF=20\n22.0\n2.158333\n4.824954\n0.0\n0.300\n0.60\n1.80\n22.799999\n\n\nF=10\n22.0\n4.039394\n9.101874\n0.0\n0.525\n1.10\n3.00\n43.200001\n\n\nF=5\n22.0\n5.890909\n13.389296\n0.0\n0.725\n1.60\n4.65\n63.599998\n\n\nF=2\n22.0\n9.408333\n17.422466\n0.0\n0.825\n2.00\n8.40\n75.599998\n\n\nF=1\n22.0\n12.641667\n25.616199\n0.0\n0.950\n2.40\n9.75\n114.000000\n\n\nF=0.5\n22.0\n15.833333\n30.442241\n0.0\n1.425\n3.40\n13.50\n134.399994\n\n\nF=0.2\n22.0\n22.475000\n36.939988\n0.0\n2.075\n5.30\n23.40\n151.199997\n\n\nF=0.1\n22.0\n27.500758\n39.726610\n0.0\n2.900\n7.60\n34.05\n153.600006\n\n\nF=0.05\n22.0\n30.627272\n42.803784\n0.0\n3.200\n8.65\n39.75\n158.399994\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Python’s scientific ecosystem pays off…\n\n\n\nNotice that using a common structure for data (e.g. DataFrame) unlocks familiar analyses independent of the original data source file format (e.g. dfs0, csv). This is an example of why converting data into a format compatible with the scientific Python ecosystem is useful.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html",
    "href": "module2_time_series/03_data_selection.html",
    "title": "11  Data Selection",
    "section": "",
    "text": "11.1 Why subset data?\nThis section explores how to select specific subsets of time series data.\nSelecting a subset of data is useful for:",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#why-subset-data",
    "href": "module2_time_series/03_data_selection.html#why-subset-data",
    "title": "11  Data Selection",
    "section": "",
    "text": "focusing analysis on data of interest (e.g. specific item or time range)\nreducing memory usage and computational overhead (helpful for large files)\ngenerating relevant illustrations (plots and table views)",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#alternative-methods",
    "href": "module2_time_series/03_data_selection.html#alternative-methods",
    "title": "11  Data Selection",
    "section": "11.2 Alternative Methods",
    "text": "11.2 Alternative Methods\nThere are various ways of selecting subsets of dfs0 data. This section covers two different approaches:\n\nUsing mikeio.read()\nUsing Pandas DataFrame\n\nAs mentioned, MIKE IO also provides additional functionality for selecting subsets, however this course focuses on Pandas for simplicity.\n\n\n\n\n\n\nMemory considerations\n\n\n\n\n\nSelecting data via the read() method is generally most performant, since it will avoid loading the entire file into memory. Selecting data via Dataset, DataArray, and DataFrame objects requires first loading the entire file into memory.\nA dfs0 file is a special case where the entire file is loaded into memory regardless, however that will not be the case for other dfs formats (e.g. dfs2, dfsu). Therefore, it’s a good practice to use the read() method when you know which data you want in advance.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#selecting-items",
    "href": "module2_time_series/03_data_selection.html#selecting-items",
    "title": "11  Data Selection",
    "section": "11.3 Selecting Items",
    "text": "11.3 Selecting Items\nWhen reading data with mikeio.read(), the items argument lets you specify which items to load. You can do this by providing a list of item names.\n\nds = mikeio.read(\n    \"data/sirius_idf_rainfall.dfs0\", \n    items=[\" F=1\", \" F=2\"]\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:22)\ntime: 2019-01-01 00:00:00 - 2019-01-02 00:00:00 (22 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  1:   F=2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n\n\nAlternatively, you can select items using their numerical indices (zero-based). For example, to load the first and third items:\n\nds = mikeio.read(\n    \"data/sirius_idf_rainfall.dfs0\",\n    items=[4, 3]\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:22)\ntime: 2019-01-01 00:00:00 - 2019-01-02 00:00:00 (22 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  1:   F=2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsing item indices can be convenient, especially for quick explorations. However, specifying item names explicitly makes your code more readable and robust to changes in the dfs0 file structure, such as if items are reordered.\n\n\nFrom a Pandas DataFrame, you can select items using standard Pandas column selection techniques.\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\")\ndf = ds.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nF=20\nF=10\nF=5\nF=2\nF=1\nF=0.5\nF=0.2\nF=0.1\nF=0.05\n\n\n\n\n2019-01-01 00:00:00\n0.00\n0.000000\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n\n\n2019-01-01 06:00:00\n0.15\n0.283333\n0.4\n0.4\n0.466667\n0.683333\n0.966667\n1.316667\n1.4\n\n\n2019-01-01 07:00:00\n0.20\n0.400000\n0.6\n0.6\n0.800000\n1.100000\n1.600000\n2.200000\n2.3\n\n\n2019-01-01 08:00:00\n0.30\n0.600000\n0.7\n0.8\n0.900000\n1.400000\n2.000000\n2.800000\n3.1\n\n\n2019-01-01 09:00:00\n0.30\n0.600000\n0.9\n1.0\n1.200000\n1.800000\n2.600000\n3.600000\n4.0\n\n\n\n\n\n\n\nTo select a single item:\n\ndf[[\" F=20\"]].head()\n\n\n\n\n\n\n\n\nF=20\n\n\n\n\n2019-01-01 00:00:00\n0.00\n\n\n2019-01-01 06:00:00\n0.15\n\n\n2019-01-01 07:00:00\n0.20\n\n\n2019-01-01 08:00:00\n0.30\n\n\n2019-01-01 09:00:00\n0.30\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIndexing with a list returns another DataFrame, whereas indexing with a single value returns a Series.\n\n\nFor multiple items, provide a list of column names:\n\ndf[[\" F=1\", \" F=2\"]].head()\n\n\n\n\n\n\n\n\nF=1\nF=2\n\n\n\n\n2019-01-01 00:00:00\n0.000000\n0.0\n\n\n2019-01-01 06:00:00\n0.466667\n0.4\n\n\n2019-01-01 07:00:00\n0.800000\n0.6\n\n\n2019-01-01 08:00:00\n0.900000\n0.8\n\n\n2019-01-01 09:00:00\n1.200000\n1.0",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#selecting-time-steps",
    "href": "module2_time_series/03_data_selection.html#selecting-time-steps",
    "title": "11  Data Selection",
    "section": "11.4 Selecting Time Steps",
    "text": "11.4 Selecting Time Steps\nWhen reading data with mikeio.read(), the time argument allows for various ways to specify the desired subset.\nYou can select by a single time step index (e.g., the first time step, index 0).\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=0\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: ()\ntime: 1993-12-02 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\nOr provide a list of indices for specific time steps (e.g., the first three time steps).\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=[0,1,2]\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3)\ntime: 1993-12-02 00:00:00 - 1993-12-02 01:00:00 (3 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\nYou can also use timestamp strings.\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=\"1993-12-02 00:00:00\"\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: ()\ntime: 1993-12-02 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\nSelect multiple timestamps with a more general string, such as all times on a specific date.\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=\"1993-12-03\"\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:48)\ntime: 1993-12-03 00:00:00 - 1993-12-03 23:30:00 (48 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\nTo specify a time range, use Python’s slice() object with start and end timestamps:\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=slice(\"1993-12-02 12:00\", \"1993-12-02 16:00\")\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:9)\ntime: 1993-12-02 12:00:00 - 1993-12-02 16:00:00 (9 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\n\n\n\n\n\n\nTip\n\n\n\nPython’s slice() method is versatile for defining ranges. While list-like slicing notation (e.g., time_series[start:end]) is common with Pandas DataFrames, slice(start, end) is the explicit way to create a slice object, often used in functions like mikeio.read().\n\n\nFrom a Pandas DataFrame, standard indexing and slicing techniques of the DatetimeIndex may be used.\nTo select by time step index, use .iloc.\n\nds = mikeio.read(\"data/single_water_level.dfs0\")\ndf = ds.to_dataframe()\ndf.iloc[[0]] \n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n-0.2689\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nProviding a list to iloc returns another DataFrame, whereas providing a single value returns a Series.\n\n\nFor the first three time steps:\n\ndf.iloc[0:3]\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n1993-12-02 01:00:00\n-0.3020\n\n\n\n\n\n\n\nFor selection by timestamp strings, use .loc.\n\ndf.loc[[\"1993-12-02 00:00:00\"]]\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n-0.2689\n\n\n\n\n\n\n\nTo select all data for a particular day:\n\ndf.loc[\"1993-12-03\"].head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-03 00:00:00\n0.0879\n\n\n1993-12-03 00:30:00\n0.0951\n\n\n1993-12-03 01:00:00\n0.0988\n\n\n1993-12-03 01:30:00\n0.0836\n\n\n1993-12-03 02:00:00\n0.0634\n\n\n\n\n\n\n\nAnd for a range between start and end timestamps:\n\ndf.loc[\"1993-12-02 12:00\":\"1993-12-02 16:00\"]\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 12:00:00\n-0.4590\n\n\n1993-12-02 12:30:00\n-0.4698\n\n\n1993-12-02 13:00:00\n-0.4812\n\n\n1993-12-02 13:30:00\n-0.4919\n\n\n1993-12-02 14:00:00\n-0.5012\n\n\n1993-12-02 14:30:00\n-0.4798\n\n\n1993-12-02 15:00:00\n-0.4486\n\n\n1993-12-02 15:30:00\n-0.4137\n\n\n1993-12-02 16:00:00\n-0.3772\n\n\n\n\n\n\n\nA key distinction in Pandas is between .iloc and .loc:\n\n.iloc is used for integer-location based indexing (by position, e.g., df.iloc[0] for the first row).\n.loc is used for label-based indexing (by index names or boolean arrays, e.g., df.loc['2023-01-01']).\n\nWhen working with time series data having a DatetimeIndex, .loc is particularly powerful as it allows you to use date/time strings for intuitive selections and slicing, as shown in the examples.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#example",
    "href": "module2_time_series/03_data_selection.html#example",
    "title": "11  Data Selection",
    "section": "11.5 Example",
    "text": "11.5 Example\nLet’s tie these concepts together with an example of plotting a subset of a dfs0 file.\n1. Read a specific item of the dfs0 file into a Dataset\n\nds = mikeio.read(\"data/single_water_level.dfs0\", items=\"ST 2: WL (m)\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:577)\ntime: 1993-12-02 00:00:00 - 1993-12-14 00:00:00 (577 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\n2. Convert to Pandas DataFrame:\n\ndf = ds.to_dataframe()\ndf\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n1993-12-02 01:00:00\n-0.3020\n\n\n1993-12-02 01:30:00\n-0.3223\n\n\n1993-12-02 02:00:00\n-0.3483\n\n\n...\n...\n\n\n1993-12-13 22:00:00\n-0.0462\n\n\n1993-12-13 22:30:00\n-0.0522\n\n\n1993-12-13 23:00:00\n-0.0619\n\n\n1993-12-13 23:30:00\n-0.0717\n\n\n1993-12-14 00:00:00\n-0.0814\n\n\n\n\n577 rows × 1 columns\n\n\n\n3. Filter the Pandas DataFrame for the time range of interest.\n\ndf = df.loc[\"1993-12-02 00:00\":\"1993-12-02 4:00\"]\ndf\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n1993-12-02 01:00:00\n-0.3020\n\n\n1993-12-02 01:30:00\n-0.3223\n\n\n1993-12-02 02:00:00\n-0.3483\n\n\n1993-12-02 02:30:00\n-0.3644\n\n\n1993-12-02 03:00:00\n-0.3778\n\n\n1993-12-02 03:30:00\n-0.3983\n\n\n1993-12-02 04:00:00\n-0.4192\n\n\n\n\n\n\n\n4. Plot\n\nax = df.plot()\nax.set_title(\"Water Level at Night\")\nax.set_ylabel(\"Water Level (m)\")\nax.grid(which=\"both\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPlot methods often return a Matplotlib Axes object, conventionally called ax. Use it to customize the plot before it’s displayed a Jupyter Cell.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html",
    "href": "module2_time_series/04_resampling.html",
    "title": "12  Resampling",
    "section": "",
    "text": "12.1 What is Resampling?\nResampling is a powerful technique for changing the frequency of time series data, a common task when working with MIKE+ model inputs or outputs.\nAt its core, resampling involves adjusting the time steps in your data. There are two main types:\nA prerequisite for resampling in Pandas is that the DataFrame must have a DatetimeIndex. This will be the case if it was created via MIKE IO’s to_dataframe() method.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#what-is-resampling",
    "href": "module2_time_series/04_resampling.html#what-is-resampling",
    "title": "12  Resampling",
    "section": "",
    "text": "Downsampling: reducing the frequency of data points (e.g., hourly to daily rainfall).\nUpsampling: increasing the frequency of data points (e.g., hourly to minutely discharge).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf a DataFrame’s index is time-like but not already a DatetimeIndex, you can usually convert it with:\ndf.index = pd.to_datetime(df.index)",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#why-resample",
    "href": "module2_time_series/04_resampling.html#why-resample",
    "title": "12  Resampling",
    "section": "12.2 Why Resample?",
    "text": "12.2 Why Resample?\nTwo common motivations for resampling time series data in MIKE+ modelling are:\n\nAligning Series: comparing time series that were recorded at different frequencies (e.g., aligning 15-minute model results with hourly observations).\nSmoothing Data: reducing noise to highlight underlying trends by aggregating data over longer periods (e.g., hourly average flow from noisy instantaneous readings).",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#how-to-resample-with-pandas",
    "href": "module2_time_series/04_resampling.html#how-to-resample-with-pandas",
    "title": "12  Resampling",
    "section": "12.3 How to resample (with Pandas)",
    "text": "12.3 How to resample (with Pandas)\nPandas provides a straightforward resample() method for time series data.\nThe general syntax is: df.resample(&lt;rule&gt;).&lt;aggregation_or_fill_method&gt;().\n\nrule\n\nA string specifying the target frequency (e.g., ‘D’ for daily).\n\naggregation_or_fill_method\n\nA function to apply to the data within each new time bin. For downsampling, this is typically an aggregation like .mean() or .sum(). For upsampling, this is a fill method like .ffill() or .interpolate().\n\n\nA quick example for illustration with the following DataFrame:\n\ndf.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n1993-12-02 01:00:00\n-0.3020\n\n\n1993-12-02 01:30:00\n-0.3223\n\n\n1993-12-02 02:00:00\n-0.3483\n\n\n\n\n\n\n\nTo resample this half-hourly data to daily mean values:\n\ndf_daily_mean = df.resample('D').mean()\ndf_daily_mean.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n-0.302979\n\n\n1993-12-03\n0.041185\n\n\n1993-12-04\n0.014558\n\n\n1993-12-05\n0.265933\n\n\n1993-12-06\n-0.004035\n\n\n\n\n\n\n\n\n\nShow Plotting Code\nax = df.plot()\ndf_daily_mean.plot(ax=ax)\nax.legend([\"Original\", \"Downsampled\"])",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#common-frequency-rules",
    "href": "module2_time_series/04_resampling.html#common-frequency-rules",
    "title": "12  Resampling",
    "section": "12.4 Common Frequency Rules",
    "text": "12.4 Common Frequency Rules\nPandas offers many frequency aliases (rules). Some of the most common include:\n\n\"M\": Month-end frequency\n\"W\": Weekly frequency (defaults to Sunday)\n\"D\": Calendar day frequency\n\"H\": Hourly frequency\n\"15min\": 15-minute frequency\n\n\n\n\n\n\n\nNote\n\n\n\nFor a comprehensive list of frequency strings (offset aliases), refer to the Pandas documentation on Time Series / Date functionality.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#aggregation-methods-downsampling",
    "href": "module2_time_series/04_resampling.html#aggregation-methods-downsampling",
    "title": "12  Resampling",
    "section": "12.5 Aggregation Methods (Downsampling)",
    "text": "12.5 Aggregation Methods (Downsampling)\nWhen downsampling, you are reducing the number of data points, so you need to decide how to aggregate the values within each new, larger time period. Common aggregation methods include:\n\n.mean(): calculate the average of the values.\n.sum(): calculate the sum of the values.\n.first(): select the first value in the period.\n.last(): select the last value in the period.\n.min(): find the minimum value.\n.max(): find the maximum value.\n\n\n\n\n\n\n\nTip\n\n\n\nThe choice of aggregation method depends on the nature of your data and what you want to represent. For instance, rainfall is often summed, while water levels or flows might be averaged.\n\n\nResample to daily values by choosing the maximum value on each day.\n\ndf_daily_max = df.resample('D').max()\ndf_daily_max.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n0.0799\n\n\n1993-12-03\n0.1486\n\n\n1993-12-04\n0.1583\n\n\n1993-12-05\n0.5106\n\n\n1993-12-06\n0.1793\n\n\n\n\n\n\n\nOr, choose the minimum value on each day.\n\ndf_daily_min = df.resample('D').min()\ndf_daily_min.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n-0.5012\n\n\n1993-12-03\n-0.0701\n\n\n1993-12-04\n-0.1112\n\n\n1993-12-05\n0.0524\n\n\n1993-12-06\n-0.1114\n\n\n\n\n\n\n\nCompare these two aggregation methods with a plot.\n\n\nShow Plotting Code\nax = df.plot(color='grey')\ndf_daily_mean.plot(ax=ax, linestyle=\"--\")\ndf_daily_min.plot(ax=ax, linestyle=\"--\")\ndf_daily_max.plot(ax=ax, linestyle=\"--\")\nax.legend([\"Original\", \"Downsample (mean)\", \"Downsample (min)\", \"Downsample (max)\"])\nax.grid(which=\"both\")",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#fill-methods-upsampling",
    "href": "module2_time_series/04_resampling.html#fill-methods-upsampling",
    "title": "12  Resampling",
    "section": "12.6 Fill Methods (Upsampling)",
    "text": "12.6 Fill Methods (Upsampling)\nWhen upsampling, you are increasing the number of data points, which means you’ll have new time steps with no existing data. You need to specify a method to fill these gaps. Common fill methods include:\n\n.ffill() (forward fill): propagate the last valid observation forward.\n.bfill() (backward fill): use the next valid observation to fill the gap.\n.interpolate(): fill nan values using an interpolation method (e.g., linear, spline).\n\n\n\n\n\n\n\nNote\n\n\n\nnan stands for ‘not a number’, which is a common way to represent missing values. See NumPy’s nan.\n\n\nRecall our original DataFrame had half-hourly time steps:\n\ndf.head(2) # show only first two rows\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n\n\n\n\n\nUpsample this to a resolution of one minute, comparing ffill and bfill:\n\ndf.resample(\"5min\").ffill().head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:05:00\n-0.2689\n\n\n1993-12-02 00:10:00\n-0.2689\n\n\n1993-12-02 00:15:00\n-0.2689\n\n\n1993-12-02 00:20:00\n-0.2689\n\n\n\n\n\n\n\n\ndf.resample(\"5min\").bfill().head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:05:00\n-0.2847\n\n\n1993-12-02 00:10:00\n-0.2847\n\n\n1993-12-02 00:15:00\n-0.2847\n\n\n1993-12-02 00:20:00\n-0.2847\n\n\n\n\n\n\n\nCompare the difference between these two. Find the new time stamps and how their values were chosen.\nDepending on use case, a more appropriate approach may be filling gaps with linear interpolation:\n\ndf_interpolated = df.resample('5min').interpolate(method='linear')\ndf_interpolated.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.268900\n\n\n1993-12-02 00:05:00\n-0.271533\n\n\n1993-12-02 00:10:00\n-0.274167\n\n\n1993-12-02 00:15:00\n-0.276800\n\n\n1993-12-02 00:20:00\n-0.279433\n\n\n\n\n\n\n\nCompare interpolation to original data for a zoomed-in time period:\n\n\nShow Plotting Code\nsubset = slice(\"1993-12-02 00:00:00\", \"1993-12-02 08:00:00\")\nax = df.loc[subset].plot(color=\"grey\", alpha=0.7, linewidth=8)\ndf_interpolated.loc[subset].plot(ax=ax, linestyle=\"--\")\nax.legend([\"Original\", \"Interpolated\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUpsampling should be done with caution, as it involves making assumptions about the data between known points. The choice of fill method can significantly impact the resulting time series.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html",
    "href": "module2_time_series/05_visualization.html",
    "title": "13  Data Visualization",
    "section": "",
    "text": "13.1 Why Visualize Data?\nVisualizing time series data is a critical step in any MIKE+ modelling workflow. Effective plots can help understand data quality, model behavior, the agreement between simulations and observations, as well as communicating key findings.\nVisual inspection of data serves several key purposes in the modelling process:",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html#why-visualize-data",
    "href": "module2_time_series/05_visualization.html#why-visualize-data",
    "title": "13  Data Visualization",
    "section": "",
    "text": "Validate input data: Quickly identify anomalies, gaps, or questionable patterns in input time series like rainfall, flow, or water levels.\nGrasp system behavior: Understand underlying trends, seasonality, and extreme events within your datasets.\nCalibrate/validate: Graphically compare simulated results against observed data to assess model performance.\nDiagnose model errors: Pinpoint discrepancies in timing, magnitude, or overall patterns between model output and reality.\nCommunicate results: Create clear visuals to share modelling outcomes, impacts of different scenarios, or model performance metrics with stakeholders.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html#simple-statistics",
    "href": "module2_time_series/05_visualization.html#simple-statistics",
    "title": "13  Data Visualization",
    "section": "13.2 Simple Statistics",
    "text": "13.2 Simple Statistics\nBefore diving into plots, it’s often useful to get a quick numerical summary of your data.\nAssuming you have a DataFrame df containing your time series with both observed and model data:\n\ndf.head()\n\n\n\n\n\n\n\n\nModel\nObserved\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n-0.219229\n\n\n1993-12-02 00:30:00\n-0.2847\n-0.298526\n\n\n1993-12-02 01:00:00\n-0.3020\n-0.237231\n\n\n1993-12-02 01:30:00\n-0.3223\n-0.169997\n\n\n1993-12-02 02:00:00\n-0.3483\n-0.371715\n\n\n\n\n\n\n\nThe describe() method provides useful statistics of each column in the DataFrame.\n\ndf.describe()\n\n\n\n\n\n\n\n\nModel\nObserved\n\n\n\n\ncount\n577.000000\n577.000000\n\n\nmean\n-0.005975\n-0.007724\n\n\nstd\n0.219331\n0.247389\n\n\nmin\n-0.501200\n-0.671723\n\n\n25%\n-0.136900\n-0.162345\n\n\n50%\n-0.000200\n-0.009162\n\n\n75%\n0.124900\n0.154040\n\n\nmax\n0.510600\n0.699579",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html#useful-time-series-plots",
    "href": "module2_time_series/05_visualization.html#useful-time-series-plots",
    "title": "13  Data Visualization",
    "section": "13.3 Useful Time Series Plots",
    "text": "13.3 Useful Time Series Plots\nThis section showcases a few useful plot types for time series data in MIKE+ modelling.\n\n13.3.1 Line Plots\nLine plots are essential for visualizing temporal patterns in hydraulic data like flows, water levels, or rainfall. They are also the primary way to compare simulated versus observed time series.\nYou can plot a single series directly from a DataFrame column:\n\ndf['Observed'].plot(\n    title='Observed Flow Over Time',\n    xlabel='Time',\n    ylabel='Flow (m$^3$/s)'\n)\n\n\n\n\n\n\n\n\nCompare two time series, such as observed and modelled flow:\n\ndf[['Observed', 'Model']].plot(\n    title='Flow Comparison: Observed vs. Model',\n    ylabel='Flow (m$^3$/s)'\n)\n\n\n\n\n\n\n\n\n\n\n13.3.2 Rolling Mean / Moving Average Plot\nThis plot helps smooth out noisy time series data, such as high-frequency sensor readings for flow or water level. This smoothing can make it easier to visualize underlying trends or long-term patterns.\n\ndf['Observed_Rolling_Mean'] = df['Observed'].rolling(window=6).mean()\n\ndf[['Observed', 'Observed_Rolling_Mean']].plot(\n    title='Observed 6-Hour Rolling Mean',\n    ylabel='Flow (m$^3$/s)'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAdjust the window size in the .rolling() method to control the amount of smoothing. Larger windows result in smoother trends but might obscure shorter-term variations.\n\n\n\n\n13.3.3 Scatter Plots\nScatter plots are particularly useful for model calibration. By plotting paired observed values against simulated values, you can assess point-by-point agreement.\n\nax = df.plot.scatter(\n    x='Observed',\n    y='Model',\n    alpha=0.5, # so we can see overlapping points better\n    title='Observed vs. Model'\n)\n\n# plot 1:1 line\nmax_val = max(df['Observed'].max(), df['Model'].max())\nmin_val = min(df['Observed'].min(), df['Model'].min())\nax.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 Line')  # black dashed line\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPoints clustering around the 1:1 line (the dashed line in the example) indicate good agreement.\n\n\n\n\n13.3.4 Box Plots\nTo understand seasonal variability in your data (e.g. diurnal or seasonal flow patterns), box plots can be effective.\n\ndf['Day'] = df.index.day_name()\nax = df.boxplot(column='Observed', by='Day')\nax.get_figure().suptitle(\"\") # remove figure title, just use axes title\nax.set_title(\"Flows by Day of Week\")\nax.set_ylabel(\"Water level (m)\")\n\nText(0, 0.5, 'Water level (m)')\n\n\n\n\n\n\n\n\n\n\n\n13.3.5 Cumulative Sum Plots\nCumulative sum plots are excellent for assessing overall water balance or comparing total accumulated volumes (e.g., rainfall, runoff) between observed and simulated data over a period.\n\ndf_discharge.cumsum().plot(ylabel=\"Cumulative Discharge (m$^3$/s)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCumulative sums for mass balance require calculating volume differentials. The example below is a simplified approach, recognizing they time series share the same time axis.\n\n\n\n\n13.3.6 Distribution Plots\nHistograms help you examine the frequency distribution of variables, such as water levels or flows. This can be useful for comparing the overall statistical profile of observed versus simulated data or understanding the prevalence of certain magnitudes.\n\ndf.plot.hist(bins=15, alpha=0.5)\n\n\n\n\n\n\n\n\nSimilarly, review frequency distribution with KDE plots:\n\ndf.plot.kde()",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html#saving-plots",
    "href": "module2_time_series/05_visualization.html#saving-plots",
    "title": "13  Data Visualization",
    "section": "13.4 Saving Plots",
    "text": "13.4 Saving Plots\nEasily save plots for inclusion in reports via plt.savefig().\n\nimport matplotlib.pyplot as plt\n\nax = df['Observed'].plot(title='Daily Average Flow', ylabel=\"Flow ($m^3/s$)\")\nplt.savefig(\"my_plot.png\")",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/06_data_cleaning.html",
    "href": "module2_time_series/06_data_cleaning.html",
    "title": "14  Data Cleaning",
    "section": "",
    "text": "14.1 Missing Values\nData cleaning is an essential step in any MIKE+ modelling workflow to ensure your input data is complete. This section covers handling missing values (e.g. nan). Additionally, it introduces the topic of detecting anomalies in time series data.\nDHI’s modelling engines typically require complete datasets for calculations, and thus dfs0 files, which are often used as inputs, should not contain missing values. For example, a rainfall boundary condition cannot have the value nan.\nAssume we have a DataFrame with missing values on 1993-12-06:\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\nax.legend(loc=\"upper right\")\nCount the number of missing values (e.g. nan) for each time series by summing the result of isna().\ndf.isna().sum()\n\nST 2: WL (m)    48\ndtype: int64",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "module2_time_series/06_data_cleaning.html#missing-values",
    "href": "module2_time_series/06_data_cleaning.html#missing-values",
    "title": "14  Data Cleaning",
    "section": "",
    "text": "Note\n\n\n\nMissing numerical data is typically represented by nan. These arise from various sources, such as sensor malfunctions during data collection, gaps that occur during data transmission, or they might be the result of previous data processing or cleaning steps.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "module2_time_series/06_data_cleaning.html#imputation",
    "href": "module2_time_series/06_data_cleaning.html#imputation",
    "title": "14  Data Cleaning",
    "section": "14.2 Imputation",
    "text": "14.2 Imputation\nThe process of filling missing values is known as imputation.\nFor missing values between valid data points (i.e. bounded), using the .interpolate() method is a common and effective approach.\n\ndf_interpolated = df.interpolate(method='time')\n\n\n\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\ndf_interpolated.columns = [\"Interpolation\"]\ndf_interpolated.loc[\"1993-12-06\"].plot(ax=ax)\nax.legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe example above uses method='time', which is a linear interpolation that considers non-equidistant DatetimeIndex indices. Refer to Pandas’s documentation for additional interpolation methods, such as polynomial.\n\n\nFor missing values appearing at the very beginning or end of your dataset (i.e. unbounded), you can make use of:\n\n.fillna()\n.ffill()\n.bfill()\n\n\n\n\n\n\n\nTip\n\n\n\nRecall: these imputation methods were introduced in the section on resampling, where upsampling introduced nan values.\n\n\nSame example as above, but using ffill().\n\ndf_interpolated = df.ffill()\n\n\n\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\ndf_interpolated.columns = [\"Interpolation\"]\ndf_interpolated.loc[\"1993-12-06\"].plot(ax=ax)\nax.legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\nSame example as above, but using bfill().\n\ndf_interpolated = df.bfill()\n\n\n\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\ndf_interpolated.columns = [\"Interpolation\"]\ndf_interpolated.loc[\"1993-12-06\"].plot(ax=ax)\nax.legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\nSame example as above, but using fillna().\n\ndf_interpolated = df.fillna(0.1) # specify the value to fill with\n\n\n\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\ndf_interpolated.columns = [\"Interpolation\"]\ndf_interpolated.loc[\"1993-12-06\"].plot(ax=ax)\nax.legend(loc=\"upper right\")",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "module2_time_series/06_data_cleaning.html#anomaly-detection-rule-based",
    "href": "module2_time_series/06_data_cleaning.html#anomaly-detection-rule-based",
    "title": "14  Data Cleaning",
    "section": "14.3 Anomaly Detection (Rule-Based)",
    "text": "14.3 Anomaly Detection (Rule-Based)\n\n\n\n\n\n\nTip\n\n\n\nShort on time? This section provides an introduction to a useful package but can be considered optional for core module understanding.\n\n\nBeyond clearly missing values, time series data can also contain anomalies. Identifying and addressing these anomalies is crucial for building robust MIKE+ models.\nAnomaly detection is a broad and complex field. This section offers a basic introduction to rule-based anomaly detection using DHI’s tsod Python package.\n\n14.3.1 Install tsod\nuv pip install tsod\n\n\n14.3.2 The Detector Concept\ntsod operates using a concept called “detectors.” Each detector is designed to implement a specific rule or heuristic to identify anomalies. Example anomaly detectors:\n\nRangeDetector: Flags values outside a set range.\nConstantValueDetector: Detects unchanging values over time.\nDiffDetector: Catches large changes between points.\nRollingStdDetector: Finds points far from rolling standard deviation.\n\nThere’s also the CombinedDetector, which allows combining the rules of several detectors.\n\n\n14.3.3 Detecting Anomalies\nPlot the initial time series.\n\nts = df[\"ST 2: WL (m)\"]\nts.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ntsod operates on Series. Select the subject Series from the DataFrame object as needed.\n\n\nSelect and instantiate a detector. If we know water levels must be in the range -0.4m to 0.4m, then a RangeDetector should be used.\n\nfrom tsod.detectors import RangeDetector\n\ndetector = RangeDetector(\n    min_value = -0.4,\n    max_value = 0.4\n)\ndetector\n\nRangeDetector(min: -4.0e-01, max: 4.0e-01)\n\n\nDetect anomalies for a given Series using the detect() method of the instantiated detector.\n\nanomaly_mask = detector.detect(ts)\nanomaly_mask.head()\n\n1993-12-02 00:00:00    False\n1993-12-02 00:30:00    False\n1993-12-02 01:00:00    False\n1993-12-02 01:30:00    False\n1993-12-02 02:00:00    False\nFreq: 30min, Name: ST 2: WL (m), dtype: bool\n\n\n\n\n\n\n\n\nNote\n\n\n\nA mask refers to a boolean indexer. In the example above, values are true for anomalies and false otherwise.\n\n\nPlot the detected anomalies.\n\nax = ts.plot()\nts[anomaly_mask].plot(\n    ax=ax,\n    style='ro',\n    label=\"Anomaly\",\n    alpha=0.5\n)\nax.legend()\n\n# horizontal lines to validate ranges\nax.axhline(0.4, color='grey', alpha=0.5)\nax.axhline(-0.4, color='grey', alpha=0.5)\n\n\n\n\n\n\n\n\nReplace anomalies with nan.\n\nimport numpy as np\n\nts_cleaned = ts.copy()\nts_cleaned[anomaly_mask] = np.nan\nts_cleaned.plot()\n\n\n\n\n\n\n\n\n\n\n14.3.4 Impute anomalies\nImpute anomalies by treating them just like missing values.\n\nts_cleaned = ts_cleaned.interpolate(method='time')\nts_cleaned.plot()",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html",
    "href": "module2_time_series/07_writing_dfs0.html",
    "title": "15  Writing dfs0",
    "section": "",
    "text": "15.1 Workflow\nCreating dfs0 files is a common need for MIKE+ modellers (e.g. rainfall from csv). This section focuses on how to create dfs0 files from a Pandas DataFrame.\nThe general workflow for creating dfs0 files from a DataFrame is as follows:",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#workflow",
    "href": "module2_time_series/07_writing_dfs0.html#workflow",
    "title": "15  Writing dfs0",
    "section": "",
    "text": "Map ItemInfo objects to each column (optional)\nCreate a Dataset object from the DataFrame\nSave the Dataset object to a dfs0 file.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#example-source-data",
    "href": "module2_time_series/07_writing_dfs0.html#example-source-data",
    "title": "15  Writing dfs0",
    "section": "15.2 Example Source Data",
    "text": "15.2 Example Source Data\nThe following DataFrame will be used in this section as an example.\n\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"data/rain_events_2021_july.csv\",\n    index_col=\"time\",\n    parse_dates=True\n)\ndf.head()\n\n\n\n\n\n\n\n\nrainfall\n\n\ntime\n\n\n\n\n\n2021-07-02 09:51:00\n0.000\n\n\n2021-07-02 09:52:00\n3.333\n\n\n2021-07-02 09:53:00\n0.333\n\n\n2021-07-02 09:54:00\n0.333\n\n\n2021-07-02 09:55:00\n0.333\n\n\n\n\n\n\n\nGet familiar with the data. Notice:\n\nThe time axis is non-equidistant\nValues represent rainfall depth since the last time step.\nRainfall events always start at values of zero.\n\n\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nrainfall\n1086.0\n1.120283\n3.219804\n0.0\n0.185\n0.37\n0.667\n36.667\n\n\n\n\n\n\n\n\ndf.plot()\n\n\n\n\n\n\n\n\n\ndf.plot.hist(bins=50)\n\n\n\n\n\n\n\n\nThis subset shows the division between two rainfall events:\n\ndf.loc[\"2021-07-25 17:32:00\":\"2021-07-25 20:30:00\"]\n\n\n\n\n\n\n\n\nrainfall\n\n\ntime\n\n\n\n\n\n2021-07-25 17:32:00\n0.476\n\n\n2021-07-25 17:33:00\n0.476\n\n\n2021-07-25 17:34:00\n0.476\n\n\n2021-07-25 20:28:00\n0.000\n\n\n2021-07-25 20:29:00\n3.333\n\n\n2021-07-25 20:30:00\n0.667",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#dataframe-to-dataset",
    "href": "module2_time_series/07_writing_dfs0.html#dataframe-to-dataset",
    "title": "15  Writing dfs0",
    "section": "15.3 DataFrame to Dataset",
    "text": "15.3 DataFrame to Dataset\nConverting a DataFrame to a Dataset is straightforward using mikeio.from_pandas():\n\nds = mikeio.from_pandas(df)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1086)\ntime: 2021-07-02 09:51:00 - 2021-07-31 13:16:00 (1086 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  rainfall &lt;Undefined&gt; (undefined)\n\n\n\n\n\n\n\n\nTip\n\n\n\nEnsure your DataFrame’s index is a DatetimeIndex for time series dfs0 files. This is crucial for MIKE IO to correctly interpret the time information.\n\n\nNotice that the item type and unit are “undefined”. Let’s inspect the ItemInfo MIKE IO used by default:\n\nitem = ds[0].item\nprint(f\"Item Name: {item.name}\")\nprint(f\"Item Type: {item.type.name}\")\nprint(f\"Item Unit: {item.unit.name}\")\nprint(f\"Item Data Value Type: {item.data_value_type.name}\")\n\nItem Name: rainfall\nItem Type: Undefined\nItem Unit: undefined\nItem Data Value Type: Instantaneous\n\n\nThis highlights the need to almost always define item metadata before calling from_pandas().\n\n\n\n\n\n\nCaution\n\n\n\nProviding accurate ItemInfo is key for ensuring compatibility with MIKE software and correctly interpreting the meaning of your data within the MIKE ecosystem.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#item-metadata",
    "href": "module2_time_series/07_writing_dfs0.html#item-metadata",
    "title": "15  Writing dfs0",
    "section": "15.4 Item Metadata",
    "text": "15.4 Item Metadata\nPlease review MIKE IO’s user guide on EUM before proceeding.\nCreate an ItemInfo object for our example rainfall data:\n\nitem = mikeio.ItemInfo(\n    name = \"Rainfall\",\n    itemtype = mikeio.EUMType.Rainfall_Depth,\n    unit = mikeio.EUMUnit.millimeter,\n    data_value_type= \"StepAccumulated\",\n)\nitem\n\nRainfall &lt;Rainfall Depth&gt; (millimeter) - 2\n\n\n\n\n\n\n\n\nData Value Type\n\n\n\n\n\nThe Data Value Type specifies how data values relate to time steps. Common options include:\n\nInstantaneous: Value at a specific point in time.\nAccumulated: Value aggregated over the entire period up to the timestamp.\nStepAccumulated: Value aggregated over the preceding time interval.\nMeanStepBackward: Average value over the preceding time interval.\n\nRefer to MIKE+ documentation for explanation of these options.\n\n\n\nLet’s recreate the Dataset using the ItemInfo object for our rainfall.\n\nds = mikeio.from_pandas(df, items=[item])\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1086)\ntime: 2021-07-02 09:51:00 - 2021-07-31 13:16:00 (1086 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  rainfall &lt;Rainfall Depth&gt; (millimeter) - 2\n\n\nNotice the item info is now correct on the Dataset.\n\n\n\n\n\n\nMapping items to column by name\n\n\n\n\n\nThe order of items matches the order of the DataFrame columns. You may prefer to explicitly name the columns:\n\nmikeio.from_pandas(df, items={\n    \"rainfall\" : item\n})\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1086)\ntime: 2021-07-02 09:51:00 - 2021-07-31 13:16:00 (1086 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  rainfall &lt;Rainfall Depth&gt; (millimeter) - 2",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#dataset-to-dfs0",
    "href": "module2_time_series/07_writing_dfs0.html#dataset-to-dfs0",
    "title": "15  Writing dfs0",
    "section": "15.5 Dataset to dfs0",
    "text": "15.5 Dataset to dfs0\nThe final step is to save your carefully prepared Dataset object, now containing the correct data and metadata, to a dfs0 file. This is done using the .to_dfs() method of the Dataset object.\n\nds.to_dfs(\"rainfall.dfs0\")\n\nThis will create a file named rainfall.dfs0 ready to be used in MIKE+.\nConfirm it worked by reading the dfs0 file back into a Dataset (optional).\n\nds_validation = mikeio.read(\"rainfall.dfs0\")\nds_validation\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1086)\ntime: 2021-07-02 09:51:00 - 2021-07-31 13:16:00 (1086 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  rainfall &lt;Rainfall Depth&gt; (millimeter) - 2",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/homework.html",
    "href": "module2_time_series/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Exercise 1\n\nDownload this dfs0 file into a new project folder.\nCreate an empty Jupyter Notebook and import mikeio.\nRead the dfs0 file into a Dataset object.\nConvert the Dataset object into a Pandas DataFrame object.\nCall the describe() method and review the statistics.\nIn the same notebook, select a subset of the items.\nExport the subset DataFrame to csv with Pandas (hint: to_csv())\n\nExercise 2\n\nDownload this dfs0 file into a new project folder.\nCreate an empty Jupyter Notebook and import mikeio.\nRead the dfs0 file into a Dataset, only including data between “1993-12-02 16:00” and “1993-12-02 20:00”.\nConvert the Dataset object into a Pandas DataFrame object.\nPlot the DataFrame using .plot().\nSelect the first 3 rows of the DataFrame in two different ways: using iloc and using loc.\n\nExercise 3\n\nRepeat steps 1-2 of the previous exercise.\nRead the dfs0 file into a Dataset object, then convert it to a DataFrame.\nResample the half-hourly data to minutely data (i.e. upsample) using time interpolation.\nResample the half-hourly data to hourly data (i.e downsample) using mean aggregation.\nTry 3-4 again, except choose a different fill/aggregation method. Compare the results.\n\nExercise 4\n\nDownload this dfs0 file into a new project folder.\nCreate an empty Jupyter Notebook and import mikeio.\nRead the dfs0 file into a Dataset object, then convert it to a DataFrame.\nCompare the observed and model values using a line plot, a scatter plot, and a histogram.\nSave the plots to a png file.\n\nExercise 5\n\nDownload this csv file into a new project folder.\nRead the csv file into a DataFrame using Pandas.\nCheck for nan values in the rainfall. How many missing values are there?\nFill the missing value(s) using an appropriate imputation method.\n\nExercise 6\n\nContinue from where you left off in the previous exercise.\nCreate an ItemInfo object for the rainfall data.\nCreate a Dataset object from the DataFrame. Ensure its item metadata is correct.\nSave the Dataset object to a dfs0 file.\nOpen the dfs0 file in MIKE+. Does it make sense?",
    "crumbs": [
      "Module 2 - Time Series",
      "Homework"
    ]
  },
  {
    "objectID": "module3_network_results/index.html",
    "href": "module3_network_results/index.html",
    "title": "Welcome to Module 3!",
    "section": "",
    "text": "This module dives into the specifics of handling 1D network model results, a common task for MIKE+ users working with collection systems, water distribution networks, or river models. Our focus is on equipping you to efficiently access, analyze, and prepare 1D simulation outputs using Python:\n\nOpen and read network result files (e.g., .res1d, .res, .resx).\nExplore and navigate the network structure (nodes, reaches, catchments, gridpoints).\nSelect and subset specific data by location, quantity, and time.\nConvert network data to Pandas DataFrames for further analysis and visualization.\nExport selected network data to common formats like dfs0 or csv.\n\nYou’ll be introduced to a powerful new library: MIKE IO 1D. Let’s begin!\n\n\n\n\n\n\nWhere can I download sample data to follow along?\n\n\n\n\n\nAll of the sample data used in this module is available for download:\n\nnetwork.res1d\ncatchments.res1d\nflow_meter_data.csv",
    "crumbs": [
      "Module 3 - Network Results",
      "Welcome to Module 3!"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html",
    "href": "module3_network_results/01_overview_mikeio1d.html",
    "title": "16  MIKE IO 1D",
    "section": "",
    "text": "16.1 What is MIKE IO 1D?\nThis section introduces MIKE IO 1D, a specialized Python package for working with 1D network result files from MIKE+ models.\nMIKE IO 1D is an open-source Python package developed by DHI, forming an integral part of DHI’s Python ecosystem. Similar to MIKE IO, it empowers modelers with full flexibility by bridging the gap between various MIKE 1D file formats and Scientific Python’s rich and powerful package ecosystem.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#mike-io-vs-mike-io-1d",
    "href": "module3_network_results/01_overview_mikeio1d.html#mike-io-vs-mike-io-1d",
    "title": "16  MIKE IO 1D",
    "section": "16.2 MIKE IO vs MIKE IO 1D",
    "text": "16.2 MIKE IO vs MIKE IO 1D\nMIKE IO 1D is specifically tailored for interacting with MIKE 1D modelling files, such as network result files (e.g., .res1d) and cross-section files (e.g., .xns11). MIKE IO, on the other hand, primarily handles n-dimensional data files (e.g., .dfs0, .dfs2, .dfsu) for gridded or mesh-based data. MIKE+ modellers will often require both packages to support their workflows.\n\n\n\n\n\n\nHistory of MIKE IO 1D\n\n\n\n\n\nMIKE IO 1D was historically part of MIKE IO. It was split into a separate library due to fundamental differences in 1D functionality and the then-objectives of MIKE IO.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#usage-in-course",
    "href": "module3_network_results/01_overview_mikeio1d.html#usage-in-course",
    "title": "16  MIKE IO 1D",
    "section": "16.3 Usage in course",
    "text": "16.3 Usage in course\nMost MIKE+ models (collection system, water distribution, rivers) require handling network result files (e.g. res1d, res, resx). We introduce a subset of MIKE IO 1D’s features that are useful for such workflows.\nWhile MIKE IO 1D includes other features, like cross-sectional data, this module excludes them to keep the content manageable for beginners. We encourage you to explore MIKE IO 1D’s documentation and examples of these features when you’re ready.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#installation",
    "href": "module3_network_results/01_overview_mikeio1d.html#installation",
    "title": "16  MIKE IO 1D",
    "section": "16.4 Installation",
    "text": "16.4 Installation\nInstall MIKE IO 1D with:\nuv pip install mikeio1d\n\n\n\n\n\n\nTip\n\n\n\nAlways check the official MIKE IO 1D’s documentation for the latest installation instructions.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#network-structure",
    "href": "module3_network_results/01_overview_mikeio1d.html#network-structure",
    "title": "16  MIKE IO 1D",
    "section": "16.5 Network Structure",
    "text": "16.5 Network Structure\nUnderstanding MIKE 1D’s network structure (e.g., nodes, reaches, catchments, gridpoints) is crucial when you work with MIKE IO 1D. You may already have a solid understanding of these concepts as a MIKE+ modeller. Please refer to MIKE IO 1D’s documentation for a brief refresher on the topic. For detailed information, refer to MIKE+’s documentation.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#quick-glance",
    "href": "module3_network_results/01_overview_mikeio1d.html#quick-glance",
    "title": "16  MIKE IO 1D",
    "section": "16.6 Quick glance",
    "text": "16.6 Quick glance\nLet’s take a quick look at some core MIKE IO 1D objects and how to access them. When you open a network result file (e.g. res1d, res), MIKE IO 1D returns a Res1D object.\n\nimport mikeio1d\n\nres = mikeio1d.open(\"data/network.res1d\") \n\n\n\n\n\n\n\nAll network result file types (res1d, res, etc.) return a Res1D object on open()\n\n\n\n\n\nMIKE IO 1D opens all result file types with open(), returning a Res1D object regardless of the initial file extension. Refer to MIKE IO 1D’s documentation on this for details.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Res1D object is the core container for network results, analogous to the Dataset object in MIKE IO.\n\n\nAll results within a Res1D object share a common time axis, much like data items in a .dfs0 file.\n\nres.time_index\n\nMIKE IO 1D provides access to standard result quantities (e.g., Water Level, Discharge).\n\nres.quantities\n\nAdditionally, it includes derived quantities (e.g., Water Depth) that are calculated on-the-fly, similar to MIKE+.\n\nres.derived_quantities\n\n\n\n\n\n\n\nNote\n\n\n\nDerived quantities are not stored in the result file, thus they always require calculation at runtime (even by MIKE+).\n\n\nYou can convert quantities at network locations into Pandas DataFrames.\n\nres.nodes[\"101\"].WaterLevel.to_dataframe()",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#key-concepts",
    "href": "module3_network_results/01_overview_mikeio1d.html#key-concepts",
    "title": "16  MIKE IO 1D",
    "section": "16.7 Key Concepts",
    "text": "16.7 Key Concepts\nUnderstanding these core concepts will help you navigate MIKE IO 1D:\n\nRes1D: This is the primary object representing the contents of a network result file (e.g., .res1d). It acts as the main entry point to all data.\nLocations: Results are associated with specific locations within the network model, such as nodes, reaches, or specific grid points along a reach.\nQuantities: Time series data with a concrete type, like ‘WaterLevel’ or ‘Discharge’, at a specific network location.\n\nImagine a hierarchy: a Res1D object contains various Locations (nodes, reaches, catchments). Each Location can have multiple Quantities available as time series data.\n\n\n\n“MIKE IO 1D Data Structure”\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a simplified diagram that excludes some details covered later in the module. For example, the hierarchical group of reaches could be further divided into gridpoints along each reach.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#additional-reading-optional",
    "href": "module3_network_results/01_overview_mikeio1d.html#additional-reading-optional",
    "title": "16  MIKE IO 1D",
    "section": "16.8 Additional reading (optional)",
    "text": "16.8 Additional reading (optional)\nThe following are useful resources for learning MIKE IO 1D:\n\nMIKE IO 1D’s official documentation\nMIKE IO 1D Example Notebooks\nMIKE IO 1D’s GitHub repository",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html",
    "href": "module3_network_results/02_reading_network_files.html",
    "title": "17  Reading res1d",
    "section": "",
    "text": "17.1 Workflow\nThis section guides you through loading network results (e.g., from res1d files) into Pandas DataFrames. The approach is similar to that used with MIKE IO, allowing you to apply your DataFrame knowledge to network results.\nThe general workflow for network results in MIKE IO 1D is as follows:",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#workflow",
    "href": "module3_network_results/02_reading_network_files.html#workflow",
    "title": "17  Reading res1d",
    "section": "",
    "text": "Open the network result file to obtain a Res1D object.\nOptionally, subset this Res1D object to select specific locations, quantities, or time steps.\nConvert the (subsetted) data to a DataFrame.\nPerform some additional analysis via the DataFrame.\n\n\n\n\n\n\n\nTip\n\n\n\nThis workflow is quite similar to the one you learned in Module 2 for handling dfs0 files with MIKE IO. The core idea of opening a file, accessing data, and then often converting to a DataFrame remains consistent.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#res1d",
    "href": "module3_network_results/02_reading_network_files.html#res1d",
    "title": "17  Reading res1d",
    "section": "17.2 Res1D",
    "text": "17.2 Res1D\nThe Res1D object is central to interacting with 1D network result files. It acts as the primary container for your simulation data, analogous to MIKE IO’s Dataset object for other DHI file types.\nTo start, you open your result file using mikeio1d.open():\n\nimport mikeio1d\n\nres = mikeio1d.open(\"data/network.res1d\")\nres\n\n&lt;mikeio1d.Res1D&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nMIKE IO uses read() to create a Dataset, whereas MIKE IO 1D uses open() to create a Res1D object.\n\n\nGet an overview of key meta data with info().\n\nres.info()\n\nStart time: 1994-08-07 16:35:00\nEnd time: 1994-08-07 18:35:00\n# Timesteps: 110\n# Catchments: 0\n# Nodes: 119\n# Reaches: 118\n# Globals: 0\n0 - Water level (m)\n1 - Discharge (m^3/s)\n\n\nNotice the this produces similar information to that of a MIKE IO Dataset:\n\nTotal number of time steps\nTimestamps for first and last time step\nAll available quantities\n\nLike dfs0 files, all results within a Res1D object share a common time axis.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#convert-to-dataframe",
    "href": "module3_network_results/02_reading_network_files.html#convert-to-dataframe",
    "title": "17  Reading res1d",
    "section": "17.3 Convert to DataFrame",
    "text": "17.3 Convert to DataFrame\nThe read() method is the primary way to convert data from a Res1D object (or its subsets) into a Pandas DataFrame. This is a versatile method that can be called at various levels.\nYou can convert the entire content of the Res1D object into a single DataFrame. Each quantity at a specific location becomes a column in the DataFrame.\n\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n195.8815\n193.604996\n193.615005\n193.625320\n193.675110\n193.765060\n193.775116\n193.804993\n...\n0.000002\n193.775070\n193.765060\n0.000031\n193.550003\n188.479996\n0.0\n193.306061\n195.005005\n0.0\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n195.8815\n193.604996\n193.615005\n193.625671\n193.675369\n193.765106\n193.775513\n193.804993\n...\n0.000002\n193.775391\n193.765106\n0.000033\n193.550034\n188.479996\n0.0\n193.307144\n195.005005\n0.0\n\n\n1994-08-07 16:38:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626236\n193.675751\n193.765228\n193.776077\n193.804993\n...\n0.000002\n193.775894\n193.765228\n0.000037\n193.550079\n188.479996\n0.0\n193.308884\n195.005005\n0.0\n\n\n1994-08-07 16:39:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626556\n193.675949\n193.765335\n193.776352\n193.804993\n...\n0.000002\n193.776154\n193.765335\n0.000039\n193.550095\n188.479996\n0.0\n193.309860\n195.005005\n0.0\n\n\n\n\n5 rows × 495 columns\n\n\n\nThe DataFrame above includes all reaches and nodes, which results in many columns. To create more manageable DataFrames, call read() on data subsets (e.g., by location or quantity).\nFor example, all results for the location group reaches:\n\ndf_reaches = res.reaches.read()\ndf_reaches.head()\n\n\n\n\n\n\n\n\nWaterLevel:100l1:0\nWaterLevel:100l1:47.6827\nWaterLevel:101l1:0\nWaterLevel:101l1:66.4361\nWaterLevel:102l1:0\nWaterLevel:102l1:10.9366\nWaterLevel:103l1:0\nWaterLevel:103l1:26.0653\nWaterLevel:104l1:0\nWaterLevel:104l1:34.4131\n...\nDischarge:93l1:24.5832\nDischarge:94l1:21.2852\nDischarge:95l1:21.9487\nDischarge:96l1:14.9257\nDischarge:97l1:5.71207\nDischarge:98l1:8.00489\nDischarge:99l1:22.2508\nDischarge:9l1:5\nDischarge:Weir:119w1:0.5\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.441498\n194.661499\n195.931503\n195.441498\n193.550003\n193.550003\n195.801498\n195.701508\n197.072006\n196.962006\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000013\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:36:01.870\n195.441498\n194.661621\n195.931503\n195.441605\n193.550140\n193.550064\n195.801498\n195.703171\n197.072006\n196.962051\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:37:07.560\n195.441498\n194.661728\n195.931503\n195.441620\n193.550232\n193.550156\n195.801498\n195.703400\n197.072006\n196.962082\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000033\n0.0\n0.0\n\n\n1994-08-07 16:38:55.828\n195.441498\n194.661804\n195.931503\n195.441605\n193.550369\n193.550308\n195.801498\n195.703690\n197.072006\n196.962112\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000037\n0.0\n0.0\n\n\n1994-08-07 16:39:55.828\n195.441498\n194.661972\n195.931503\n195.441605\n193.550430\n193.550369\n195.801498\n195.703827\n197.072006\n196.962128\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000039\n0.0\n0.0\n\n\n\n\n5 rows × 376 columns\n\n\n\nOr results for a reach named 100l1:\n\ndf_reach_100l1 = res.reaches[\"100l1\"].read()\ndf_reach_100l1.head()\n\n\n\n\n\n\n\n\nWaterLevel:100l1:0\nWaterLevel:100l1:47.6827\nDischarge:100l1:23.8414\n\n\n\n\n1994-08-07 16:35:00.000\n195.441498\n194.661499\n0.000006\n\n\n1994-08-07 16:36:01.870\n195.441498\n194.661621\n0.000006\n\n\n1994-08-07 16:37:07.560\n195.441498\n194.661728\n0.000006\n\n\n1994-08-07 16:38:55.828\n195.441498\n194.661804\n0.000006\n\n\n1994-08-07 16:39:55.828\n195.441498\n194.661972\n0.000006\n\n\n\n\n\n\n\nOr just the Discharge quantity of the previous reach.\n\ndf_reach_100l1_q = res.reaches[\"100l1\"].Discharge.read()\ndf_reach_100l1_q.head()\n\n\n\n\n\n\n\n\nDischarge:100l1:23.8414\n\n\n\n\n1994-08-07 16:35:00.000\n0.000006\n\n\n1994-08-07 16:36:01.870\n0.000006\n\n\n1994-08-07 16:37:07.560\n0.000006\n\n\n1994-08-07 16:38:55.828\n0.000006\n\n\n1994-08-07 16:39:55.828\n0.000006\n\n\n\n\n\n\n\nIn the coming sections, we will cover how to explore the network structure of Res1D and select data. For now, just know that it’s possible to call read() from various sub-objects of Res1D to obtain a DataFrame.\n\n\n\n\n\n\nTip\n\n\n\nOnce you have your network data in a Pandas DataFrame using .read(), you can apply all the powerful analysis, manipulation, and visualization techniques you learned in Module 1 and Module 2.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#plotting-and-other-formats",
    "href": "module3_network_results/02_reading_network_files.html#plotting-and-other-formats",
    "title": "17  Reading res1d",
    "section": "17.4 Plotting and other formats",
    "text": "17.4 Plotting and other formats\nFor convenience, a plot() method is available anywhere that read() can be called. This allows for quick visualization of the data.\n\nres.reaches[\"100l1\"].Discharge.plot()\n\n\n\n\n\n\n\n\nSimilarly, wherever read() is available, you can export data to other common formats. For example:\n\nres.reaches[\"100l1\"].Discharge.to_dfs0(\"discharge_of_interest.dfs0\")\n\n\nres.reaches[\"100l1\"].Discharge.to_csv(\"discharge_of_interest.csv\")",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#practical-example",
    "href": "module3_network_results/02_reading_network_files.html#practical-example",
    "title": "17  Reading res1d",
    "section": "17.5 Practical Example",
    "text": "17.5 Practical Example\nLet’s combine some of these concepts to accomplish the following objectives:\n\nGet descriptive statistics of all node water levels\nGet descriptive statistics of all reach discharges\n\nFirst, open the result file.\n\nimport mikeio1d\n\nres = mikeio1d.open(\"data/network.res1d\")\n\nThen, get our node water levels and reach discharges into DataFrames.\n\ndf_nodes_wl = res.nodes.WaterLevel.read()\ndf_reaches_q = res.reaches.Discharge.read()\n\nUse describe() on each DataFrame, just like from previous modules.\n\nwl_stats = df_nodes_wl.describe().T\nwl_stats.head()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nWaterLevel:1\n110.0\n195.198898\n0.159523\n195.052994\n195.090881\n195.131989\n195.258179\n195.669006\n\n\nWaterLevel:2\n110.0\n195.822342\n0.000657\n195.821503\n195.821503\n195.822784\n195.822937\n195.822968\n\n\nWaterLevel:3\n110.0\n195.881470\n0.000000\n195.881500\n195.881500\n195.881500\n195.881500\n195.881500\n\n\nWaterLevel:4\n110.0\n193.947418\n0.348278\n193.604996\n193.614719\n193.891739\n194.173325\n194.661331\n\n\nWaterLevel:5\n110.0\n194.010544\n0.379473\n193.615005\n193.659035\n193.940742\n194.260757\n194.793060\n\n\n\n\n\n\n\n\nq_stats = df_reaches_q.describe().T\nq_stats.head()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nDischarge:100l1:23.8414\n110.0\n0.014078\n0.026875\n0.000006\n0.000636\n0.001032\n0.005988\n0.099751\n\n\nDischarge:101l1:33.218\n110.0\n-0.000034\n0.005649\n-0.022655\n0.000004\n0.000005\n0.000022\n0.019202\n\n\nDischarge:102l1:5.46832\n110.0\n0.069058\n0.100331\n-0.011316\n0.003062\n0.017987\n0.096062\n0.326383\n\n\nDischarge:103l1:13.0327\n110.0\n-0.000011\n0.001084\n-0.006748\n0.000002\n0.000017\n0.000337\n0.001056\n\n\nDischarge:104l1:17.2065\n110.0\n0.000005\n0.000002\n0.000003\n0.000005\n0.000005\n0.000005\n0.000025\n\n\n\n\n\n\n\nFrom here we might want to use existing Pandas functionality to export our data for reporting purposes:\n\nwl_stats.to_excel(\"node_water_levels.xlsx\")\nq_stats.to_excel(\"reaches_discharges.xlsx\")\n\n\n\n\n\n\n\nTip\n\n\n\nYou need to install a Python package like openpyxl to use Pandas’ to_excel() method.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html",
    "href": "module3_network_results/03_exploring_network_and_results.html",
    "title": "18  Exploring Networks",
    "section": "",
    "text": "18.1 Nodes\nThis section guides you through navigating MIKE IO 1D’s Res1D object. You’ll learn how to access nodes, catchments, reaches, and individual gridpoints, along with their associated data and properties. This exploration is key to understanding the structure and results of your MIKE+ 1D models.\nNode data is accessible via the Res1D object, typically named res in our examples. The res.nodes attribute provides access to all nodes in your result file.\nres.nodes\n\n&lt;ResultNodes&gt; (119)\n    \n    Quantities (1)Water level (m)Derived Quantities (3)NodeFloodingNodeWaterDepthNodeWaterLevelAboveCritical\nThe nodes object behaves like a Python dictionary, where node IDs are the keys and specific node objects are the values. You can access a specific node by its ID.\nres.nodes[\"1\"]\n\n&lt;Manhole: 1&gt;\n    \n    Attributes (8)id: 1type: Manholexcoord: -687934.6000976562ycoord: -1056500.69921875ground_level: 197.07000732421875bottom_level: 195.0500030517578critical_level: infdiameter: 1.0Quantities (1)Water level (m)Derived Quantities (3)NodeFloodingNodeWaterDepthNodeWaterLevelAboveCritical\nEach node object contains both dynamic quantities (time series results like water level) and static properties (like invert level).\nTo access a dynamic quantity:\nres.nodes[\"1\"].WaterLevel\n\n&lt;Quantity: Water level (m)&gt;\nAs shown in the last section, dynamic quantities can be converted to a DataFrame with read(), or plotted with plot():\nres.nodes[\"1\"].WaterLevel.plot()\nTo access a static property:\nres.nodes[\"1\"].bottom_level\n\n195.0500030517578\nYou can iterate through all nodes, similar to a Python dictionary. For example:\nfor node_id, node in res.nodes.items():\n    if node.type == \"Outlet\":\n        display(node)\n\n&lt;Outlet: 120&gt;\n    \n    Attributes (8)id: 120type: Outletxcoord: -687917.30078125ycoord: -1056709.4993896484ground_level: 194.0bottom_level: 193.4499969482422critical_level: Nonediameter: NoneQuantities (1)Water level (m)Derived Quantities (3)NodeFloodingNodeWaterDepthNodeWaterLevelAboveCritical\n\n\n&lt;Outlet: Weir Outlet:119w1&gt;\n    \n    Attributes (8)id: Weir Outlet:119w1type: Outletxcoord: -687941.8002929688ycoord: -1056652.400024414ground_level: 197.1699981689453bottom_level: 183.47999572753906critical_level: Nonediameter: NoneQuantities (1)Water level (m)Derived Quantities (3)NodeFloodingNodeWaterDepthNodeWaterLevelAboveCritical",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#nodes",
    "href": "module3_network_results/03_exploring_network_and_results.html#nodes",
    "title": "18  Exploring Networks",
    "section": "",
    "text": "Tip\n\n\n\nNotice that displaying res.nodes shows relevant metadata. This is true for all location objects.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#catchments",
    "href": "module3_network_results/03_exploring_network_and_results.html#catchments",
    "title": "18  Exploring Networks",
    "section": "18.2 Catchments",
    "text": "18.2 Catchments\nYou handle catchments similarly to nodes, but they are typically in a separate result file, often from rainfall-runoff simulations.\n\nres_rr.catchments\n\n&lt;ResultCatchments&gt; (31)\n    \n    Quantities (5)Total Runoff (m^3/s)Actual Rainfall (m/s)Zink, Load, RR (kg/s)Zink, Mass, Accumulated, RR (kg)Zink, RR (mg/l)Derived Quantities (0)\n\n\n\n\n\n\n\n\nWhy are separate result files used?\n\n\n\n\n\nRecall that all quantities in a network result file share the same time axis. Hydraulic simulations require finer time steps than hydrologic simulations, and thus a different time index. This is the main reason hydrologic and hydraulic results are stored in separate files.\n\n\n\nThe catchments object also acts like a dictionary, with catchment IDs as keys.\n\nres_rr.catchments[\"100_16_16\"]\n\n&lt;Catchment: 100_16_16&gt;\n    \n    Attributes (3)id: 100_16_16area: 22800.0type: Kinematic WaveQuantities (5)Total Runoff (m^3/s)Actual Rainfall (m/s)Zink, Load, RR (kg/s)Zink, Mass, Accumulated, RR (kg)Zink, RR (mg/l)Derived Quantities (0)\n\n\nCatchment objects also have dynamic quantities and static properties. For instance, TotalRunOff is a dynamic quantity, and area is a static property.\n\nres_rr.catchments[\"100_16_16\"].TotalRunOff.plot()\n\n\n\n\n\n\n\n\n\nres_rr.catchments[\"100_16_16\"].area\n\n22800.0\n\n\nJust like nodes, you can iterate through catchments like a Python dictionary. For example:\n\nfor catchment_id, catchment in res_rr.catchments.items():\n    if \"28\" in catchment_id:\n        display(catchment)\n\n&lt;Catchment: 28_6_6&gt;\n    \n    Attributes (3)id: 28_6_6area: 3500.0type: Kinematic WaveQuantities (5)Total Runoff (m^3/s)Actual Rainfall (m/s)Zink, Load, RR (kg/s)Zink, Mass, Accumulated, RR (kg)Zink, RR (mg/l)Derived Quantities (0)\n\n\n&lt;Catchment: 90_28_28&gt;\n    \n    Attributes (3)id: 90_28_28area: 48000.0type: Kinematic WaveQuantities (5)Total Runoff (m^3/s)Actual Rainfall (m/s)Zink, Load, RR (kg/s)Zink, Mass, Accumulated, RR (kg)Zink, RR (mg/l)Derived Quantities (0)",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#reaches",
    "href": "module3_network_results/03_exploring_network_and_results.html#reaches",
    "title": "18  Exploring Networks",
    "section": "18.3 Reaches",
    "text": "18.3 Reaches\nReaches are accessed similarly to nodes and catchments. A key difference is that individual reach objects also contain gridpoints.\n\nres.reaches\n\n&lt;ResultReaches&gt; (118)\n    \n    Quantities (2)Water level (m)Discharge (m^3/s)Derived Quantities (6)ReachAbsoluteDischargeReachFillingReachFloodingReachQQManningReachWaterDepthReachWaterLevelAboveCritical\n\n\nThe reaches object is dictionary-like, with reach names as keys and specific reach objects as values.\n\nres.reaches[\"101l1\"]\n\n&lt;Reach: 101l1&gt;\n    \n    Attributes (9)name: 101l1length: 66.4360966980845start_chainage: 0.0end_chainage: 66.4360966980845n_gridpoints: 3start_node: 101end_node: 100height: 0.30000001192092896full_flow_discharge: 0.0809713208695954Quantities (2)Water level (m)Discharge (m^3/s)Derived Quantities (6)ReachAbsoluteDischargeReachFillingReachFloodingReachQQManningReachWaterDepthReachWaterLevelAboveCritical\n\n\nEach reach object contains dynamic quantities and static properties.\n\nres.reaches[\"101l1\"].WaterLevel.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that two water levels are plotted above, representing the start and end H-points of the reach.\n\n\n\nres.reaches[\"101l1\"].n_gridpoints\n\n3\n\n\nJust like before, you can iterate through reaches like a Python dictionary. For example:\n\n# This may print multiple lines\nfor reach_name, reach in res.reaches.items():\n    if reach.start_node == \"1\":\n        display(reach)\n\n&lt;Reach: 1l1&gt;\n    \n    Attributes (9)name: 1l1length: 12.0784172521484start_chainage: 0.0end_chainage: 12.0784172521484n_gridpoints: 3start_node: 1end_node: 16height: 0.6000000238418579full_flow_discharge: 0.7808684423624622Quantities (2)Water level (m)Discharge (m^3/s)Derived Quantities (6)ReachAbsoluteDischargeReachFillingReachFloodingReachQQManningReachWaterDepthReachWaterLevelAboveCritical",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#gridpoints",
    "href": "module3_network_results/03_exploring_network_and_results.html#gridpoints",
    "title": "18  Exploring Networks",
    "section": "18.4 Gridpoints",
    "text": "18.4 Gridpoints\nSpecific reach objects are also both list- and dictionary-like, mapping to their constituent gridpoint objects. Gridpoints are the locations along a reach where results are calculated.\n\n\n\n\n\n\nNote\n\n\n\nUnlike Python dictionaries, reaches also support sequential indexing (e.g., [0] for the first gridpoint) for convenient gridpoint access.\n\n\nAccess the first gridpoint of reach “101l1”:\n\nres.reaches[\"101l1\"][0]\n\n&lt;ResultGridPoint&gt;\n    \n    Attributes (5)reach_name: 101l1chainage: 0.0xcoord: -687859.5004882812ycoord: -1056308.700012207bottom_level: 195.92999267578125Quantities (1)Water level (m)Derived Quantities (0)\n\n\nAccess the last gridpoint of reach “101l1”:\n\nres.reaches[\"101l1\"][-1]\n\n&lt;ResultGridPoint&gt;\n    \n    Attributes (5)reach_name: 101l1chainage: 66.4360966980845xcoord: -687887.6008911133ycoord: -1056368.9006958008bottom_level: 195.44000244140625Quantities (1)Water level (m)Derived Quantities (0)\n\n\nYou can also access gridpoints by their chainage value. Chainage can be a float or a string.\n\nres.reaches[\"100l1\"]['23.841']\n\n&lt;ResultGridPoint&gt;\n    \n    Attributes (5)reach_name: 100l1chainage: 23.8413574216414xcoord: -687897.8000488281ycoord: -1056390.4503479004bottom_level: 195.0500030517578Quantities (1)Discharge (m^3/s)Derived Quantities (0)\n\n\nNotice that each gridpoint has its own dynamic quantities and static properties.\nAccess WaterLevel at the first gridpoint:\n\nres.reaches[\"100l1\"][0].WaterLevel.plot()\n\n\n\n\n\n\n\n\nAccess Discharge at the second gridpoint:\n\nres.reaches[\"100l1\"][1].Discharge.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nGridpoint quantities can vary along a reach. For example, WaterLevel is typically available at H-points (calculation points), while Discharge is available at Q-points (flow points, often at gridpoint centers or structures). Refer to the MIKE+ documentation for details on H-points and Q-points.\n\n\nAccess the chainage static property of a gridpoint:\n\nres.reaches[\"100l1\"][-1].chainage\n\n47.6827148432828\n\n\nJust like before, you can iterate through reaches like a Python dictionary. The keys are the gridpoint chainage along the reach. For example:\n\nfor chainage, gridpoint in res.reaches[\"100l1\"].items():\n    print(f\"Bottom level at chainage {chainage} is {gridpoint.bottom_level}\")\n\nBottom level at chainage 0.000 is 195.44000244140625\nBottom level at chainage 23.841 is 195.0500030517578\nBottom level at chainage 47.683 is 194.66000366210938",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#example---dynamic-autocompletion",
    "href": "module3_network_results/03_exploring_network_and_results.html#example---dynamic-autocompletion",
    "title": "18  Exploring Networks",
    "section": "18.5 Example - Dynamic Autocompletion",
    "text": "18.5 Example - Dynamic Autocompletion\nDynamic autocompletion in environments like Jupyter or VS Code significantly aids in exploring these objects. Watch this video to see it in action.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html",
    "href": "module3_network_results/04_selecting_network_data.html",
    "title": "19  Data Selection",
    "section": "",
    "text": "19.1 Alternative Methods\nThis section covers how to select specific subsets of network result data.\nThere are various ways of selecting subsets of network result data. This section covers three main approaches:",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#alternative-methods",
    "href": "module3_network_results/04_selecting_network_data.html#alternative-methods",
    "title": "19  Data Selection",
    "section": "",
    "text": "Using Pandas DataFrame.\nUsing the Res1D object to subset locations and quantities.\nUsing mikeio1d.open().\n\n\n\n\n\n\n\nMemory considerations\n\n\n\n\n\nSimilar to MIKE IO, selecting data via open() is generally most performant since it avoids loading the entire file into memory.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#selecting-locations",
    "href": "module3_network_results/04_selecting_network_data.html#selecting-locations",
    "title": "19  Data Selection",
    "section": "19.2 Selecting Locations",
    "text": "19.2 Selecting Locations\nWhen dealing with network results, you often want to focus on specific parts of your model, like certain nodes or reaches.\nSelecting data purely with DataFrames can be difficult due to the large number of columns and their header format. For example, this DataFrame has 495 columns:\n\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n195.8815\n193.604996\n193.615005\n193.625320\n193.675110\n193.765060\n193.775116\n193.804993\n...\n0.000002\n193.775070\n193.765060\n0.000031\n193.550003\n188.479996\n0.0\n193.306061\n195.005005\n0.0\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n195.8815\n193.604996\n193.615005\n193.625671\n193.675369\n193.765106\n193.775513\n193.804993\n...\n0.000002\n193.775391\n193.765106\n0.000033\n193.550034\n188.479996\n0.0\n193.307144\n195.005005\n0.0\n\n\n1994-08-07 16:38:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626236\n193.675751\n193.765228\n193.776077\n193.804993\n...\n0.000002\n193.775894\n193.765228\n0.000037\n193.550079\n188.479996\n0.0\n193.308884\n195.005005\n0.0\n\n\n1994-08-07 16:39:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626556\n193.675949\n193.765335\n193.776352\n193.804993\n...\n0.000002\n193.776154\n193.765335\n0.000039\n193.550095\n188.479996\n0.0\n193.309860\n195.005005\n0.0\n\n\n\n\n5 rows × 495 columns\n\n\n\nTo select the ‘Water Level’ quantity for node ‘1’ from the DataFrame, you need to use a column name formed by concatenating the quantity and ID with a ‘:’ separator:\n\ndf[[\"WaterLevel:1\"]]\n\n\n\n\n\n\n\n\nWaterLevel:1\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n\n\n1994-08-07 16:36:01.870\n195.052994\n\n\n1994-08-07 16:37:07.560\n195.052994\n\n\n1994-08-07 16:38:55.828\n195.052994\n\n\n1994-08-07 16:39:55.828\n195.052994\n\n\n...\n...\n\n\n1994-08-07 18:30:07.967\n195.119919\n\n\n1994-08-07 18:31:07.967\n195.118607\n\n\n1994-08-07 18:32:07.967\n195.117310\n\n\n1994-08-07 18:33:07.967\n195.115753\n\n\n1994-08-07 18:35:00.000\n195.112534\n\n\n\n\n110 rows × 1 columns\n\n\n\nSimilarly, to select the “Water Level” of reach “100l1” at chainage 47.6827:\n\ndf[[\"WaterLevel:100l1:47.6827\"]]\n\n\n\n\n\n\n\n\nWaterLevel:100l1:47.6827\n\n\n\n\n1994-08-07 16:35:00.000\n194.661499\n\n\n1994-08-07 16:36:01.870\n194.661621\n\n\n1994-08-07 16:37:07.560\n194.661728\n\n\n1994-08-07 16:38:55.828\n194.661804\n\n\n1994-08-07 16:39:55.828\n194.661972\n\n\n...\n...\n\n\n1994-08-07 18:30:07.967\n194.689072\n\n\n1994-08-07 18:31:07.967\n194.688934\n\n\n1994-08-07 18:32:07.967\n194.688812\n\n\n1994-08-07 18:33:07.967\n194.688354\n\n\n1994-08-07 18:35:00.000\n194.686172\n\n\n\n\n110 rows × 1 columns\n\n\n\nThis format is not very user-friendly, especially for selecting all quantities of a specific reach. The fluent-like API, shown earlier, offers a more intuitive way to do this.\n\nres.reaches[\"100l1\"].read()\n\n\n\n\n\n\n\n\nWaterLevel:100l1:0\nWaterLevel:100l1:47.6827\nDischarge:100l1:23.8414\n\n\n\n\n1994-08-07 16:35:00.000\n195.441498\n194.661499\n0.000006\n\n\n1994-08-07 16:36:01.870\n195.441498\n194.661621\n0.000006\n\n\n1994-08-07 16:37:07.560\n195.441498\n194.661728\n0.000006\n\n\n1994-08-07 16:38:55.828\n195.441498\n194.661804\n0.000006\n\n\n1994-08-07 16:39:55.828\n195.441498\n194.661972\n0.000006\n\n\n...\n...\n...\n...\n\n\n1994-08-07 18:30:07.967\n195.455109\n194.689072\n0.000588\n\n\n1994-08-07 18:31:07.967\n195.455063\n194.688934\n0.000583\n\n\n1994-08-07 18:32:07.967\n195.455002\n194.688812\n0.000579\n\n\n1994-08-07 18:33:07.967\n195.453049\n194.688354\n0.000526\n\n\n1994-08-07 18:35:00.000\n195.450409\n194.686172\n0.000343\n\n\n\n\n110 rows × 3 columns\n\n\n\nA more computationally efficient approach is to select locations when you open the file. You can specify lists of IDs for nodes, reaches, or catchments. For example:\n\nres = mikeio1d.open(\"data/network.res1d\", reaches=[\"100l1\"])\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nWaterLevel:100l1:0\nWaterLevel:100l1:47.6827\nDischarge:100l1:23.8414\n\n\n\n\n1994-08-07 16:35:00.000\n195.441498\n194.661499\n0.000006\n\n\n1994-08-07 16:36:01.870\n195.441498\n194.661621\n0.000006\n\n\n1994-08-07 16:37:07.560\n195.441498\n194.661728\n0.000006\n\n\n1994-08-07 16:38:55.828\n195.441498\n194.661804\n0.000006\n\n\n1994-08-07 16:39:55.828\n195.441498\n194.661972\n0.000006\n\n\n\n\n\n\n\nSimilarly for nodes:\n\nres = mikeio1d.open(\"data/network.res1d\", nodes=[\"1\", \"2\"])\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n\n\n1994-08-07 16:38:55.828\n195.052994\n195.821503\n\n\n1994-08-07 16:39:55.828\n195.052994\n195.821503\n\n\n\n\n\n\n\nAnd for catchments:\n\nres = mikeio1d.open(\"data/catchments.res1d\", catchments=[\"100_16_16\"])\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nTotalRunOff:100_16_16\nActualRainfall:100_16_16\nZinkLoadRR:100_16_16\nZinkMassAccumulatedRR:100_16_16\nZinkRR:100_16_16\n\n\n\n\n1994-08-07 16:35:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0\n\n\n1994-08-07 16:36:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0\n\n\n1994-08-07 16:37:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0\n\n\n1994-08-07 16:38:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0\n\n\n1994-08-07 16:39:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#selecting-quantities",
    "href": "module3_network_results/04_selecting_network_data.html#selecting-quantities",
    "title": "19  Data Selection",
    "section": "19.3 Selecting Quantities",
    "text": "19.3 Selecting Quantities\nJust as with locations, you can select specific physical quantities (like Water Level or Discharge). Trying to pick these out from a full DataFrame is one approach:\n\ndf = res.read()\ndischarge_columns = [column for column in df.columns if \"Discharge\" in column]\ndf[discharge_columns].head()\n\n\n\n\n\n\n\n\nDischarge:100l1:23.8414\nDischarge:101l1:33.218\nDischarge:102l1:5.46832\nDischarge:103l1:13.0327\nDischarge:104l1:17.2065\nDischarge:105l1:13.4997\nDischarge:106l1:11.4056\nDischarge:107l1:8.46476\nDischarge:108l1:15.3589\nDischarge:109l1:13.546\n...\nDischarge:93l1:24.5832\nDischarge:94l1:21.2852\nDischarge:95l1:21.9487\nDischarge:96l1:14.9257\nDischarge:97l1:5.71207\nDischarge:98l1:8.00489\nDischarge:99l1:22.2508\nDischarge:9l1:5\nDischarge:Weir:119w1:0.5\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n0.000006\n0.000004\n0.000000\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000013\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:36:01.870\n0.000006\n0.000004\n0.000008\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:37:07.560\n0.000006\n0.000004\n0.000016\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000033\n0.0\n0.0\n\n\n1994-08-07 16:38:55.828\n0.000006\n0.000004\n0.000022\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000037\n0.0\n0.0\n\n\n1994-08-07 16:39:55.828\n0.000006\n0.000004\n0.000024\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000039\n0.0\n0.0\n\n\n\n\n5 rows × 129 columns\n\n\n\nHowever, it’s often expressed more succinctly using the Res1D fluent-like API:\n\ndf = res.reaches.Discharge.read()\ndf.head()\n\n\n\n\n\n\n\n\nDischarge:100l1:23.8414\nDischarge:101l1:33.218\nDischarge:102l1:5.46832\nDischarge:103l1:13.0327\nDischarge:104l1:17.2065\nDischarge:105l1:13.4997\nDischarge:106l1:11.4056\nDischarge:107l1:8.46476\nDischarge:108l1:15.3589\nDischarge:109l1:13.546\n...\nDischarge:93l1:24.5832\nDischarge:94l1:21.2852\nDischarge:95l1:21.9487\nDischarge:96l1:14.9257\nDischarge:97l1:5.71207\nDischarge:98l1:8.00489\nDischarge:99l1:22.2508\nDischarge:9l1:5\nDischarge:Weir:119w1:0.5\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n0.000006\n0.000004\n0.000000\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000013\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:36:01.870\n0.000006\n0.000004\n0.000008\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:37:07.560\n0.000006\n0.000004\n0.000016\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000033\n0.0\n0.0\n\n\n1994-08-07 16:38:55.828\n0.000006\n0.000004\n0.000022\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000037\n0.0\n0.0\n\n\n1994-08-07 16:39:55.828\n0.000006\n0.000004\n0.000024\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000039\n0.0\n0.0\n\n\n\n\n5 rows × 129 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou must use the fluent-like API for quantity selection on a location group (e.g., res.reaches.Discharge) or a specific element, not directly on the top-level Res1D object. For top-level quantity filtering, use the quantities parameter in open().\n\n\nSimilar to location filtering, it’s also more computationally efficient to do this on open():\n\nres = mikeio1d.open(\"data/network.res1d\", quantities=[\"Discharge\"])\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nDischarge:100l1:23.8414\nDischarge:101l1:33.218\nDischarge:102l1:5.46832\nDischarge:103l1:13.0327\nDischarge:104l1:17.2065\nDischarge:105l1:13.4997\nDischarge:106l1:11.4056\nDischarge:107l1:8.46476\nDischarge:108l1:15.3589\nDischarge:109l1:13.546\n...\nDischarge:93l1:24.5832\nDischarge:94l1:21.2852\nDischarge:95l1:21.9487\nDischarge:96l1:14.9257\nDischarge:97l1:5.71207\nDischarge:98l1:8.00489\nDischarge:99l1:22.2508\nDischarge:9l1:5\nDischarge:Weir:119w1:0.5\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n0.000006\n0.000004\n0.000000\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000013\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:36:01.870\n0.000006\n0.000004\n0.000008\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:37:07.560\n0.000006\n0.000004\n0.000016\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000033\n0.0\n0.0\n\n\n1994-08-07 16:38:55.828\n0.000006\n0.000004\n0.000022\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000037\n0.0\n0.0\n\n\n1994-08-07 16:39:55.828\n0.000006\n0.000004\n0.000024\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000039\n0.0\n0.0\n\n\n\n\n5 rows × 129 columns\n\n\n\n\n\n\n\n\n\nWhere do I find quantity names?\n\n\n\n\n\nTo see all available quantities in a Res1D object (res), you can inspect res.quantities. To see all possible MIKE 1D quantities, you can get a list as follows:\n\nfrom mikeio1d.res1d import mike1d_quantities\n\nall_quantities = mike1d_quantities()",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#selecting-time-steps",
    "href": "module3_network_results/04_selecting_network_data.html#selecting-time-steps",
    "title": "19  Data Selection",
    "section": "19.4 Selecting Time Steps",
    "text": "19.4 Selecting Time Steps\nFiltering by time is another common requirement. If you have your data in a Pandas DataFrame, you can use the time indexing techniques covered in Module 2. For example, the first three time steps:\n\ndf = res.read()\ndf.iloc[:3]\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n195.8815\n193.604996\n193.615005\n193.625320\n193.675110\n193.765060\n193.775116\n193.804993\n...\n0.000002\n193.775070\n193.765060\n0.000031\n193.550003\n188.479996\n0.0\n193.306061\n195.005005\n0.0\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n195.8815\n193.604996\n193.615005\n193.625671\n193.675369\n193.765106\n193.775513\n193.804993\n...\n0.000002\n193.775391\n193.765106\n0.000033\n193.550034\n188.479996\n0.0\n193.307144\n195.005005\n0.0\n\n\n\n\n3 rows × 495 columns\n\n\n\nAs with other selections, filtering by time when opening the file with open() is more efficient. To select time steps when opening the file, use the time parameter to specify the start and end bounds.\n\nres = mikeio1d.open(\"data/network.res1d\", time=('1994-08-07 16:35:00', '1994-08-07 16:38'))\nres.read()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n195.8815\n193.604996\n193.615005\n193.625320\n193.675110\n193.765060\n193.775116\n193.804993\n...\n0.000002\n193.775070\n193.765060\n0.000031\n193.550003\n188.479996\n0.0\n193.306061\n195.005005\n0.0\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n195.8815\n193.604996\n193.615005\n193.625671\n193.675369\n193.765106\n193.775513\n193.804993\n...\n0.000002\n193.775391\n193.765106\n0.000033\n193.550034\n188.479996\n0.0\n193.307144\n195.005005\n0.0\n\n\n\n\n3 rows × 495 columns\n\n\n\nTo select every nth time step, you can use the step_every parameter:\n\nres = mikeio1d.open(\"data/network.res1d\", step_every=5)\nres.read().head()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:40:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626877\n193.676117\n193.765457\n193.776596\n193.805038\n...\n0.000002\n193.776382\n193.765457\n0.000042\n193.550110\n188.479996\n0.0\n193.310852\n195.005005\n0.0\n\n\n1994-08-07 16:45:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615128\n193.628540\n193.676895\n193.766235\n193.777557\n193.805786\n...\n0.000003\n193.777252\n193.766235\n0.000053\n193.550125\n188.479996\n0.0\n193.315674\n195.005005\n0.0\n\n\n1994-08-07 16:51:29.529\n195.099609\n195.821655\n195.8815\n193.605225\n193.624008\n193.648056\n193.678299\n193.774384\n193.795883\n193.829300\n...\n0.000717\n193.794693\n193.774384\n0.000513\n193.550461\n188.479996\n0.0\n193.321274\n195.005005\n0.0\n\n\n1994-08-07 16:58:12.888\n195.088943\n195.821503\n195.8815\n193.607758\n193.634552\n193.672958\n193.706161\n193.829422\n193.852097\n193.869766\n...\n0.001972\n193.846848\n193.829422\n0.005163\n193.555649\n188.479996\n0.0\n193.353027\n195.005005\n0.0\n\n\n\n\n5 rows × 495 columns\n\n\n\nNotice that these options are similar to when loading network result files in MIKE+:\n\n\n\nLoading network results in MIKE+",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#practical-example",
    "href": "module3_network_results/04_selecting_network_data.html#practical-example",
    "title": "19  Data Selection",
    "section": "19.5 Practical Example",
    "text": "19.5 Practical Example\nImagine you’re calibrating a MIKE+ model. There’s two specific points in the network where you have observed flow data. You also have a list of calibration event time stamps you deem relevant. Let’s use Python to automate generating some plots that could be useful during the calibration process.\nFirst, make a list of the reach IDs where the flow meters are.\n\ncalibration_points = [\"12l1\", \"116l1\"]\n\nNext let’s define the event start and stop times.\n\ncalibration_events = [\n    (\"1994-08-07 17:00\", \"1994-08-07 18:30\"),\n    # here we could add another event, but for this example we only use one\n]\n\nLoad the flow meter data from csv.\n\nimport pandas as pd\ndf_obs = pd.read_csv(\"data/flow_meter_data.csv\", index_col=0, parse_dates=True)\ndf_obs.head()\n\n\n\n\n\n\n\n\n116l1_observed\n12l1_observed\n\n\ntime\n\n\n\n\n\n\n1994-08-07 16:35:00\n-0.014113\n-0.095583\n\n\n1994-08-07 16:36:00\n0.043355\n-0.058748\n\n\n1994-08-07 16:37:00\n-0.129244\n0.040089\n\n\n1994-08-07 16:38:00\n-0.052462\n0.068110\n\n\n1994-08-07 16:39:00\n-0.051976\n-0.024882\n\n\n\n\n\n\n\nNow let’s create plots for each calibration point:\n\nfor event in calibration_events:\n    event_start = event[0]  # event start time\n    event_end = event[1]    # event end time\n\n    res = mikeio1d.open(\"data/network.res1d\", time=(event_start, event_end))\n    df_obs_event = df_obs.loc[event_start : event_end]\n\n    for reach in calibration_points:\n        ax = res.reaches[reach].Discharge.plot()\n        df_obs_event[f\"{reach}_observed\"].plot(ax=ax, color='grey', linestyle=\"--\", zorder=-1)\n        ax.legend()\n        ax.grid()\n        ax.set_title(f\"Calibration Plot for Reach '{reach}'\")\n        \n        # optional: save the figure to a PNG file using standard Matplotlib functionality",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/homework.html",
    "href": "module3_network_results/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Exercise 1\n\nDownload network.res1d into a new project folder.\nCreate an empty Jupyter Notebook and import mikeio1d.\nOpen the network.res1d file into a Res1D object.\nUse the info() method to get an overview of the results.\nRead all ‘WaterLevel’ quantities for all nodes into a Pandas DataFrame.\nDisplay the describe() statistics for this DataFrame.\n\nExercise 2\n\nContinue with the Res1D object from Exercise 1 (or reopen network.res1d).\nAccess the reach named “101l1”.\nPrint the static properties: length and n_gridpoints for this reach.\nPlot the ‘WaterLevel’ time series for reach “101l1”.\nOn a separate plot, plot the ‘Discharge’ time series for reach “101l1”.\n\nExercise 3\n\nReopen network.res1d using mikeio1d.open().\nThis time, during the open() call, specify that you only want to load:\n\nNodes: “1”, “5”, and “10”.\nQuantities: Only “WaterLevel”.\nTime range: From “1994-08-07 17:00:00” to “1994-08-07 17:30:00”.\n\nRead the data from the resulting Res1D object into a Pandas DataFrame.\nPrint the head() of this DataFrame. How many columns does it have?\n\nExercise 4\n\nOpen network.res1d into a Res1D object.\nAccess the first grid point on the reach named “100l1”.\nPlot its water level.\nAccess the last grid point on the reach named “100l1”.\nPlot its water level.\n\nExercise 5\n\nDownload model.res1d into a new project folder.\nOpen model.res1d into a Res1D object.\nRead the discharge of reach G60F260_G60F240_l1 into a Pandas DateFrame. Use the discharge closest to the end node.\nConfirm the end node of the reach is a node of type Outlet with id G60F240.\nDownload observed.csv into your project folder.\nRead observed flows from the CSV file into a DataFrame (remembering to use a DatetimeIndex). These represent observed flows of the reach referenced in step 3 above.\nPlot the time series comparison of model vs observed for the following events:\n\n2021-07-31 12:00 to 2021-07-31 15:45\n2021-07-28 17:25 to 2021-07-28 20:55\n2021-06-21 06:55 to 2021-06-21 18:00",
    "crumbs": [
      "Module 3 - Network Results",
      "Homework"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/index.html",
    "href": "module4_calibration_plots_and_statistics/index.html",
    "title": "Welcome to Module 4!",
    "section": "",
    "text": "This module introduces powerful techniques for model calibration and validation. We will explore ModelSkill, a dedicated Python package for comparing MIKE+ model outputs against observed data.\nYou’ll see how ModelSkill builds upon MIKE IO, leveraging your existing skills in data handling.\nThroughout this module, you will learn to:\n\nPrepare Observation and ModelResult objects.\nMatch observational data with model results.\nVisualize model performance using standard validation plots.\nQuantify model accuracy using statistical skill scores.\n\nThis module culminates in a practical homework assignment where you’ll apply these skills to validate a sample MIKE+ model.\nLet’s dive in!\n\n\n\n\n\n\nWhere can I download sample data to follow along?\n\n\n\n\n\nAll of the sample data used in this module is available for download:\n\nflow_meter_data.dfs0\nflow_meter_data.csv\nmodel_results.dfs0\nnetwork.res1d",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "Welcome to Module 4!"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/01_intro_modelskill.html",
    "href": "module4_calibration_plots_and_statistics/01_intro_modelskill.html",
    "title": "ModelSkill",
    "section": "",
    "text": "What is ModelSkill?\nModelSkill is a Python package designed to streamline and standardize the validation of models, including those built with MIKE+. It helps you compare your model results against observed data, calculate skill scores, and generate insightful visualizations.\nThe primary purpose of ModelSkill is to provide a robust framework for quantitative model skill assessment. It builds upon libraries like MIKE IO by leveraging their data reading capabilities to easily ingest model outputs and observational data from various formats. This makes it easier to integrate model validation into your Python-based workflows.\nFor comprehensive information, refer to the modelskill documentation.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ModelSkill</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#key-features",
    "href": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#key-features",
    "title": "ModelSkill",
    "section": "Key Features",
    "text": "Key Features\nModelSkill offers several high-impact features for model validation:\n\nEasy comparison of one or more model results against observations.\nAutomatic calculation of a wide range of statistical skill metrics.\nGeneration of standard validation plots (e.g., time series, scatter plots).\nFlexible handling of various data types and structures.\n\n\n\n\n\n\n\nNote\n\n\n\nModelSkill offers a rich set of functionalities. For a detailed list, please refer to the official documentation’s feature overview.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ModelSkill</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#installation",
    "href": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#installation",
    "title": "ModelSkill",
    "section": "Installation",
    "text": "Installation\nYou can install ModelSkill using uv in your terminal.\nuv pip install modelskill\n\n\n\n\n\n\nTip\n\n\n\nInstallation methods and specific package versions can change. Always refer to the official ModelSkill installation guide for the most up-to-date instructions and troubleshooting.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ModelSkill</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#core-concepts",
    "href": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#core-concepts",
    "title": "ModelSkill",
    "section": "Core Concepts",
    "text": "Core Concepts\nUnderstanding a few core concepts will help you use ModelSkill effectively. The main components are:\n\nObservation: Represents your observed data (e.g., sensor time series).\nModelResult: Represents your model simulation data (e.g., MIKE+ time series output).\nComparer: Matches one Observation with one ModelResult. It aligns them (e.g., spatially and in time) and is used to calculate skill scores and generate plots for this specific pair.\nComparerCollection: Groups multiple Comparer objects. Use it to assess model performance against several observation points or to get overall skill scores.\n\nThe general workflow involves preparing Observation and ModelResult objects, using ms.match() to create Comparer objects, optionally grouping them into a ComparerCollection, and then extracting skill metrics and visualizations.\n\n\n\n\n\n\nFiltering and Selecting Data in ModelSkill\n\n\n\n\n\nModelSkill includes powerful methods like .sel(), .query(), and .where() to select and filter data in its Observation, ModelResult, and Comparer objects. These are excellent for refining your analysis, for example, by focusing on specific events or conditions.\nThis module focuses on the essentials. While these advanced selection tools are highly useful, they are not covered in detail here. We encourage you to explore them after the course; they are valuable for more in-depth analysis.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ModelSkill</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/02_preparing_data.html",
    "href": "module4_calibration_plots_and_statistics/02_preparing_data.html",
    "title": "Preparing Data",
    "section": "",
    "text": "Observations\nModelSkill requires data in Observation and ModelResult objects. These objects are inputs for ModelSkill’s Comparer, which matches data and assesses skill. This section covers PointObservation and PointModelResult for comparing time series at specific points.\nA PointObservation represents measured data, often a time series from one sensor. Each object handles one point and variable. For API details, see the PointObservation documentation.\nKey parameters for PointObservation:",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Preparing Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/02_preparing_data.html#observations",
    "href": "module4_calibration_plots_and_statistics/02_preparing_data.html#observations",
    "title": "Preparing Data",
    "section": "",
    "text": "Parameter\nDescription\n\n\n\n\nname\nA unique identifier (e.g., “Gauge_A_WaterLevel”). Useful for distinguishing observations and labeling plots.\n\n\ndata\nThe data source: a dfs0 file path, MIKE IO Dataset, or Pandas DataFrame.\n\n\nitem\nSpecifies the data column (for Pandas DataFrame) or item (for MIKE IO Dataset or dfs0 path) from the source. Refer by name (string) or numerical index.\n\n\nquantity\nA modelskill.Quantity defining the variable name (e.g., “Water Level”) and unit (e.g., “m”). Essential if the data source (e.g., Pandas DataFrame) lacks this metadata. ModelSkill often infers this from dfs0 files with EUM information.\n\n\n\n\n\n\n\n\n\nUnderstanding the Quantity object in ModelSkill\n\n\n\n\n\nThe quantity parameter (ms.Quantity(name=\"...\", unit=\"...\")) is vital for ModelSkill. It defines the data’s variable (e.g., “Water Level,” “Discharge”) and unit (e.g., “m,” “m^3/s”). This information is used for:\n\nClear plot labeling.\nCompatibility checks between observations and model results.\n\nModelSkill often infers quantity from dfs0 files with EUM information. For other sources like Pandas DataFrames or CSV files, you must define quantity explicitly.\nConsult the ModelSkill documentation on Quantity for details, including EUM handling and more examples.\n\n\n\n\n\n\n\n\n\nCoordinates (x, y) for a PointObservation\n\n\n\n\n\nModelSkill examples often include x and y coordinates for PointObservation objects. ModelSkill uses these coordinates mainly to interpolate data from spatial model outputs (e.g., dfsu, dfs2 files) to the observation point. This is useful for comparing point observations to 2D or 3D model fields.\nThis module focuses on comparing time series already extracted for specific points (e.g., from a res1d node to dfs0, or a point sensor dfs0). Thus, we won’t use the x and y spatial interpolation capability extensively here.\n\n\n\n\nFrom Dataset\nFirst, read a dfs0 file into a MIKE IO Dataset.\n\nds_obs = mikeio.read(\"data/flow_meter_data.dfs0\")\nds_obs\n\n&lt;mikeio.Dataset&gt;\ndims: (time:121)\ntime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00 (121 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  116l1_observed &lt;Discharge&gt; (meter pow 3 per sec)\n  1:  12l1_observed &lt;Discharge&gt; (meter pow 3 per sec)\n\n\nCreate a PointObservation from this Dataset, selecting one item.\n\nobs_116l1 = ms.PointObservation(\n    data=ds_obs,\n    item=\"116l1_observed\",    # Selects one column/item\n    name=\"116l1_Gauge\",       # Descriptive name for this specific observation\n)\nobs_116l1\n\n&lt;PointObservation&gt;: 116l1_Gauge\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\n\n\nA PointObservation has useful attributes and methods. Plot to verify:\n\nobs_116l1.plot()\n\n\n\n\n\n\n\n\n\n\nFrom dfs0 file\nAlternatively, create a PointObservation using the dfs0 file path directly:\n\nobs_116l1_from_file = ms.PointObservation(\n    data=\"data/flow_meter_data.dfs0\",\n    item=\"116l1_observed\",\n    name=\"116l1_Gauge\",\n)\nobs_116l1_from_file.to_dataframe().head()\n\n\n\n\n\n\n\n\n116l1_Gauge\n\n\ntime\n\n\n\n\n\n1994-08-07 16:35:00\n-0.014113\n\n\n1994-08-07 16:36:00\n0.043355\n\n\n1994-08-07 16:37:00\n-0.129244\n\n\n1994-08-07 16:38:00\n-0.052462\n\n\n1994-08-07 16:39:00\n-0.051976\n\n\n\n\n\n\n\n\n\nFrom Pandas DataFrame\nFirst, prepare a Pandas DataFrame.\n\ndf_obs_csv = pd.read_csv(\"data/flow_meter_data.csv\", index_col=\"time\", parse_dates=True)\ndf_obs_csv.head()\n\n\n\n\n\n\n\n\n116l1_observed\n12l1_observed\n\n\ntime\n\n\n\n\n\n\n1994-08-07 16:35:00\n-0.014113\n-0.095583\n\n\n1994-08-07 16:36:00\n0.043355\n-0.058748\n\n\n1994-08-07 16:37:00\n-0.129244\n0.040089\n\n\n1994-08-07 16:38:00\n-0.052462\n0.068110\n\n\n1994-08-07 16:39:00\n-0.051976\n-0.024882\n\n\n\n\n\n\n\nCreate a PointObservation from the DataFrame. Provide quantity as DataFrames lack EUM information.\n\nobs_12l1_from_df = ms.PointObservation(\n    data=df_obs_csv,\n    item=\"12l1_observed\",\n    name=\"12l1_Gauge\",\n    quantity=ms.Quantity(name=\"Discharge\", unit=\"m^3/s\"),\n)\nobs_12l1_from_df.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEnsure DataFrames have a DatetimeIndex, as mentioned in previous modules.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Preparing Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/02_preparing_data.html#model-results",
    "href": "module4_calibration_plots_and_statistics/02_preparing_data.html#model-results",
    "title": "Preparing Data",
    "section": "Model Results",
    "text": "Model Results\nPointModelResult objects represent model simulation outputs. Each PointModelResult handles one variable from a specific model output point and represents a model simulation run. See the PointModelResult documentation for API details.\nKey parameters for PointModelResult are similar to PointObservation:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nname\nIdentifies the model simulation run (e.g., “MIKE_Plus_Scenario_A”).\n\n\ndata\nThe data source: a dfs0 file path, MIKE IO Dataset, or Pandas DataFrame.\n\n\nitem\nSpecifies the data column (for Pandas DataFrame) or item (for MIKE IO Dataset or dfs0 path) from the source.\n\n\nquantity\nA modelskill.Quantity. Crucial if metadata is missing (e.g., Pandas DataFrame). Often inferred from dfs0 files with EUM info.\n\n\n\n\n\n\n\n\n\nMany PointModelResult objects can share the same name\n\n\n\n\n\nThe name parameter in PointModelResult identifies the overall model simulation, not a specific point. You may create several PointModelResult objects that all come from the same simulation but represent different output locations (e.g., water level at point A, discharge at point B). All these objects should share the same name (e.g., “Model_Run_Alpha”). This shared name signifies they originate from the same model execution. Later, when using ModelSkill’s Comparer, you will explicitly match each of these individual PointModelResult objects to its corresponding Observation object.\n\n\n\n\nFrom Dataset\nFirst, read a dfs0 file with model output into a MIKE IO Dataset.\n\nds_model_data = mikeio.read(\"data/model_results.dfs0\")\nds_model_data\n\n&lt;mikeio.Dataset&gt;\ndims: (time:110)\ntime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00 (110 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  reach:Discharge:116l1:37.651 &lt;Discharge&gt; (meter pow 3 per sec)\n  1:  reach:Discharge:12l1:28.410 &lt;Discharge&gt; (meter pow 3 per sec)\n\n\nCreate the PointModelResult from the Dataset. name identifies the model simulation. quantity is often inferred from dfs0 files with EUM information.\n\nmod_116l1_dataset = ms.PointModelResult(\n    data=ds_model_data,\n    item=\"reach:Discharge:116l1:37.651\",       # Item name from the dfs0\n    name=\"MIKE+_RunA\",                         # Model simulation identifier\n)\nmod_116l1_dataset\n\n&lt;PointModelResult&gt;: MIKE+_RunA\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\n\n\nLike with observations, the PointModelResult object has useful attributes and methods. For example, plot to verify:\n\nmod_116l1_dataset.plot()\n\n\n\n\n\n\n\n\n\n\nFrom dfs0 file\nCreate a PointModelResult using the dfs0 file path directly.\n\nmod_12l1_file = ms.PointModelResult(\n    data=\"data/model_results.dfs0\",\n    item=\"reach:Discharge:12l1:28.410\",\n    name=\"MIKE+_RunA\",                      # Same simulation as above, different location/item\n)\nmod_12l1_file.to_dataframe().head()\n\n\n\n\n\n\n\n\nMIKE+_RunA\n\n\ntime\n\n\n\n\n\n1994-08-07 16:35:00.000\n0.000000\n\n\n1994-08-07 16:36:01.870\n-0.000004\n\n\n1994-08-07 16:37:07.560\n-0.000009\n\n\n1994-08-07 16:38:55.828\n-0.000004\n\n\n1994-08-07 16:39:55.828\n0.000006\n\n\n\n\n\n\n\n\n\nFrom Pandas DataFrame\nFirst, prepare a Pandas DataFrame with model data. This example reads a dfs0 file into a DataFrame.\n\ndf_model = mikeio.read(\"data/model_results.dfs0\").to_dataframe()\ndf_model.head()\n\n\n\n\n\n\n\n\nreach:Discharge:116l1:37.651\nreach:Discharge:12l1:28.410\n\n\n\n\n1994-08-07 16:35:00.000\n0.000000\n0.000000\n\n\n1994-08-07 16:36:01.870\n0.000007\n-0.000004\n\n\n1994-08-07 16:37:07.560\n0.000022\n-0.000009\n\n\n1994-08-07 16:38:55.828\n0.000043\n-0.000004\n\n\n1994-08-07 16:39:55.828\n0.000054\n0.000006\n\n\n\n\n\n\n\nCreate a PointModelResult from the DataFrame. Provide quantity as DataFrames lack EUM information.\n\nmod_116l1_df = ms.PointModelResult(\n    data=df_model,\n    item=\"reach:Discharge:116l1:37.651\",            # Column name in the DataFrame\n    name=\"MIKE+_RunA\",                                   # Identifies the overall model simulation\n    quantity=ms.Quantity(name=\"Discharge\", unit=\"m^3/s\"),\n)\nmod_116l1_df.plot()\n\n\n\n\n\n\n\n\n\n\nFrom res1d file\nMIKE+ res1d files store results for an entire network. For point comparisons with PointObservation in ModelSkill, first extract the specific time series for the point(s) into an intermediate format (e.g., dfs0 file, Pandas DataFrame). This example extracts one model output point to a dfs0 file, then creates a PointModelResult.\nFirst, extract model output (one point, one variable) to a dfs0 file.\n\nres = mikeio1d.open(\"data/network.res1d\")\nres.reaches[\"116l1\"][\"37.651\"].Discharge.to_dfs0(\"data/model_Q_116l1.dfs0\")\nds = mikeio.read(\"data/model_Q_116l1.dfs0\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:110)\ntime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00 (110 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  reach:Discharge:116l1:37.651 &lt;Discharge&gt; (meter pow 3 per sec)\n\n\nNow, create a PointModelResult from this new dfs0 file. Use the item name from the Dataset object created above.\n\nmod_116l1 = ms.PointModelResult(\n    data=ds,\n    item=\"reach:Discharge:116l1:37.651\",\n    name=\"MIKE+\",\n)\nmod_116l1.plot() # Verify\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuture: better integration of res1d with ModelSkill\n\n\n\n\n\nFuture versions of ModelSkill may allow creating a network result, instead of a point result. This would allow network results to automatically be matched with corresponding observations, eliminating the need to manually match individual model result points with observation points.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Preparing Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/02_preparing_data.html#best-practices",
    "href": "module4_calibration_plots_and_statistics/02_preparing_data.html#best-practices",
    "title": "Preparing Data",
    "section": "Best Practices",
    "text": "Best Practices\nConsistent data organization and naming are key.\n\nOrganize Data: Structure observation and model result files (e.g., separate folders, clear names). This helps when programmatically accessing many files.\nDescriptive Names: Use name in PointObservation and PointModelResult for clear identifiers (e.g., PointObservation(name=\"Flow_Gauge_West\")). This aids in managing objects, improves plot clarity, and helps programmatic creation with many observations or runs.\nSpecify Units and Quantities: Always provide quantity for sources like DataFrames or CSVs. ModelSkill often infers this from dfs0 files with EUM information. Correct metadata is crucial for comparisons, visualizations, and automated workflows. See the modelskill.Quantity callout and official documentation.\n\n\n\n\n\n\n\nBeyond points: Exploring other data types in ModelSkill\n\n\n\n\n\nModelSkill is versatile. This section focuses on point data, but the package also supports TrackObservation (data along a path) and GridObservation (gridded data). These are useful for different validation scenarios. See the official documentation for examples and use cases.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Preparing Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/03_matching_data.html",
    "href": "module4_calibration_plots_and_statistics/03_matching_data.html",
    "title": "Matching Data",
    "section": "",
    "text": "Comparer\nNow that you’ve prepared your Observation and ModelResult data, the next crucial step is to bring them together for direct comparison. ModelSkill makes this easy with ms.match() which creates a Comparer object. For situations where you need to assess performance across multiple locations or aggregate results, you can group several Comparer objects into a ComparerCollection.\nThis section uses Observation and ModelResult objects prepared as in previous examples.\nUse ms.match() to create a Comparer object. A Comparer is designed to match one Observation with one ModelResult. For point data, match() intelligently interpolates model results to observation timestamps, ensuring a direct, like-for-like comparison.\nLet’s match our previously defined obs_116l1 with mod_116l1:\nobs_116l1\n\n&lt;PointObservation&gt;: 116l1\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\nmod_116l1\n\n&lt;PointModelResult&gt;: MIKE+\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\ncomparer_116l1 = ms.match(obs_116l1, mod_116l1)\ncomparer_116l1\n\n&lt;Comparer&gt;\nQuantity: Discharge [m^3/s]\nObservation: 116l1, n_points=121\nModel(s):\n0: MIKE+\nThe Comparer now conveniently holds your matched data, interpolated and aligned, ready for detailed analysis and visualization. You can convert this to a Pandas DataFrame to inspect the aligned data. Notice how the DataFrame contains raw observation values and model values interpolated to the exact observation times.\ndf_aligned_single = comparer_116l1.to_dataframe()\ndf_aligned_single.head()\n\n\n\n\n\n\n\n\nObservation\nMIKE+\n\n\ntime\n\n\n\n\n\n\n1994-08-07 16:35:00\n-0.014113\n0.000000\n\n\n1994-08-07 16:36:00\n0.043355\n0.000007\n\n\n1994-08-07 16:37:00\n-0.129244\n0.000021\n\n\n1994-08-07 16:38:00\n-0.052462\n0.000032\n\n\n1994-08-07 16:39:00\n-0.051976\n0.000044\nA Comparer object is your gateway to generating insightful plots and calculating a range of skill scores. Plotting and skill assessment are detailed later in this module, but here’s a quick preview:\ncomparer_116l1.plot.timeseries()\ncomparer_116l1.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Matching Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/03_matching_data.html#comparer",
    "href": "module4_calibration_plots_and_statistics/03_matching_data.html#comparer",
    "title": "Matching Data",
    "section": "",
    "text": "Ensure Correct Pairing\n\n\n\nIt’s crucial that the Observation and ModelResult objects passed to ms.match() represent the same physical location and variable. ModelSkill relies on you to provide these correctly paired inputs.\n\n\n\n\n\n\n\n\nHandling Gaps in Model Data During Matching\n\n\n\n\n\nWhen matching, ModelSkill interpolates model results to observation times. If your model data has significant time gaps, you might not want to interpolate across very large intervals. For example, this is often the case with LTS simulations. The max_model_gap parameter in ms.match() controls this. It specifies the maximum gap (in seconds) in the model data over which to interpolate. If a gap is larger, the corresponding observation points will not have a matched model value.\n\ncomparer_116l1_gapped = ms.match(obs_116l1, mod_116l1, max_model_gap=600)\ncomparer_116l1_gapped\n\n&lt;Comparer&gt;\nQuantity: Discharge [m^3/s]\nObservation: 116l1, n_points=121\nModel(s):\n0: MIKE+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering Matched Data\n\n\n\n\n\nAfter matching, you might want to further filter the Comparer data before calculating skill scores. For example, you might want to exclude periods of low flow or focus only on specific events. The .query() method allows you to apply conditions, similar to Pandas. It returns a new Comparer object with the filtered data.\n\ncomparer_116l1_filtered = comparer_116l1.query(\"Observation &gt; 0.8\")\ncomparer_116l1_filtered.plot.timeseries()\n\n\n\n\n\n\n\n\nModelSkill offers more filtering options not covered here — see the documentation for details.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Matching Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/03_matching_data.html#comparercollection",
    "href": "module4_calibration_plots_and_statistics/03_matching_data.html#comparercollection",
    "title": "Matching Data",
    "section": "ComparerCollection",
    "text": "ComparerCollection\nOften, you’ll want to evaluate your model against multiple observation points simultaneously or assess its overall performance across different locations. For this, ModelSkill provides the ComparerCollection, which groups multiple Comparer objects.\n\n\n\n\n\n\nComparing models against other models\n\n\n\n\n\nA ComparerCollection can also be used to compare multiple different models against the same set of observations. That use case is not covered in this module. Refer to ModelSkill’s documentation for details on this powerful feature.\n\n\n\nFirst, let’s create another Comparer for our second observation point, obs_12l1, and its corresponding model result, mod_12l1 (which comes from the same MIKE+ simulation).\n\nobs_12l1\n\n&lt;PointObservation&gt;: 12l1\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\n\n\n\nmod_12l1\n\n&lt;PointModelResult&gt;: MIKE+\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\n\n\n\ncomparer_12l1 = ms.match(obs_12l1, mod_12l1)\ncomparer_12l1\n\n&lt;Comparer&gt;\nQuantity: Discharge [m^3/s]\nObservation: 12l1, n_points=121\nModel(s):\n0: MIKE+\n\n\nNow, we can combine comparer_116l1 and comparer_12l1 into a ComparerCollection:\n\ncc = ms.ComparerCollection([comparer_116l1, comparer_12l1])\ncc\n\n&lt;ComparerCollection&gt;\nComparers:\n0: 116l1 - Discharge [m^3/s]\n1: 12l1 - Discharge [m^3/s]\n\n\nThe ComparerCollection (cc) now manages both comparisons. This allows for powerful aggregate views. For instance, it provides skill assessment for each individual observation:\n\ncc.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405\n\n\n12l1\n121\n-0.004083\n0.063414\n0.063282\n0.049679\n0.971574\n0.305942\n0.942928\n\n\n\n\n\n\n\nMore importantly, it enables aggregate views of model performance, such as overall skill scores that consider all included comparisons:\n\ncc.mean_skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\n\nMIKE+\n242\n-0.000427\n0.065764\n0.06566\n0.052098\n0.981404\n0.239268\n0.962667\n\n\n\n\n\n\n\nAnd aggregate plots, like a histogram of residual errors across all matched observation points:\n\ncc.plot.residual_hist()\n\n\n\n\n\n\n\n\nYou can inspect the collection’s properties to see which observations and models are included:\nUnique observation names within the collection:\n\ncc.obs_names\n\n['116l1', '12l1']\n\n\nUnique model names. Since both mod_116l1 and mod_12l1 were created with name=\"MIKE+\", “MIKE+” appears only once, signifying they are from the same model run.\n\ncc.mod_names\n\n['MIKE+']\n\n\nYou can also select an individual Comparer from the collection by its observation name, allowing you to focus on a specific comparison:\n\nselected_comparer = cc['116l1']\nselected_comparer.plot.timeseries()",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Matching Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/04_visualizing_performance.html",
    "href": "module4_calibration_plots_and_statistics/04_visualizing_performance.html",
    "title": "Visualization",
    "section": "",
    "text": "Comparer Plots\nAfter matching observations and model results into Comparer and ComparerCollection objects (as shown in the previous section), you can visualize these comparisons. ModelSkill simplifies generating standard validation plots, offering a more direct approach than manually creating them with Pandas and Matplotlib as covered in Module 2. This section demonstrates these built-in plotting capabilities for assessing model performance, both for individual comparison points and aggregated across multiple locations.\nA Comparer (one observation vs. one model result) offers several plot types for detailed inspection.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/04_visualizing_performance.html#comparer-plots",
    "href": "module4_calibration_plots_and_statistics/04_visualizing_performance.html#comparer-plots",
    "title": "Visualization",
    "section": "",
    "text": "Time Series Plot\nOverlays observed and model time series. Shows how well the model captures temporal patterns, peaks, and timing.\n\ncomparer_116l1.plot.timeseries()\n\n\n\n\n\n\n\n\n\n\nScatter Plot\nPlots observed values against model values. Points near the 1:1 line indicate good agreement. Helps identify bias or scaling issues.\n\ncomparer_116l1.plot.scatter()\n\n\n\n\n\n\n\n\n\n\nHistogram Plot\nCompares frequency distributions of observed and model data. Shows if the model reproduces the overall value spread.\n\ncomparer_116l1.plot.hist()\n\n\n\n\n\n\n\n\nModelSkill offers additional Comparer plots, such as residual histograms and Q-Q plots. See the official ModelSkill ComparerPlotter API documentation for a complete list.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/04_visualizing_performance.html#comparercollection-plots",
    "href": "module4_calibration_plots_and_statistics/04_visualizing_performance.html#comparercollection-plots",
    "title": "Visualization",
    "section": "ComparerCollection Plots",
    "text": "ComparerCollection Plots\nA ComparerCollection allows for aggregate plots, summarizing performance across all included comparisons. These aggregated plots are powerful because they give you a broader picture of your model’s performance across all your chosen validation points, rather than just looking at one location in isolation.\n\nTemporal Coverage Plot\nShows the temporal data availability for each observation and model result in the collection, indicating periods of overlap and data gaps.\n\ncc.plot.temporal_coverage()\n\n\n\n\n\n\n\n\n\n\nScatter Plot\nAggregates all (observed, model) pairs from the collection. This gives an overview of model performance across all locations, providing a holistic view of point-by-point agreement.\n\ncc.plot.scatter()\n\n\n\n\n\n\n\n\n\n\nHistogram Plot\nCombines data from all comparisons. This shows if the model matches the overall statistical profile of the observed data when considering all sites together.\n\ncc.plot.hist()\n\n\n\n\n\n\n\n\nThe ComparerCollection offers other aggregate plots, like box plots. For more options, consult the ModelSkill ComparerCollectionPlotter API documentation.\n\n\n\n\n\n\nCustomizing and Saving Plots\n\n\n\nModelSkill plots are Matplotlib objects. Customize and save them using standard Matplotlib functions (e.g., ax.set_title(\"My Custom Title\"), plt.savefig(\"my_plot.png\")).\n\n\nThese plots offer a qualitative assessment of model performance. The next section will cover how to quantify performance using ModelSkill’s statistical skill scores.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html",
    "title": "Skill Scores",
    "section": "",
    "text": "Comparer\nVisualizing model performance provides qualitative insights, but quantitative metrics are essential for objective assessment and comparison. Skill scores serve this purpose by providing numerical measures of how well a model’s predictions match observed data. ModelSkill facilitates the calculation of these key statistics through its Comparer and ComparerCollection objects.\nThe Comparer object (e.g., comparer_116l1) calculates skill scores for a single observation-model pair.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#comparer",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#comparer",
    "title": "Skill Scores",
    "section": "",
    "text": "Skill Table\nThe skill() method returns a SkillTable object, which is a specialized data structure provided by ModelSkill for presenting multiple skill scores in a clear, tabular format.\n\nsk_single = comparer_116l1.skill()\nsk_single\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvailable metrics\n\n\n\n\n\nYou’ll notice several metrics listed (e.g., bias, rmse, nse). We’ll cover the definitions of common metrics in more detail at the end of this section.\n\n\n\n\n\n\n\n\n\nSkill metrics in DataFrame\n\n\n\n\n\nThe SkillTable object can be converted to a Pandas DataFrame using sk_single.to_dataframe().\n\n\n\nTo get a subset of metrics, pass a list of metric names to the metrics argument.\n\nsk_subset_single = comparer_116l1.skill(metrics=['rmse', 'bias', 'nse'])\nsk_subset_single\n\n\n\n\n\n\n\n\nn\nrmse\nbias\nnse\n\n\nobservation\n\n\n\n\n\n\n\n\n116l1\n121\n0.068114\n0.003229\n0.982405\n\n\n\n\n\n\n\n\n\nSingle Score\nUse score() for direct access to a single numerical value for a specific metric. If model results within the Comparer are named (as in this example with “MIKE+”), this method returns a dictionary where keys are model names.\n\nrmse_val_dict = comparer_116l1.score(metric='rmse')\nprint(f\"RMSE for MIKE+ at 116l1: {rmse_val_dict['MIKE+']:.4f}\")\n\nbias_val_dict = comparer_116l1.score(metric='bias')\nprint(f\"Bias for MIKE+ at 116l1: {bias_val_dict['MIKE+']:.4f}\")\n\nRMSE for MIKE+ at 116l1: 0.0681\nBias for MIKE+ at 116l1: 0.0032",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#comparercollection",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#comparercollection",
    "title": "Skill Scores",
    "section": "ComparerCollection",
    "text": "ComparerCollection\nThe ComparerCollection (e.g., cc) assesses model performance across multiple observation points.\n\nSkill Table\nCalling skill() on a ComparerCollection returns a SkillTable object summarizing skill for each Comparer within the collection.\n\nsk_coll = cc.skill()\nsk_coll\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405\n\n\n12l1\n121\n-0.004083\n0.063414\n0.063282\n0.049679\n0.971574\n0.305942\n0.942928\n\n\n\n\n\n\n\nYou can request specific metrics for all comparisons.\n\nsk_subset_coll = cc.skill(metrics=['rmse', 'bias'])\nsk_subset_coll\n\n\n\n\n\n\n\n\nn\nrmse\nbias\n\n\nobservation\n\n\n\n\n\n\n\n116l1\n121\n0.068114\n0.003229\n\n\n12l1\n121\n0.063414\n-0.004083\n\n\n\n\n\n\n\n\n\nMean Skill Table\nThe mean_skill() method calculates average skill scores across all locations, presented in a SkillTable.\n\nsk_mean = cc.mean_skill()\nsk_mean\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\n\nMIKE+\n242\n-0.000427\n0.065764\n0.06566\n0.052098\n0.981404\n0.239268\n0.962667\n\n\n\n\n\n\n\nAnd for specific metrics:\n\nsk_mean_subset = cc.mean_skill(metrics=['rmse', 'bias', 'nse'])\nsk_mean_subset\n\n\n\n\n\n\n\n\nn\nrmse\nbias\nnse\n\n\nmodel\n\n\n\n\n\n\n\n\nMIKE+\n242\n0.065764\n-0.000427\n0.962667\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeighted mean skill\n\n\n\n\n\nThe mean_skill() method allows for weighted averages. You can provide weights for each observation if, for example, you trust some observation points more than others or if they represent areas of different importance. See the ModelSkill documentation for details on applying weights.\n\n\n\n\n\nScore\nThe score() method on a ComparerCollection calculates a score for each model across all relevant observations. It returns a Python dictionary where keys are the model names (e.g., ‘MIKE+’) and values are these scores (e.g., mean RMSE for ‘MIKE+’). This provides a single summary value for each model’s performance on a specific metric.\n\n# For our ComparerCollection 'cc' containing one model named \"MIKE+\"\nscore_rmse_scores = cc.score(metric='rmse')\nprint(f\"Mean RMSE for models: {score_rmse_scores}\")\n\nscore_bias_scores = cc.score(metric='bias')\nprint(f\"Mean Bias for models: {score_bias_scores}\")\n\nMean RMSE for models: {'MIKE+': 0.06576367286619714}\nMean Bias for models: {'MIKE+': -0.00042693940822553073}\n\n\n\n\n\n\n\n\nWeighted mean score\n\n\n\n\n\nSimilar to mean_skill(), the score() method on a ComparerCollection also supports weighting. This enables you to calculate a weighted mean score (e.g., weighted RMSE) for each model across all observations.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#working-with-skilltables",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#working-with-skilltables",
    "title": "Skill Scores",
    "section": "Working with SkillTables",
    "text": "Working with SkillTables\nSkillTable objects are more than just static tables; they offer several useful features for analysis and presentation.\n\nSorting Values\nYou can sort the SkillTable by any of its columns (metrics or identifiers). This is useful for ranking models or observations.\n\n# Sort by RMSE in ascending order\nsk_coll_sorted = sk_coll.sort_values('rmse', ascending=True)\nsk_coll_sorted\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n12l1\n121\n-0.004083\n0.063414\n0.063282\n0.049679\n0.971574\n0.305942\n0.942928\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405\n\n\n\n\n\n\n\n\n\nStyling Tables\nSkillTable objects integrate with Pandas’ styling capabilities, allowing you to highlight important values, apply color maps, or format numbers for better readability in Jupyter environments.\n\nsk_coll.style()\n\n\n\n\n\n\n \nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n \n \n \n \n \n \n \n \n\n\n\n\n116l1\n121\n0.003\n0.068\n0.068\n0.055\n0.991\n0.173\n0.982\n\n\n12l1\n121\n-0.004\n0.063\n0.063\n0.050\n0.972\n0.306\n0.943\n\n\n\n\n\n\n\nPlotting Skills\nSkillTable objects have a .plot accessor for quickly visualizing skill scores, such as creating bar charts of metrics.\n\n# Bar plot of RMSE for each observation point\nsk_coll[\"rmse\"].plot.bar()\n\n\n\n\n\n\n\n\nThese are just a few examples. The SkillTable’s .style and .plot accessors offer more customization. Refer to the ModelSkill documentation and Pandas styling documentation for further details.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#useful-skill-metrics",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#useful-skill-metrics",
    "title": "Skill Scores",
    "section": "Useful Skill Metrics",
    "text": "Useful Skill Metrics\nModelSkill calculates numerous metrics. The choice of metrics depends on your modelling goals. Some useful metrics include:\n\nBias (bias): Average difference (Modeled - Observed). Ideal: 0.\nRMSE (Root Mean Square Error) (rmse): Typical magnitude of error. Ideal: 0.\nNSE (Nash-Sutcliffe Efficiency) (nse): Measures the predictive power of the model compared to using the mean of the observed data as the prediction. Ranges from -\\(\\infty\\) to 1. Ideal: 1.\nKGE (Kling-Gupta Efficiency) (kge): A composite metric evaluating correlation, bias, and variability components. Ranges from -\\(\\infty\\) to 1. Ideal: 1.\nWillmott’s Index of Agreement (willmott): Measures the degree of model prediction error, standardized by observed variability. Ranges from 0 to 1. Ideal: 1.\nPeak Ratio (pr): Ratio of the maximum modeled value to the maximum observed value over the matched time period. Ideal: 1.0.\n\nYou can change the default list of metrics that are used by skill() and mean_skill() as follows:\n\nms.set_option(\"metrics.list\", ['bias', 'rmse', 'nse', 'kge', 'willmott', 'pr'])\ncc.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nnse\nkge\nwillmott\npr\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.982405\n0.977502\n0.995498\n0.944704\n\n\n12l1\n121\n-0.004083\n0.063414\n0.942928\n0.932798\n0.984693\n0.891797\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nReset to default values with ms.reset_option(\"metrics.list\").\n\n\nFor a comprehensive list of all available metrics and their precise definitions, please refer to the official ModelSkill API documentation for metrics.\n\n\n\n\n\n\nAdding Custom Metrics\n\n\n\n\n\nModelSkill’s metrics are extensible. You can define and use your own custom skill score functions if needed. If you believe a metric would be broadly useful, consider suggesting it for inclusion in ModelSkill via a GitHub issue.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/homework.html",
    "href": "module4_calibration_plots_and_statistics/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Exercise 1\n\nDownload module4_model.zip into a new project folder and extract its content.\nReview the folder structure and files of the model. It should be as follows:\n\nmodule4_model\n├── data\n│   ├── flow_meter_A_2020_09.dfs0\n│   ├── flow_meter_B_2020_09.dfs0\n│   └── rainfall_events_2020-09.dfs0\n├── Dyrup_uncalibrated.mupp\n├── Dyrup_uncalibrated.sqlite\n├── LTS\n│   └── rainfall_events_LTS_2020_09.MJL\n└── results\n    ├── rainfallBaseDefault_Network_HD.res1d\n    └── rainfallBaseDefault_Surface_runoff.res1d\n\nOpen Dyrup_uncalibrated.mupp in MIKE+ and familiarize yourself with the model.\n\nReview the boundary conditions. Can you see where the rainfall is applied?\nFind the system’s only node outlet: G60F360.\nLocate the two flow meters (A and B). Which reach IDs are they associated with?\nReview the simulation setup.\n\nMake a new LTS job list for the simulation. How does it compare with LTS/rainfall_events_LTS_2020_09.MJL?\nRun a simulation. Do you get the same results as those in the results folder?\n\nExercise 2\n\nCreate a new Jupyter Notebook and save it in your project folder.\nCreate a PointObservation object for “Flow Meter A” using data/flow_meter_A_2020_09.dfs0.\nCreate a PointObservation object for “Flow Meter B” using data/flow_meter_B_2020_09.dfs0.\nFor both obs_A and obs_B:\n\nPrint their objects and inspect the data.\nPlot their time series.\n\n\nExercise 3\n\nUse the model network results from Exercise 1 (or the provided reference results).\nFor “Flow Meter A” (model reach G60F380_G60F360_l1), extract its ‘Discharge’ quantity to a dfs0 file.\nCreate a PointModelResult object from the dfs0 file created above.\nFor “Flow Meter B” (model reach G62F070_G62F060_l1), extract its ‘Discharge’ quantity to a dfs0 file.\nCreate a PointModelResult object from the dfs0 file created above.\nFor both mod_A and mod_B:\n\nPrint the object summary.\nPlot the model result time series.\n\n\nExercise 4\n\nMatch obs_A with mod_A using ms.match(). Store in comparer_A.\nMatch obs_B with mod_B. Store in comparer_B.\nFor comparer_A:\n\nGenerate time series plot: comparer_A.plot.timeseries().\nGenerate scatter plot: comparer_A.plot.scatter().\nCalculate and display skill table: comparer_A.skill().\n\nRepeat step 3 for comparer_B.\nExamine the skill tables. Qualitatively, which metrics suggest better or worse performance for each location?\n\nExercise 5\n\nCreate a ComparerCollection named cc from comparer_A and comparer_B.\nDisplay cc. Check cc.obs_names and cc.mod_names.\nMake a temporal coverage plot: cc.plot.temporal_coverage().\nMake a scatter plot: cc.plot.scatter().\nMake a residual histogram: cc.plot.residual_hist().\nCalculate and display the skill table for the collection: cc.skill().\n\nModify to show only ‘rmse’, ‘bias’, ‘nse’, and ‘kge’ metrics.\n\nCalculate and display the mean skill table: cc.mean_skill().\n\nModify to show only ‘rmse’, ‘bias’, ‘nse’, and ‘kge’.\n\nUsing cc.score(), retrieve and print the mean ‘kge’ score for the model.\n\nExercise 6\n\nRepeat Exercise 4 and 5, but using the max_model_gap parameter of ms.match(). Use a value of 600 seconds.\nHow do the number of observation points in each Comparer change?\nHow does the temporal coverage plot change?\nHow does this approach impact the skill assessment?\nReview ModelSkill’s documentation on ‘nse’. How do both flow meters, and the overall model perform on this metric?",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "Homework"
    ]
  }
]